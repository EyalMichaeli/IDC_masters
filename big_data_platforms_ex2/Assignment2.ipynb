{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Platform\n",
    "## Assignment 2: MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**By:**  \n",
    "\n",
    "John Doe, 300123123  \n",
    "Jane Doe, 200123123\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The goal of this assignment is to:**\n",
    "- Understand and practice the details of MapReduceEngine\n",
    "\n",
    "**Instructions:**\n",
    "- Students will form teams of two people each, and submit a single homework for each team.\n",
    "- The same score for the homework will be given to each member of your team.\n",
    "- Your solution is in the form of a Jupyter notebook file (with extension ipynb).\n",
    "- Images/Graphs/Tables should be submitted inside the notebook.\n",
    "- The notebook should be runnable and properly documented. \n",
    "- Please answer all the questions and include all your code.\n",
    "- You are expected to submit a clear and pythonic code.\n",
    "- You can change functions signatures/definitions.\n",
    "\n",
    "**Submission:**\n",
    "- Submission of the homework will be done via Moodle by uploading a Jupyter notebook.\n",
    "- The homework needs to be entirely in English.\n",
    "- The deadline for submission is on Moodle.\n",
    "- Late submission won't be allowed.\n",
    "  \n",
    "  \n",
    "- In case of identical code submissions - both groups will get a Zero. \n",
    "- Some groups might be selected randomly to present their code.\n",
    "\n",
    "**Requirements:**  \n",
    "- Python 3.6 should be used.  \n",
    "- You should implement the algorithms by yourself using only basic Python libraries (such as numpy,pandas,etc.)\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grading:**\n",
    "- Q1 - 5 points - Initial Steps\n",
    "- Q2 - 50 points - MapReduceEngine\n",
    "- Q3 - 30 points - Implement the MapReduce Inverted index of the JSON documents\n",
    "- Q4 - 5 points - Testing Your MapReduce\n",
    "- Q5 - 10 points - Final Thoughts \n",
    "\n",
    "`Total: 100`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prerequisites**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "import itertools\n",
    "import sqlite3\n",
    "import traceback\n",
    "#!pip install --quiet zipfile36"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "import threading # you can use easier threading packages\n",
    "from threading import Thread\n",
    "import concurrent\n",
    "from pathlib import Path\n",
    "from multiprocessing import Queue\n",
    "# ml\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "# visual\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# notebook\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hide Warnings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disable Autoscrolling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "IPython.OutputArea.prototype._should_scroll = function(lines) {\n    return false;\n}\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set Random Seed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "# Question 1\n",
    "# Initial Steps\n",
    "\n",
    "Write Python code to create 20 different CSV files in this format:  `myCSV[Number].csv`, where each file contains 10 records. \n",
    "\n",
    "The schema is `(‘firstname’,’secondname’,city’)`  \n",
    "\n",
    "Values should be randomly chosen from the lists: \n",
    "- `firstname` : `[John, Dana, Scott, Marc, Steven, Michael, Albert, Johanna]`  \n",
    "- `city` : `[New York, Haifa, München, London, Palo Alto,  Tel Aviv, Kiel, Hamburg]`  \n",
    "- `secondname`: any value  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "my_path = \"/Users/eyalmichaeli/Desktop/School/Master's\"\n",
    "# my_path = \"/Users/mymac\"\n",
    "exercise_path = \"IDC_masters/big_data_platforms_ex2\"\n",
    "\n",
    "path = Path(my_path) / Path(exercise_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 20 CSV files\n"
     ]
    }
   ],
   "source": [
    "firstname = ['John', 'Dana', 'Scott', 'Marc', 'Steven', 'Michael', 'Albert', 'Johanna']\n",
    "city = ['NewYork', 'Haifa', 'Munchen', 'London', 'PaloAlto', 'TelAviv', 'Kiel', 'Hamburg']\n",
    "secondname = ['Lennon', 'McCartney', 'Starr', 'Harrison', 'Ono', 'Sutcliffe', 'Epstein', 'Preston']\n",
    "\n",
    "csvs_path = Path(path / \"csvs\")\n",
    "csvs_path.mkdir(parents=True, exist_ok=True)\n",
    "for i in range(1, 21):\n",
    "    temp_df = pd.DataFrame({\"firstname\": np.random.choice(firstname, 10),\n",
    "                            \"secondname\": np.random.choice(secondname, 10),\n",
    "                            \"city\": np.random.choice(city, 10),\n",
    "                            })\n",
    "    temp_df.to_csv(str(csvs_path / f\"myCSV[{i}].csv\"))\n",
    "\n",
    "print(\"Created 20 CSV files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use python to Create `mapreducetemp` and `mapreducefinal` folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created folders\n"
     ]
    }
   ],
   "source": [
    "mapreducetemp_folder = Path(path / \"mapreducetemp\")\n",
    "mapreducetemp_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "mapreducefinal_folder = Path(path / \"mapreducefinal\")\n",
    "mapreducefinal_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Created folders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "# Question 2\n",
    "## MapReduceEngine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write Python code to create an SQLite database with the following table\n",
    "\n",
    "`TableName: temp_results`   \n",
    "`schema: (key:TEXT,value:TEXT)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-90-c0c9da703476>\", line 5, in <module>\n",
      "    cursor.execute(\"\"\"CREATE TABLE temp_results (\n",
      "sqlite3.DatabaseError: file is not a database\n"
     ]
    }
   ],
   "source": [
    "\n",
    "conn = None\n",
    "try:\n",
    "    conn = sqlite3.connect(str(path / \"db.db\"))\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"\"\"CREATE TABLE temp_results (\n",
    "                   key TEXT\n",
    "                   value TEXT\n",
    "                   );\"\"\")\n",
    "\n",
    "except Exception as e:\n",
    "    traceback.print_exc()\n",
    "\n",
    "# finally:\n",
    "#     cursor.close()\n",
    "#     if conn:\n",
    "#         conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Create a Python class** `MapReduceEngine` with method `def execute(input_data, map_function, reduce_function)`, such that:\n",
    "    - `input_data`: is an array of elements\n",
    "    - `map_function`: is a pointer to the Python function that returns a list where each entry of the form (key,value) \n",
    "    - `reduce_function`: is pointer to the Python function that returns a list where each entry of the form (key,value)\n",
    "\n",
    "<br><br>\n",
    "\n",
    "**Implement** the following functionality in the `execute(...)` function:\n",
    "\n",
    "<br>\n",
    "\n",
    "1. For each key  from the  input_data, start a new Python thread that executes map_function(key) \n",
    "<br><br>\n",
    "2. Each thread will store results of the map_function into mapreducetemp/part-tmp-X.csv where X is a unique number per each thread. \n",
    "<br><br>\n",
    "3. Keep the list of all threads and check whether they are completed.\n",
    "<br><br>\n",
    "4. Once all threads completed, load content of all CSV files into the temp_results table in SQLite.\n",
    "\n",
    "    Remark: Easiest way to loop over all CSV files and load them into Pandas first, then load into SQLite  \n",
    "    `data = pd.read_csv(path to csv)`  \n",
    "    `data.to_sql(‘temp_results’,sql_conn, if_exists=’append’,index=False)`\n",
    "<br><br>\n",
    "\n",
    "5. **Write SQL statement** that generates a sorted list by key of the form `(key, value)` where value is concatenation of ALL values in the value column that match specific key. For example, if table has records\n",
    "<table>\n",
    "    <tbody>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center\">John</td>\n",
    "                <td style=\"text-align:center\">myCSV1.csv</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center\">Dana</td>\n",
    "                <td style=\"text-align:center\">myCSV5.csv</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center\">John</td>\n",
    "                <td style=\"text-align:center\">myCSV7.csv</td>\n",
    "            </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "    Then SQL statement will return `(‘John’,’myCSV1.csv, myCSV7.csv’)`  \n",
    "    Remark: use GROUP_CONCAT and also GROUP BY ORDER BY\n",
    "<br><br><br>\n",
    "6. **Start a new thread** for each value from the generated list in the previous step, to execute `reduce_function(key,value)` \n",
    "<br>    \n",
    "7. Each thread will store results of reduce_function into `mapreducefinal/part-X-final.csv` file  \n",
    "<br>\n",
    "8. Keep list of all threads and check whether they are completed  \n",
    "<br>\n",
    "9. Once all threads completed, print on the screen `MapReduce Completed` otherwise print `MapReduce Failed` \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "# implement all of the class here\n",
    "def map_function(key):\n",
    "    logging.info(\"Thread %s: starting\", key)\n",
    "    logging.info(\"Thread %s: finishing\", key)\n",
    "    return key\n",
    "\n",
    "\n",
    "class MapReduceEngine:\n",
    "\n",
    "    def __init__(self, conn):\n",
    "        self.conn = conn\n",
    "\n",
    "    def execute(self, input_data, map_function, reduce_function):\n",
    "        thread_list = list()\n",
    "        csvs_paths = list()\n",
    "        for csv_key in input_data:\n",
    "            temp_df = pd.read_csv(csv_key)\n",
    "\n",
    "            exec = concurrent.futures.ThreadPoolExecutor()\n",
    "\n",
    "            csv_index = input_data.index(csv_key)  # an index of the relative csv in the input_array\n",
    "            t = exec.submit(map_function, temp_df, csv_index)\n",
    "            threads_returns = t.result()\n",
    "            csv_path = f'{mapreducetemp_folder}/part-tmp-{csv_index}.csv'\n",
    "            csvs_paths.append(csv_path)\n",
    "            pd.DataFrame(threads_returns).to_csv(csv_path,\n",
    "                                                 header=['key', 'value'],\n",
    "                                                 index=False)\n",
    "            thread_list.append(t)\n",
    "\n",
    "\n",
    "        # wait until the threads are completed\n",
    "        exec.shutdown(wait=True)\n",
    "\n",
    "        # Once all threads completed, load content of all CSV files into the temp_results table in Sqlite\n",
    "        for path_to_csv in csvs_paths:\n",
    "            data = pd.read_csv(path_to_csv)\n",
    "            data.to_sql('temp_results', self.conn, if_exists='append', index=False)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "def inverted_map(key, index):\n",
    "    tuple_df = key['firstname'] # take only firstname as a key\n",
    "    tuple_df = tuple_df.reset_index()\n",
    "    tuple_df['value'] = ['myCSV'+str(index)+'.csv'] * len(key['firstname']) # create filname for each key\n",
    "\n",
    "    tuple_df = tuple_df[[\"firstname\", \"value\"]] # slice the dataframe into firstname and value only\n",
    "    tuple_df = tuple_df.set_index('firstname') # set index to firstname in order to generate key-value list\n",
    "\n",
    "    tuple_list = tuple_df.reset_index().T.reset_index().T.values.tolist() # make a list of (name,value) list\n",
    "    tuple_list = tuple_list[1::] # cut 'row' and 'column' headline\n",
    "    tuple_list = list(map(tuple,tuple_list)) # convert list of lists into list of tuples\n",
    "\n",
    "    return tuple_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "ename": "ProgrammingError",
     "evalue": "Cannot operate on a closed database.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mProgrammingError\u001B[0m                          Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-89-a103ccd21e37>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[0mmapreduce\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mMapReduceEngine\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconn\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mconn\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 9\u001B[0;31m \u001B[0mstatus\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmapreduce\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexecute\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput_data\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minverted_map\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minverted_map\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     10\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstatus\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-87-176ab2b3be9c>\u001B[0m in \u001B[0;36mexecute\u001B[0;34m(self, input_data, map_function, reduce_function)\u001B[0m\n\u001B[1;32m     36\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mpath_to_csv\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mcsvs_paths\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     37\u001B[0m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread_csv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath_to_csv\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 38\u001B[0;31m             \u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto_sql\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'temp_results'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mif_exists\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'append'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mindex\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     39\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     40\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\u001B[0m in \u001B[0;36mto_sql\u001B[0;34m(self, name, con, schema, if_exists, index, index_label, chunksize, dtype, method)\u001B[0m\n\u001B[1;32m   2603\u001B[0m         \u001B[0;32mfrom\u001B[0m \u001B[0mpandas\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mio\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0msql\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2604\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2605\u001B[0;31m         sql.to_sql(\n\u001B[0m\u001B[1;32m   2606\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2607\u001B[0m             \u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/sql.py\u001B[0m in \u001B[0;36mto_sql\u001B[0;34m(frame, name, con, schema, if_exists, index, index_label, chunksize, dtype, method)\u001B[0m\n\u001B[1;32m    587\u001B[0m         )\n\u001B[1;32m    588\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 589\u001B[0;31m     pandas_sql.to_sql(\n\u001B[0m\u001B[1;32m    590\u001B[0m         \u001B[0mframe\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    591\u001B[0m         \u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/sql.py\u001B[0m in \u001B[0;36mto_sql\u001B[0;34m(self, frame, name, if_exists, index, index_label, schema, chunksize, dtype, method)\u001B[0m\n\u001B[1;32m   1825\u001B[0m             \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1826\u001B[0m         )\n\u001B[0;32m-> 1827\u001B[0;31m         \u001B[0mtable\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcreate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1828\u001B[0m         \u001B[0mtable\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minsert\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mchunksize\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmethod\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1829\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/sql.py\u001B[0m in \u001B[0;36mcreate\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    719\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    720\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mcreate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 721\u001B[0;31m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexists\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    722\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mif_exists\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m\"fail\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    723\u001B[0m                 \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"Table '{self.name}' already exists.\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/sql.py\u001B[0m in \u001B[0;36mexists\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    706\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    707\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mexists\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 708\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpd_sql\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhas_table\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mschema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    709\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    710\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0msql_schema\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/sql.py\u001B[0m in \u001B[0;36mhas_table\u001B[0;34m(self, name, schema)\u001B[0m\n\u001B[1;32m   1836\u001B[0m         \u001B[0mquery\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34mf\"SELECT name FROM sqlite_master WHERE type='table' AND name={wld};\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1837\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1838\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexecute\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mquery\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfetchall\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1839\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1840\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mget_table\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtable_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/sql.py\u001B[0m in \u001B[0;36mexecute\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1677\u001B[0m             \u001B[0mcur\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcon\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1678\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1679\u001B[0;31m             \u001B[0mcur\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcon\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcursor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1680\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1681\u001B[0m             \u001B[0mcur\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexecute\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mProgrammingError\u001B[0m: Cannot operate on a closed database."
     ]
    }
   ],
   "source": [
    "engine = MapReduceEngine(conn=conn)\n",
    "\n",
    "input_data = []\n",
    "for i in range(1, 21):   # generate input data as file names from 0 to 19\n",
    "    csv_file = 'myCSV['+ str(i)+'].csv'\n",
    "    input_data.append(str(csvs_path / f\"myCSV[{i}].csv\"))\n",
    "\n",
    "mapreduce = MapReduceEngine(conn=conn)\n",
    "status = mapreduce.execute(input_data, inverted_map, inverted_map)\n",
    "print(status)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement all of the class here\n",
    "def map_function(key):\n",
    "    logging.info(\"Thread %s: starting\", key)\n",
    "    logging.info(\"Thread %s: finishing\", key)\n",
    "    return key\n",
    "\n",
    "\n",
    "class MapReduceEngine():\n",
    "\n",
    "    def __init__(self, conn):\n",
    "        self.conn = conn\n",
    "\n",
    "    def execute(self, input_data, map_function, reduce_function):\n",
    "\n",
    "        thread_list, qs = [], []\n",
    "        for num, key in enumerate(input_data):\n",
    "            logging.info(\"Main    : before creating thread\")\n",
    "            que = Queue()\n",
    "            qs.append(que)\n",
    "            t = Thread(target=lambda q, arg1: q.put(map_function(arg1)), args=(que, key))\n",
    "            logging.info(\"Main    : before running thread\")\n",
    "            t.start()\n",
    "            thread_list.append(t)\n",
    "\n",
    "        for thread in thread_list:\n",
    "            logging.info(\"Main    : wait for the thread to finish\")\n",
    "            thread.join()\n",
    "\n",
    "        csvs_paths = list()\n",
    "        for que in qs:\n",
    "            while not que.empty():\n",
    "                result = que.get()\n",
    "                print(result)\n",
    "                csv_path = f'{mapreducetemp_folder}/part-tmp-{num}.csv'\n",
    "                #result.to_csv(csv_path)\n",
    "                csvs_paths.append(csv_path)\n",
    "\n",
    "        # Once all threads completed, load content of all CSV files into the temp_results table in Sqlite\n",
    "        for path_to_csv in csvs_paths:\n",
    "            data = pd.read_csv(path_to_csv)\n",
    "            data.to_sql('temp_results', self.conn, if_exists='append', index=False)\n",
    "\n",
    "        logging.info(\"Main    : all done\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "MapReduceEngine.execute(list(range(50)), MapReduceEngine.map_function, MapReduceEngine.map_function)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:54:21: Main    : before creating thread\n",
      "17:54:21: Main    : before running thread\n",
      "17:54:21: Thread 1: starting\n",
      "17:54:21: Main    : wait for the thread to finish\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:54:23: Thread 1: finishing\n",
      "17:54:23: Main    : all done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import threading\n",
    "import time\n",
    "\n",
    "def thread_function(name):\n",
    "    logging.info(\"Thread %s: starting\", name)\n",
    "    print(name)\n",
    "    time.sleep(2.5)\n",
    "    print(name + 3)\n",
    "    logging.info(\"Thread %s: finishing\", name)\n",
    "    return name\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    format = \"%(asctime)s: %(message)s\"\n",
    "    logging.basicConfig(format=format, level=logging.INFO,\n",
    "                        datefmt=\"%H:%M:%S\")\n",
    "\n",
    "    logging.info(\"Main    : before creating thread\")\n",
    "    x = threading.Thread(target=thread_function, args=(1,))\n",
    "    logging.info(\"Main    : before running thread\")\n",
    "    \n",
    "    \n",
    "    x.start()\n",
    "    logging.info(\"Main    : wait for the thread to finish\")\n",
    "    x.join()\n",
    "#     print(num)\n",
    "    logging.info(\"Main    : all done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:46:15: Main    : all done\n",
      "18:46:15: Main    : before creating thread\n",
      "18:46:15: Main    : before running thread\n",
      "Exception in thread 18:46:15: Main    : before creating thread\n",
      "Thread-48:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mymac/opt/anaconda3/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    18:46:15: Main    : before running thread\n",
      "self.run()\n",
      "  File \"/Users/mymac/opt/anaconda3/lib/python3.8/threading.py\", line 870, in run\n",
      "Exception in thread Thread-49:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mymac/opt/anaconda3/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "18:46:15: Main    : before creating thread\n",
      "        self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-56-11a5a5aa93c6>\", line 7, in <lambda>\n",
      "18:46:15: Main    : before running thread\n",
      "self.run()\n",
      "  File \"/Users/mymac/opt/anaconda3/lib/python3.8/threading.py\", line 870, in run\n",
      "Exception in thread 18:46:15: Main    : before creating thread\n",
      "Thread-50:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mymac/opt/anaconda3/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "        self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-56-11a5a5aa93c6>\", line 7, in <lambda>\n",
      "self.run()\n",
      "  File \"/Users/mymac/opt/anaconda3/lib/python3.8/threading.py\", line 870, in run\n",
      "18:46:15: Main    : before running thread\n",
      "TypeError    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-56-11a5a5aa93c6>\", line 7, in <lambda>\n",
      "Exception in thread : 18:46:15: Main    : before creating thread\n",
      "Thread-51:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mymac/opt/anaconda3/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "map_function() missing 1 required positional argument: 'num'\n",
      "TypeErrorTypeError: map_function() missing 1 required positional argument: 'num'\n",
      ": map_function() missing 1 required positional argument: 'num'\n",
      "18:46:15: Main    : before running thread\n",
      "    self.run()\n",
      "  File \"/Users/mymac/opt/anaconda3/lib/python3.8/threading.py\", line 870, in run\n",
      "Exception in thread Thread-52:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mymac/opt/anaconda3/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "18:46:15: Main    : before creating thread\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-56-11a5a5aa93c6>\", line 7, in <lambda>\n",
      "    18:46:15: Main    : before running thread\n",
      "TypeErrorself.run()\n",
      "  File \"/Users/mymac/opt/anaconda3/lib/python3.8/threading.py\", line 870, in run\n",
      ": map_function() missing 1 required positional argument: 'num'\n",
      "Exception in thread 18:46:15: Main    : wait for the thread to finish\n",
      "Thread-53:\n",
      "Traceback (most recent call last):\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-56-11a5a5aa93c6>\", line 7, in <lambda>\n",
      "  File \"/Users/mymac/opt/anaconda3/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "18:46:15: Main    : wait for the thread to finish\n",
      "    self.run()\n",
      "  File \"/Users/mymac/opt/anaconda3/lib/python3.8/threading.py\", line 870, in run\n",
      "TypeError: map_function() missing 1 required positional argument: 'num'\n",
      "18:46:15: Main    : wait for the thread to finish\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-56-11a5a5aa93c6>\", line 7, in <lambda>\n",
      "18:46:15: Main    : wait for the thread to finish\n",
      "TypeError: map_function() missing 1 required positional argument: 'num'\n",
      "18:46:15: Main    : wait for the thread to finish\n",
      "18:46:15: Main    : wait for the thread to finish\n"
     ]
    }
   ],
   "source": [
    "    def execute(input_data):#, map_function, reduce_function):\n",
    "        thread_list, qs = [], []\n",
    "        for num, key in enumerate(input_data):\n",
    "            logging.info(\"Main    : before creating thread\")\n",
    "            que = Queue()\n",
    "            qs.append(que)\n",
    "            t = threading.Thread(target=lambda q, arg1: q.put(map_function(arg1)), args=(que, key \n",
    "                                                                                        ))\n",
    "            logging.info(\"Main    : before running thread\")\n",
    "            t.start()\n",
    "            thread_list.append(t)\n",
    "        for thread in thread_list:\n",
    "            logging.info(\"Main    : wait for the thread to finish\")\n",
    "            thread.join()\n",
    "        for que in qs:\n",
    "            while not que.empty():\n",
    "                result = que.get()\n",
    "#                 result.to_csv(f'{mapreducetemp_folder}/part-tmp-{num}.csv')\n",
    "                print(result)\n",
    "\n",
    "    logging.info(\"Main    : all done\")\n",
    "            \n",
    "\n",
    "            \n",
    "    def map_function(key):\n",
    "        logging.info(\"Thread %s: starting\", key)\n",
    "        logging.info(\"Thread %s: finishing\", key)\n",
    "        return key + 2\n",
    "        \n",
    "    execute([1,2,3,4,5,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Thread(Thread-12, stopped 123145410392064)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "\n",
    "# Question 3\n",
    "## Implement the MapReduce Inverted index of the JSON documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function `inverted_map(document_name)` which reads the CSV document from the local disc and return a list that contains entries of the form (key_value, document name).\n",
    "\n",
    "For example, if myCSV4.csv document has values like:  \n",
    "`{‘firstname’:’John’,‘secondname’:’Rambo’,‘city’:’Palo Alto’}`\n",
    "\n",
    "Then `inverted_map(‘myCSV4.csv’)` function will return a list:  \n",
    "`[(‘firstname_John’,’ myCSV4.csv’),(‘secondname_Rambo’,’ myCSV4.csv’), (‘city_Palo Alto’,’ myCSV4.csv’)]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverted_map(document_name):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a reduce function `inverted_reduce(value, documents)`, where the field “documents” contains a list of all CSV documents per given value.   \n",
    "This list might have duplicates.   \n",
    "Reduce function will return new list without duplicates.\n",
    "\n",
    "For example,  \n",
    "calling the function `inverted_reduce(‘firstname_Albert’,’myCSV2.csv, myCSV5.csv,myCSV2.csv’)`   \n",
    "will return a list `[‘firstname_Albert’,’myCSV2.csv, myCSV5.csv,myCSV2.csv’]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverted_reduce(value, documents):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "# Question 4\n",
    "## Testing Your MapReduce\n",
    "\n",
    "**Create Python list** `input_data` : `[‘myCSV1.csv’,.. ,‘myCSV20.csv’]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Submit MapReduce as follows:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapreduce = MapReduceEngine()\n",
    "status = mapreduce.execute(input_data, inverted_map, inverted_reduce)\n",
    "print(status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that `MapReduce Completed` should be printed and `mapreducefinal` folder should contain the result files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use python to delete all temporary data from mapreducetemp folder and delete SQLite database:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "\n",
    "# Question 5\n",
    "# Final Thoughts\n",
    "\n",
    "The phase where `MapReduceEngine` reads all temporary files generated by maps and sort them to provide each reducer a specific key is called the **shuffle step**.\n",
    "\n",
    "Please explain **clearly** what would be the main problem of MapReduce when processing Big Data, if there is no shuffle step at all, meaning reducers will directly read responses from the mappers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "            If you say \"I dont know\" you will get 2 points :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "Good Luck :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
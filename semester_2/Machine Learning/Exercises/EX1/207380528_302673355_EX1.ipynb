{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-35def0d0f4b47a0a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Exercise 1: Linear Regression\n",
    "\n",
    "### This notebook is executed automatically. Failing to meet any of the submission requirements will results in a 25 point fine or your submission not being graded at all. Kindly reminder: the homework assignments grade is 50% of the final grade. \n",
    "\n",
    "### Make sure you restart the notebook and check the filename before submission. Appeals based on wrong filenames and errors due to syntax and execution errors will not be accepted.\n",
    "\n",
    "### Do not start the exercise until you fully understand the submission guidelines.\n",
    "\n",
    "## Read the following instructions carefully:\n",
    "\n",
    "1. This jupyter notebook contains all the step by step instructions needed for this exercise.\n",
    "1. Submission includes this notebook only with the exercise number and your ID as the filename. For example: `hw1_123456789_987654321.ipynb` if you submitted in pairs and `hw1_123456789.ipynb` if you submitted the exercise alone.\n",
    "1. Write **efficient vectorized** code whenever possible. Some calculations in this exercise take several minutes when implemented efficiently, and might take much longer otherwise. Unnecessary loops will result in point deduction.\n",
    "1. You are responsible for the correctness of your code and should add as many tests as you see fit. Tests will not be graded nor checked.\n",
    "1. Write your functions in this notebook only. **Do not create Python modules and import them**.\n",
    "1. You are allowed to use functions and methods from the [Python Standard Library](https://docs.python.org/3/library/) and [numpy](https://www.numpy.org/devdocs/reference/) only. **Do not import anything else.**\n",
    "1. Your code must run without errors. Make sure your `numpy` version is at least 1.15.4 and that you are using at least python 3.6. Changes of the configuration we provided are at your own risk. Any code that cannot run will not be graded.\n",
    "1. Write your own code. Cheating will not be tolerated.\n",
    "1. Answers to qualitative questions should be written in **markdown** cells (with $\\LaTeX$ support). Answers that will be written in commented code blocks will not be checked.\n",
    "\n",
    "## In this exercise you will perform the following:\n",
    "1. Load a dataset and perform basic data exploration using a powerful data science library called [pandas](https://pandas.pydata.org/pandas-docs/stable/).\n",
    "1. Preprocess the data for linear regression.\n",
    "1. Compute the cost and perform gradient descent in pure numpy in vectorized form.\n",
    "1. Fit a linear regression model using a single feature.\n",
    "1. Visualize your results using matplotlib.\n",
    "1. Perform multivariate linear regression.\n",
    "1. Perform polynomial regression.\n",
    "1. Experiment with adaptive learning rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I have read and understood the instructions: *** YOUR ID HERE ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5ed0076cec86f623",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np # used for scientific computing\n",
    "import pandas as pd # used for data analysis and manipulation\n",
    "import matplotlib.pyplot as plt # used for visualization and plotting\n",
    "\n",
    "np.random.seed(42) \n",
    "\n",
    "# make matplotlib figures appear inline in the notebook\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (14.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-916f46de8cde2ca7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Part 1: Data Preprocessing (10 Points)\n",
    "\n",
    "For the following exercise, we will use a dataset containing housing prices in King County, USA. The dataset contains 5,000 observations with 18 features and a single target value - the house price. \n",
    "\n",
    "First, we will read and explore the data using pandas and the `.read_csv` method. Pandas is an open source library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9ef8b2769c2c1949",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Read comma separated data\n",
    "df = pd.read_csv('data.csv') # Make sure this cell runs regardless of your absolute path.\n",
    "# df stands for dataframe, which is the default format for datasets in pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6966afc155aa6616",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Data Exploration\n",
    "A good practice in any data-oriented project is to first try and understand the data. Fortunately, pandas is built for that purpose. Start by looking at the top of the dataset using the `df.head()` command. This will be the first indication that you read your data properly, and that the headers are correct. Next, you can use `df.describe()` to show statistics on the data and check for trends and irregularities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>condition</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7129300520</td>\n",
       "      <td>20141013T000000</td>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180</td>\n",
       "      <td>5650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1180</td>\n",
       "      <td>1955</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "      <td>1340</td>\n",
       "      <td>5650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6414100192</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570</td>\n",
       "      <td>7242</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2170</td>\n",
       "      <td>1951</td>\n",
       "      <td>1991</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "      <td>1690</td>\n",
       "      <td>7639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5631500400</td>\n",
       "      <td>20150225T000000</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770</td>\n",
       "      <td>10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>770</td>\n",
       "      <td>1933</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>2720</td>\n",
       "      <td>8062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2487200875</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>604000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1960</td>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "      <td>1360</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1954400510</td>\n",
       "      <td>20150218T000000</td>\n",
       "      <td>510000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1680</td>\n",
       "      <td>8080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1680</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1800</td>\n",
       "      <td>7503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id             date     price  bedrooms  bathrooms  sqft_living  \\\n",
       "0  7129300520  20141013T000000  221900.0         3       1.00         1180   \n",
       "1  6414100192  20141209T000000  538000.0         3       2.25         2570   \n",
       "2  5631500400  20150225T000000  180000.0         2       1.00          770   \n",
       "3  2487200875  20141209T000000  604000.0         4       3.00         1960   \n",
       "4  1954400510  20150218T000000  510000.0         3       2.00         1680   \n",
       "\n",
       "   sqft_lot  floors  waterfront  view  condition  grade  sqft_above  yr_built  \\\n",
       "0      5650     1.0           0     0          3      7        1180      1955   \n",
       "1      7242     2.0           0     0          3      7        2170      1951   \n",
       "2     10000     1.0           0     0          3      6         770      1933   \n",
       "3      5000     1.0           0     0          5      7        1050      1965   \n",
       "4      8080     1.0           0     0          3      8        1680      1987   \n",
       "\n",
       "   yr_renovated  zipcode      lat     long  sqft_living15  sqft_lot15  \n",
       "0             0    98178  47.5112 -122.257           1340        5650  \n",
       "1          1991    98125  47.7210 -122.319           1690        7639  \n",
       "2             0    98028  47.7379 -122.233           2720        8062  \n",
       "3             0    98136  47.5208 -122.393           1360        5000  \n",
       "4             0    98074  47.6168 -122.045           1800        7503  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-5bd0d6844b64ea1a",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>condition</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.000000e+03</td>\n",
       "      <td>5.000000e+03</td>\n",
       "      <td>5000.0000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5.000000e+03</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.00000</td>\n",
       "      <td>5000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.630823e+09</td>\n",
       "      <td>5.394699e+05</td>\n",
       "      <td>3.3714</td>\n",
       "      <td>2.062150</td>\n",
       "      <td>2061.036800</td>\n",
       "      <td>1.615893e+04</td>\n",
       "      <td>1.432600</td>\n",
       "      <td>0.009400</td>\n",
       "      <td>0.243000</td>\n",
       "      <td>3.455000</td>\n",
       "      <td>7.595200</td>\n",
       "      <td>1753.151000</td>\n",
       "      <td>1966.660800</td>\n",
       "      <td>95.052800</td>\n",
       "      <td>98078.812600</td>\n",
       "      <td>47.559312</td>\n",
       "      <td>-122.215864</td>\n",
       "      <td>1976.84520</td>\n",
       "      <td>13451.164600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.870890e+09</td>\n",
       "      <td>3.873115e+05</td>\n",
       "      <td>0.9104</td>\n",
       "      <td>0.773592</td>\n",
       "      <td>923.727509</td>\n",
       "      <td>4.600220e+04</td>\n",
       "      <td>0.510793</td>\n",
       "      <td>0.096506</td>\n",
       "      <td>0.774643</td>\n",
       "      <td>0.677692</td>\n",
       "      <td>1.166537</td>\n",
       "      <td>818.390844</td>\n",
       "      <td>28.286855</td>\n",
       "      <td>425.234932</td>\n",
       "      <td>54.126332</td>\n",
       "      <td>0.139521</td>\n",
       "      <td>0.141807</td>\n",
       "      <td>674.73601</td>\n",
       "      <td>26514.749009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000102e+06</td>\n",
       "      <td>7.500000e+04</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>380.000000</td>\n",
       "      <td>6.090000e+02</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>380.000000</td>\n",
       "      <td>1900.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>98001.000000</td>\n",
       "      <td>47.155900</td>\n",
       "      <td>-122.514000</td>\n",
       "      <td>620.00000</td>\n",
       "      <td>660.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.154075e+09</td>\n",
       "      <td>3.179062e+05</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1410.000000</td>\n",
       "      <td>5.400000e+03</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1190.000000</td>\n",
       "      <td>1949.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>98033.000000</td>\n",
       "      <td>47.463675</td>\n",
       "      <td>-122.329000</td>\n",
       "      <td>1490.00000</td>\n",
       "      <td>5391.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.022900e+09</td>\n",
       "      <td>4.490000e+05</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1890.000000</td>\n",
       "      <td>7.875000e+03</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1530.000000</td>\n",
       "      <td>1968.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>98070.000000</td>\n",
       "      <td>47.572850</td>\n",
       "      <td>-122.235000</td>\n",
       "      <td>1820.00000</td>\n",
       "      <td>7800.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.345078e+09</td>\n",
       "      <td>6.500000e+05</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2500.000000</td>\n",
       "      <td>1.123400e+04</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2130.000000</td>\n",
       "      <td>1990.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>98118.000000</td>\n",
       "      <td>47.679200</td>\n",
       "      <td>-122.129000</td>\n",
       "      <td>2340.00000</td>\n",
       "      <td>10469.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.842300e+09</td>\n",
       "      <td>7.060000e+06</td>\n",
       "      <td>9.0000</td>\n",
       "      <td>6.750000</td>\n",
       "      <td>10040.000000</td>\n",
       "      <td>1.651359e+06</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>7680.000000</td>\n",
       "      <td>2015.000000</td>\n",
       "      <td>2015.000000</td>\n",
       "      <td>98199.000000</td>\n",
       "      <td>47.777600</td>\n",
       "      <td>-121.315000</td>\n",
       "      <td>5790.00000</td>\n",
       "      <td>434728.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id         price   bedrooms    bathrooms   sqft_living  \\\n",
       "count  5.000000e+03  5.000000e+03  5000.0000  5000.000000   5000.000000   \n",
       "mean   4.630823e+09  5.394699e+05     3.3714     2.062150   2061.036800   \n",
       "std    2.870890e+09  3.873115e+05     0.9104     0.773592    923.727509   \n",
       "min    1.000102e+06  7.500000e+04     0.0000     0.000000    380.000000   \n",
       "25%    2.154075e+09  3.179062e+05     3.0000     1.500000   1410.000000   \n",
       "50%    4.022900e+09  4.490000e+05     3.0000     2.000000   1890.000000   \n",
       "75%    7.345078e+09  6.500000e+05     4.0000     2.500000   2500.000000   \n",
       "max    9.842300e+09  7.060000e+06     9.0000     6.750000  10040.000000   \n",
       "\n",
       "           sqft_lot       floors   waterfront         view    condition  \\\n",
       "count  5.000000e+03  5000.000000  5000.000000  5000.000000  5000.000000   \n",
       "mean   1.615893e+04     1.432600     0.009400     0.243000     3.455000   \n",
       "std    4.600220e+04     0.510793     0.096506     0.774643     0.677692   \n",
       "min    6.090000e+02     1.000000     0.000000     0.000000     1.000000   \n",
       "25%    5.400000e+03     1.000000     0.000000     0.000000     3.000000   \n",
       "50%    7.875000e+03     1.000000     0.000000     0.000000     3.000000   \n",
       "75%    1.123400e+04     2.000000     0.000000     0.000000     4.000000   \n",
       "max    1.651359e+06     3.500000     1.000000     4.000000     5.000000   \n",
       "\n",
       "             grade   sqft_above     yr_built  yr_renovated       zipcode  \\\n",
       "count  5000.000000  5000.000000  5000.000000   5000.000000   5000.000000   \n",
       "mean      7.595200  1753.151000  1966.660800     95.052800  98078.812600   \n",
       "std       1.166537   818.390844    28.286855    425.234932     54.126332   \n",
       "min       3.000000   380.000000  1900.000000      0.000000  98001.000000   \n",
       "25%       7.000000  1190.000000  1949.000000      0.000000  98033.000000   \n",
       "50%       7.000000  1530.000000  1968.000000      0.000000  98070.000000   \n",
       "75%       8.000000  2130.000000  1990.000000      0.000000  98118.000000   \n",
       "max      13.000000  7680.000000  2015.000000   2015.000000  98199.000000   \n",
       "\n",
       "               lat         long  sqft_living15     sqft_lot15  \n",
       "count  5000.000000  5000.000000     5000.00000    5000.000000  \n",
       "mean     47.559312  -122.215864     1976.84520   13451.164600  \n",
       "std       0.139521     0.141807      674.73601   26514.749009  \n",
       "min      47.155900  -122.514000      620.00000     660.000000  \n",
       "25%      47.463675  -122.329000     1490.00000    5391.500000  \n",
       "50%      47.572850  -122.235000     1820.00000    7800.000000  \n",
       "75%      47.679200  -122.129000     2340.00000   10469.250000  \n",
       "max      47.777600  -121.315000     5790.00000  434728.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9b9bd1b387905904",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "We will start with one variable linear regression by extracting the target column and the `sqft_living` variable from the dataset. We use pandas and select both columns as separate variables and transform them into a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-c7cd243e8b5fe5aa",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "X = df['sqft_living'].values\n",
    "y = df['price'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-508e7e1a13f9bbe4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Preprocessing\n",
    "\n",
    "As the number of features grows, calculating gradients gets computationally expensive. We can speed this up by normalizing the input data to ensure all values are within the same range. This is especially important for datasets with high standard deviations or differences in the ranges of the attributes. Use [mean normalization](https://en.wikipedia.org/wiki/Feature_scaling) for the fearures (`X`) and the true labels (`y`).\n",
    "\n",
    "Implement the cost function `preprocess`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X, y):\n",
    "    \"\"\"\n",
    "    Perform mean normalization on the features and true labels.\n",
    "\n",
    "    Input:\n",
    "    - X: Inputs (n features over m instances).\n",
    "    - y: True labels.\n",
    "\n",
    "    Returns a two vales:\n",
    "    - X: The mean normalized inputs.\n",
    "    - y: The mean normalized labels.\n",
    "    \"\"\"\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the normalization function.                             #\n",
    "    ###########################################################################\n",
    "\n",
    "    X = (X - X.mean(axis=0)) / (np.max(X, axis=0) - np.min(X, axis=0))\n",
    "    y = (y - y.mean(axis=0)) / (np.max(y, axis=0) - np.min(y, axis=0))\n",
    "    \n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-9bb6a28b6b6932fa",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "X, y = preprocess(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will split the data into two datasets: \n",
    "1. The training dataset will contain 80% of the data and will always be used for model training.\n",
    "2. The validation dataset will contain the remaining 20% of the data and will be used for model evaluation. For example, we will pick the best alpha and the best features using the validation dataset, while still training the model using the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and validation split\n",
    "np.random.seed(42)\n",
    "indices = np.random.permutation(X.shape[0])\n",
    "idx_train, idx_val = indices[:int(0.8*X.shape[0])], indices[int(0.8*X.shape[0]):]\n",
    "X_train, X_val = X[idx_train], X[idx_val]\n",
    "y_train, y_val = y[idx_train], y[idx_val]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0c168d036748663e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Data Visualization\n",
    "Another useful tool is data visualization. Since this problem has only two parameters, it is possible to create a two-dimensional scatter plot to visualize the data. Note that many real-world datasets are highly dimensional and cannot be visualized naively. We will be using `matplotlib` for all data visualization purposes since it offers a wide range of visualization tools and is easy to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cbad8871e083093f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X_train, y_train, 'ro', ms=1, mec='k') # the parameters control the size, shape and color of the scatter plot\n",
    "plt.ylabel('Price in USD')\n",
    "plt.xlabel('sq.ft')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c50f0a0e569142ed",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Bias Trick\n",
    "\n",
    "Make sure that `X` takes into consideration the bias $\\theta_0$ in the linear model. Hint, recall that the predications of our linear model are of the form:\n",
    "\n",
    "$$\n",
    "\\hat{y} = h_\\theta(x) = \\theta^T x = \\theta_0 + \\theta_1 x_1\n",
    "$$\n",
    "\n",
    "Add columns of ones as the zeroth column of the features (do this for both the training and validation sets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-44853962dc1651df",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "#                            START OF YOUR CODE                           #\n",
    "###########################################################################\n",
    "X_train = np.column_stack([np.ones(X_train.shape[0]), X_train])\n",
    "X_val = np.column_stack([np.ones(X_val.shape[0]), X_val])\n",
    "\n",
    "###########################################################################\n",
    "#                             END OF YOUR CODE                            #\n",
    "###########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c7d7fd68c1b24943",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Part 2: Single Variable Linear Regression (40 Points)\n",
    "Simple linear regression is a linear regression model with a single explanatory varaible and a single target value. \n",
    "\n",
    "$$\n",
    "\\hat{y} = h_\\theta(x) = \\theta^T x = \\theta_0 + \\theta_1 x_1\n",
    "$$\n",
    "\n",
    "## Gradient Descent \n",
    "\n",
    "Our task is to find the best possible linear line that explains all the points in our dataset. We start by guessing initial values for the linear regression parameters $\\theta$ and updating the values using gradient descent. \n",
    "\n",
    "The objective of linear regression is to minimize the cost function $J$:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{n}(h_\\theta(x^{(i)})-y^{(i)})^2\n",
    "$$\n",
    "\n",
    "where the hypothesis (model) $h_\\theta(x)$ is given by a **linear** model:\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = \\theta^T x = \\theta_0 + \\theta_1 x_1\n",
    "$$\n",
    "\n",
    "$\\theta_j$ are parameters of your model. and by changing those values accordingly you will be able to lower the cost function $J(\\theta)$. One way to accopmlish this is to use gradient descent:\n",
    "\n",
    "$$\n",
    "\\theta_j = \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)})-y^{(i)})x_j^{(i)}\n",
    "$$\n",
    "\n",
    "In linear regresion, we know that with each step of gradient descent, the parameters $\\theta_j$ get closer to the optimal values that will achieve the lowest cost $J(\\theta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0f83af93c0436542",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Implement the cost function `compute_cost`. (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, theta):\n",
    "    \"\"\"\n",
    "    Computes the average squared difference between an obserbation's actual and\n",
    "    predicted values for linear regression.  \n",
    "\n",
    "    Input:\n",
    "    - X: inputs  (n features over m instances).\n",
    "    - y: true labels (1 value over m instances).\n",
    "    - theta: the parameters (weights) of the model being learned.\n",
    "\n",
    "    Returns a single value:\n",
    "    - J: the cost associated with the current set of parameters (single number).\n",
    "    \"\"\"\n",
    "    \n",
    "    J = 0  # Use J for the cost.\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the MSE cost function.                                  #\n",
    "    ###########################################################################\n",
    "\n",
    "    y_hat = np.matmul(X, theta)\n",
    "    J = np.sum( (y_hat - y)**2 ) / (X.shape[0] * 2.0)\n",
    "    \n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4c1cfec24e144479",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5110382451954535"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta = np.array([-1, 2])\n",
    "J = compute_cost(X_train, y_train, theta)\n",
    "J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-afdc527b73d275bb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Implement the gradient descent function `gradient_descent`. (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, theta, alpha, num_iters):\n",
    "    \"\"\"\n",
    "    Learn the parameters of the model using gradient descent using \n",
    "    the *training set*. Gradient descent is an optimization algorithm \n",
    "    used to minimize some (loss) function by iteratively moving in \n",
    "    the direction of steepest descent as defined by the negative of \n",
    "    the gradient. We use gradient descent to update the parameters\n",
    "    (weights) of our model.\n",
    "\n",
    "    Input:\n",
    "    - X: Inputs  (n features over m instances).\n",
    "    - y: True labels (1 value over m instances).\n",
    "    - theta: The parameters (weights) of the model being learned.\n",
    "    - alpha: The learning rate of your model.\n",
    "    - num_iters: The number of updates performed.\n",
    "\n",
    "    Returns two values:\n",
    "    - theta: The learned parameters of your model.\n",
    "    - J_history: the loss value for every iteration.\n",
    "    \"\"\"\n",
    "    \n",
    "    J_history = [] # Use a python list to save cost in every iteration\n",
    "    theta = theta.copy() # avoid changing the original thetas\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the gradient descent optimization algorithm.            #\n",
    "    ###########################################################################\n",
    "    m = X.shape[0]\n",
    "    for _ in range(num_iters):\n",
    "        # cost\n",
    "        J = compute_cost(X, y, theta)\n",
    "        J_history.append(J)\n",
    "\n",
    "        y_hat = np.matmul(X, theta)\n",
    "\n",
    "        dJ_dtheta = np.matmul(X.T, (y_hat - y)) / m \n",
    "        \n",
    "        theta = theta - alpha * dJ_dtheta\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return theta, J_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-59b95cbea13e7fc1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.94301549e-04, 4.08751964e-01]),\n",
       " [0.07244667608564788,\n",
       "  0.05909008502223921,\n",
       "  0.048270775925842925,\n",
       "  0.03950666558098806,\n",
       "  0.03240726667985371,\n",
       "  0.02665628458113544,\n",
       "  0.021997520688276838,\n",
       "  0.018223454188712393,\n",
       "  0.015165993264983291,\n",
       "  0.012688983577627627,\n",
       "  0.010682140137849601,\n",
       "  0.009056132125619265,\n",
       "  0.007738601593335899,\n",
       "  0.006670938616607401,\n",
       "  0.005805669167039374,\n",
       "  0.005104339289727087,\n",
       "  0.004535801287455525,\n",
       "  0.004074825530251001,\n",
       "  0.0037009760214011104,\n",
       "  0.0033976996061602332,\n",
       "  0.0031515882309867747,\n",
       "  0.002951781373675798,\n",
       "  0.0027894820118883356,\n",
       "  0.0026575645577588676,\n",
       "  0.002550257285007331,\n",
       "  0.0024628850949646253,\n",
       "  0.0023916611571057366,\n",
       "  0.0023335181379240577,\n",
       "  0.0022859714963540103,\n",
       "  0.0022470087530912478,\n",
       "  0.0022149997987645966,\n",
       "  0.002188624243573623,\n",
       "  0.002166812570509875,\n",
       "  0.002148698469478078,\n",
       "  0.0021335802279440355,\n",
       "  0.0021208894573672833,\n",
       "  0.0021101657616179248,\n",
       "  0.002101036218399421,\n",
       "  0.0020931987592052374,\n",
       "  0.002086408707087107,\n",
       "  0.002080467872250066,\n",
       "  0.002075215719486655,\n",
       "  0.0020705222138004136,\n",
       "  0.002066282025362365,\n",
       "  0.0020624098355269526,\n",
       "  0.0020588365347058916,\n",
       "  0.0020555061426467632,\n",
       "  0.0020523733138593037,\n",
       "  0.002049401317011191,\n",
       "  0.0020465603982390505,\n",
       "  0.0020438264554306926,\n",
       "  0.0020411799643939884,\n",
       "  0.0020386051090538794,\n",
       "  0.0020360890769121225,\n",
       "  0.0020336214883698235,\n",
       "  0.002031193934478801,\n",
       "  0.00202879960252028,\n",
       "  0.0020264329727237057,\n",
       "  0.0020240895726090376,\n",
       "  0.002021765778004056,\n",
       "  0.0020194586518684126,\n",
       "  0.0020171658137411413,\n",
       "  0.0020148853339931622,\n",
       "  0.0020126156481718276,\n",
       "  0.002010355487620016,\n",
       "  0.002008103823277611,\n",
       "  0.002005859820160695,\n",
       "  0.0020036228004897162,\n",
       "  0.0020013922138232914,\n",
       "  0.001999167612866599,\n",
       "  0.0019969486338761696,\n",
       "  0.001994734980787769,\n",
       "  0.00199252641235998,\n",
       "  0.0019903227317605056,\n",
       "  0.001988123778131065,\n",
       "  0.001985929419754961,\n",
       "  0.0019837395485228053,\n",
       "  0.0019815540754497527,\n",
       "  0.001979372927044454,\n",
       "  0.0019771960423679074,\n",
       "  0.001975023370651121,\n",
       "  0.0019728548693654173,\n",
       "  0.0019706905026593735,\n",
       "  0.001968530240092741,\n",
       "  0.001966374055610912,\n",
       "  0.001964221926714238,\n",
       "  0.0019620738337851663,\n",
       "  0.001959929759543225,\n",
       "  0.0019577896886035463,\n",
       "  0.0019556536071192723,\n",
       "  0.001953521502491897,\n",
       "  0.0019513933631366406,\n",
       "  0.0019492691782923924,\n",
       "  0.001947148937867766,\n",
       "  0.0019450326323163978,\n",
       "  0.0019429202525359313,\n",
       "  0.0019408117897861926,\n",
       "  0.0019387072356229062,\n",
       "  0.0019366065818440029,\n",
       "  0.001934509820446119,\n",
       "  0.001932416943589363,\n",
       "  0.0019303279435687667,\n",
       "  0.0019282428127911561,\n",
       "  0.0019261615437564188,\n",
       "  0.0019240841290423148,\n",
       "  0.0019220105612921792,\n",
       "  0.00191994083320495,\n",
       "  0.0019178749375270904,\n",
       "  0.0019158128670460363,\n",
       "  0.0019137546145848905,\n",
       "  0.0019117001729981144,\n",
       "  0.0019096495351680369,\n",
       "  0.0019076026940020227,\n",
       "  0.001905559642430172,\n",
       "  0.0019035203734034515,\n",
       "  0.0019014848798921831,\n",
       "  0.0018994531548848048,\n",
       "  0.0018974251913868709,\n",
       "  0.0018954009824202335,\n",
       "  0.0018933805210223763,\n",
       "  0.0018913638002458705,\n",
       "  0.0018893508131579294,\n",
       "  0.0018873415528400423,\n",
       "  0.0018853360123876745,\n",
       "  0.001883334184910021,\n",
       "  0.0018813360635297994,\n",
       "  0.0018793416413830806,\n",
       "  0.0018773509116191448,\n",
       "  0.0018753638674003641,\n",
       "  0.0018733805019020967,\n",
       "  0.0018714008083126053,\n",
       "  0.0018694247798329777,\n",
       "  0.0018674524096770656,\n",
       "  0.001865483691071427,\n",
       "  0.0018635186172552723,\n",
       "  0.0018615571814804238,\n",
       "  0.0018595993770112711,\n",
       "  0.0018576451971247358,\n",
       "  0.0018556946351102352,\n",
       "  0.0018537476842696511,\n",
       "  0.0018518043379172977,\n",
       "  0.0018498645893798935,\n",
       "  0.0018479284319965314,\n",
       "  0.0018459958591186554,\n",
       "  0.0018440668641100315,\n",
       "  0.001842141440346722,\n",
       "  0.0018402195812170627,\n",
       "  0.0018383012801216376,\n",
       "  0.001836386530473253,\n",
       "  0.0018344753256969175,\n",
       "  0.001832567659229814,\n",
       "  0.00183066352452128,\n",
       "  0.001828762915032783,\n",
       "  0.0018268658242378976,\n",
       "  0.0018249722456222827,\n",
       "  0.0018230821726836586,\n",
       "  0.0018211955989317855,\n",
       "  0.0018193125178884403,\n",
       "  0.0018174329230873924,\n",
       "  0.0018155568080743853,\n",
       "  0.0018136841664071106,\n",
       "  0.0018118149916551874,\n",
       "  0.0018099492774001408,\n",
       "  0.001808087017235378,\n",
       "  0.0018062282047661676,\n",
       "  0.0018043728336096176,\n",
       "  0.0018025208973946531,\n",
       "  0.0018006723897619943,\n",
       "  0.0017988273043641335,\n",
       "  0.001796985634865317,\n",
       "  0.0017951473749415186,\n",
       "  0.0017933125182804219,\n",
       "  0.0017914810585813949,\n",
       "  0.001789652989555472,\n",
       "  0.00178782830492533,\n",
       "  0.0017860069984252673,\n",
       "  0.0017841890638011831,\n",
       "  0.0017823744948105534,\n",
       "  0.0017805632852224127,\n",
       "  0.0017787554288173303,\n",
       "  0.0017769509193873913,\n",
       "  0.0017751497507361722,\n",
       "  0.0017733519166787221,\n",
       "  0.0017715574110415404,\n",
       "  0.001769766227662556,\n",
       "  0.0017679783603911066,\n",
       "  0.0017661938030879144,\n",
       "  0.0017644125496250713,\n",
       "  0.0017626345938860104,\n",
       "  0.0017608599297654916,\n",
       "  0.0017590885511695755,\n",
       "  0.0017573204520156067,\n",
       "  0.0017555556262321898,\n",
       "  0.00175379406775917,\n",
       "  0.0017520357705476115,\n",
       "  0.001750280728559778,\n",
       "  0.00174852893576911,\n",
       "  0.0017467803861602068,\n",
       "  0.0017450350737288026,\n",
       "  0.0017432929924817496,\n",
       "  0.0017415541364369936,\n",
       "  0.001739818499623556,\n",
       "  0.0017380860760815126,\n",
       "  0.0017363568598619736,\n",
       "  0.0017346308450270618,\n",
       "  0.0017329080256498933,\n",
       "  0.0017311883958145567,\n",
       "  0.0017294719496160943,\n",
       "  0.001727758681160479,\n",
       "  0.0017260485845645968,\n",
       "  0.0017243416539562237,\n",
       "  0.00172263788347401,\n",
       "  0.0017209372672674548,\n",
       "  0.0017192397994968897,\n",
       "  0.0017175454743334575,\n",
       "  0.0017158542859590916,\n",
       "  0.0017141662285664982,\n",
       "  0.001712481296359133,\n",
       "  0.001710799483551185,\n",
       "  0.0017091207843675533,\n",
       "  0.001707445193043829,\n",
       "  0.0017057727038262748,\n",
       "  0.001704103310971807,\n",
       "  0.0017024370087479733,\n",
       "  0.001700773791432934,\n",
       "  0.0016991136533154431,\n",
       "  0.0016974565886948285,\n",
       "  0.0016958025918809704,\n",
       "  0.0016941516571942854,\n",
       "  0.001692503778965703,\n",
       "  0.0016908589515366504,\n",
       "  0.0016892171692590283,\n",
       "  0.0016875784264951963,\n",
       "  0.0016859427176179496,\n",
       "  0.0016843100370105021,\n",
       "  0.0016826803790664668,\n",
       "  0.0016810537381898348,\n",
       "  0.0016794301087949578,\n",
       "  0.0016778094853065301,\n",
       "  0.0016761918621595651,\n",
       "  0.0016745772337993805,\n",
       "  0.0016729655946815779,\n",
       "  0.001671356939272023,\n",
       "  0.001669751262046827,\n",
       "  0.0016681485574923285,\n",
       "  0.0016665488201050725,\n",
       "  0.0016649520443917942,\n",
       "  0.0016633582248693983,\n",
       "  0.0016617673560649403,\n",
       "  0.001660179432515608,\n",
       "  0.0016585944487687032,\n",
       "  0.0016570123993816229,\n",
       "  0.0016554332789218383,\n",
       "  0.001653857081966881,\n",
       "  0.0016522838031043196,\n",
       "  0.0016507134369317428,\n",
       "  0.001649145978056743,\n",
       "  0.0016475814210968934,\n",
       "  0.0016460197606797333,\n",
       "  0.0016444609914427484,\n",
       "  0.0016429051080333514,\n",
       "  0.001641352105108866,\n",
       "  0.0016398019773365053,\n",
       "  0.001638254719393357,\n",
       "  0.0016367103259663617,\n",
       "  0.0016351687917522976,\n",
       "  0.0016336301114577606,\n",
       "  0.001632094279799146,\n",
       "  0.0016305612915026315,\n",
       "  0.0016290311413041584,\n",
       "  0.0016275038239494139,\n",
       "  0.0016259793341938127,\n",
       "  0.0016244576668024782,\n",
       "  0.0016229388165502256,\n",
       "  0.0016214227782215451,\n",
       "  0.0016199095466105816,\n",
       "  0.0016183991165211178,\n",
       "  0.0016168914827665568,\n",
       "  0.0016153866401699033,\n",
       "  0.0016138845835637467,\n",
       "  0.0016123853077902438,\n",
       "  0.0016108888077010994,\n",
       "  0.0016093950781575499,\n",
       "  0.0016079041140303456,\n",
       "  0.0016064159101997326,\n",
       "  0.0016049304615554354,\n",
       "  0.0016034477629966396,\n",
       "  0.0016019678094319747,\n",
       "  0.0016004905957794945,\n",
       "  0.0015990161169666635,\n",
       "  0.0015975443679303349,\n",
       "  0.0015960753436167372,\n",
       "  0.0015946090389814542,\n",
       "  0.0015931454489894103,\n",
       "  0.0015916845686148493,\n",
       "  0.0015902263928413208,\n",
       "  0.001588770916661662,\n",
       "  0.0015873181350779793,\n",
       "  0.0015858680431016317,\n",
       "  0.0015844206357532146,\n",
       "  0.0015829759080625424,\n",
       "  0.0015815338550686294,\n",
       "  0.0015800944718196762,\n",
       "  0.0015786577533730504,\n",
       "  0.0015772236947952688,\n",
       "  0.0015757922911619846,\n",
       "  0.001574363537557965,\n",
       "  0.0015729374290770776,\n",
       "  0.0015715139608222745,\n",
       "  0.0015700931279055718,\n",
       "  0.0015686749254480374,\n",
       "  0.0015672593485797689,\n",
       "  0.001565846392439882,\n",
       "  0.0015644360521764912,\n",
       "  0.0015630283229466934,\n",
       "  0.001561623199916551,\n",
       "  0.0015602206782610755,\n",
       "  0.0015588207531642127,\n",
       "  0.0015574234198188228,\n",
       "  0.0015560286734266665,\n",
       "  0.0015546365091983877,\n",
       "  0.0015532469223534975,\n",
       "  0.001551859908120356,\n",
       "  0.001550475461736159,\n",
       "  0.0015490935784469187,\n",
       "  0.0015477142535074485,\n",
       "  0.0015463374821813485,\n",
       "  0.0015449632597409858,\n",
       "  0.00154359158146748,\n",
       "  0.0015422224426506882,\n",
       "  0.0015408558385891866,\n",
       "  0.0015394917645902555,\n",
       "  0.0015381302159698637,\n",
       "  0.0015367711880526515,\n",
       "  0.0015354146761719144,\n",
       "  0.0015340606756695883,\n",
       "  0.0015327091818962324,\n",
       "  0.0015313601902110139,\n",
       "  0.0015300136959816913,\n",
       "  0.0015286696945845996,\n",
       "  0.0015273281814046338,\n",
       "  0.0015259891518352327,\n",
       "  0.0015246526012783633,\n",
       "  0.001523318525144506,\n",
       "  0.0015219869188526371,\n",
       "  0.0015206577778302153,\n",
       "  0.0015193310975131623,\n",
       "  0.001518006873345852,\n",
       "  0.0015166851007810903,\n",
       "  0.0015153657752801032,\n",
       "  0.0015140488923125201,\n",
       "  0.0015127344473563546,\n",
       "  0.0015114224358979953,\n",
       "  0.001510112853432185,\n",
       "  0.0015088056954620083,\n",
       "  0.001507500957498875,\n",
       "  0.0015061986350625057,\n",
       "  0.0015048987236809133,\n",
       "  0.0015036012188903917,\n",
       "  0.0015023061162354986,\n",
       "  0.0015010134112690397,\n",
       "  0.0014997230995520544,\n",
       "  0.0014984351766538003,\n",
       "  0.0014971496381517367,\n",
       "  0.0014958664796315122,\n",
       "  0.0014945856966869474,\n",
       "  0.0014933072849200194,\n",
       "  0.0014920312399408484,\n",
       "  0.0014907575573676816,\n",
       "  0.001489486232826879,\n",
       "  0.001488217261952896,\n",
       "  0.0014869506403882725,\n",
       "  0.0014856863637836136,\n",
       "  0.001484424427797578,\n",
       "  0.0014831648280968605,\n",
       "  0.0014819075603561796,\n",
       "  0.0014806526202582606,\n",
       "  0.0014794000034938216,\n",
       "  0.0014781497057615594,\n",
       "  0.0014769017227681332,\n",
       "  0.0014756560502281512,\n",
       "  0.0014744126838641552,\n",
       "  0.0014731716194066058,\n",
       "  0.0014719328525938686,\n",
       "  0.0014706963791721985,\n",
       "  0.0014694621948957254,\n",
       "  0.0014682302955264408,\n",
       "  0.0014670006768341805,\n",
       "  0.0014657733345966135,\n",
       "  0.0014645482645992258,\n",
       "  0.0014633254626353041,\n",
       "  0.0014621049245059253,\n",
       "  0.0014608866460199383,\n",
       "  0.0014596706229939528,\n",
       "  0.0014584568512523224,\n",
       "  0.0014572453266271316,\n",
       "  0.001456036044958181,\n",
       "  0.0014548290020929733,\n",
       "  0.0014536241938866994,\n",
       "  0.0014524216162022231,\n",
       "  0.0014512212649100676,\n",
       "  0.0014500231358884005,\n",
       "  0.0014488272250230225,\n",
       "  0.0014476335282073486,\n",
       "  0.0014464420413423976,\n",
       "  0.0014452527603367779,\n",
       "  0.0014440656811066715,\n",
       "  0.00144288079957582,\n",
       "  0.0014416981116755134,\n",
       "  0.001440517613344574,\n",
       "  0.0014393393005293418,\n",
       "  0.0014381631691836618,\n",
       "  0.0014369892152688705,\n",
       "  0.0014358174347537814,\n",
       "  0.0014346478236146698,\n",
       "  0.0014334803778352616,\n",
       "  0.0014323150934067183,\n",
       "  0.0014311519663276226,\n",
       "  0.0014299909926039649,\n",
       "  0.0014288321682491304,\n",
       "  0.0014276754892838862,\n",
       "  0.0014265209517363635,\n",
       "  0.0014253685516420493,\n",
       "  0.0014242182850437686,\n",
       "  0.0014230701479916738,\n",
       "  0.0014219241365432296,\n",
       "  0.0014207802467631992,\n",
       "  0.0014196384747236313,\n",
       "  0.0014184988165038472,\n",
       "  0.0014173612681904265,\n",
       "  0.0014162258258771931,\n",
       "  0.0014150924856652044,\n",
       "  0.0014139612436627346,\n",
       "  0.0014128320959852629,\n",
       "  0.0014117050387554614,\n",
       "  0.001410580068103178,\n",
       "  0.0014094571801654293,\n",
       "  0.0014083363710863802,\n",
       "  0.0014072176370173355,\n",
       "  0.0014061009741167256,\n",
       "  0.001404986378550092,\n",
       "  0.001403873846490076,\n",
       "  0.0014027633741164054,\n",
       "  0.001401654957615878,\n",
       "  0.001400548593182354,\n",
       "  0.0013994442770167387,\n",
       "  0.0013983420053269703,\n",
       "  0.00139724177432801,\n",
       "  0.0013961435802418226,\n",
       "  0.0013950474192973706,\n",
       "  0.0013939532877305968,\n",
       "  0.0013928611817844124,\n",
       "  0.0013917710977086847,\n",
       "  0.001390683031760223,\n",
       "  0.001389596980202768,\n",
       "  0.0013885129393069761,\n",
       "  0.0013874309053504097,\n",
       "  0.0013863508746175203,\n",
       "  0.0013852728433996402,\n",
       "  0.0013841968079949672,\n",
       "  0.0013831227647085518,\n",
       "  0.0013820507098522858,\n",
       "  0.0013809806397448892,\n",
       "  0.001379912550711897,\n",
       "  0.0013788464390856463,\n",
       "  0.001377782301205265,\n",
       "  0.0013767201334166591,\n",
       "  0.0013756599320724984,\n",
       "  0.001374601693532207,\n",
       "  0.001373545414161946,\n",
       "  0.0013724910903346065,\n",
       "  0.0013714387184297937,\n",
       "  0.0013703882948338164,\n",
       "  0.0013693398159396722,\n",
       "  0.0013682932781470373,\n",
       "  0.0013672486778622533,\n",
       "  0.001366206011498314,\n",
       "  0.0013651652754748565,\n",
       "  0.0013641264662181433,\n",
       "  0.001363089580161055,\n",
       "  0.0013620546137430762,\n",
       "  0.0013610215634102824,\n",
       "  0.0013599904256153292,\n",
       "  0.0013589611968174396,\n",
       "  0.0013579338734823914,\n",
       "  0.001356908452082506,\n",
       "  0.0013558849290966353,\n",
       "  0.0013548633010101508,\n",
       "  0.0013538435643149298,\n",
       "  0.0013528257155093456,\n",
       "  0.0013518097510982522,\n",
       "  0.0013507956675929767,\n",
       "  0.0013497834615113042,\n",
       "  0.0013487731293774655,\n",
       "  0.0013477646677221276,\n",
       "  0.0013467580730823793,\n",
       "  0.001345753342001722,\n",
       "  0.0013447504710300548,\n",
       "  0.0013437494567236647,\n",
       "  0.0013427502956452147,\n",
       "  0.0013417529843637304,\n",
       "  0.0013407575194545904,\n",
       "  0.0013397638974995136,\n",
       "  0.001338772115086546,\n",
       "  0.0013377821688100513,\n",
       "  0.0013367940552706978,\n",
       "  0.0013358077710754481,\n",
       "  0.0013348233128375453,\n",
       "  0.0013338406771765032,\n",
       "  0.0013328598607180938,\n",
       "  0.0013318808600943366,\n",
       "  0.0013309036719434855,\n",
       "  0.0013299282929100187,\n",
       "  0.0013289547196446263,\n",
       "  0.0013279829488041996,\n",
       "  0.001327012977051819,\n",
       "  0.0013260448010567428,\n",
       "  0.001325078417494395,\n",
       "  0.0013241138230463557,\n",
       "  0.0013231510144003477,\n",
       "  0.0013221899882502259,\n",
       "  0.0013212307412959672,\n",
       "  0.0013202732702436563,\n",
       "  0.0013193175718054773,\n",
       "  0.0013183636426997007,\n",
       "  0.0013174114796506732,\n",
       "  0.0013164610793888049,\n",
       "  0.0013155124386505601,\n",
       "  0.0013145655541784442,\n",
       "  0.0013136204227209938,\n",
       "  0.001312677041032765,\n",
       "  0.0013117354058743223,\n",
       "  0.0013107955140122273,\n",
       "  0.001309857362219029,\n",
       "  0.0013089209472732502,\n",
       "  0.0013079862659593781,\n",
       "  0.0013070533150678534,\n",
       "  0.0013061220913950585,\n",
       "  0.0013051925917433062,\n",
       "  0.001304264812920831,\n",
       "  0.0013033387517417743,\n",
       "  0.0013024144050261782,\n",
       "  0.0013014917695999687,\n",
       "  0.0013005708422949515,\n",
       "  0.001299651619948796,\n",
       "  0.001298734099405026,\n",
       "  0.0012978182775130098,\n",
       "  0.0012969041511279485,\n",
       "  0.0012959917171108649,\n",
       "  0.0012950809723285927,\n",
       "  0.001294171913653768,\n",
       "  0.0012932645379648154,\n",
       "  0.001292358842145938,\n",
       "  0.0012914548230871087,\n",
       "  0.0012905524776840578,\n",
       "  0.0012896518028382617,\n",
       "  0.0012887527954569356,\n",
       "  0.0012878554524530176,\n",
       "  0.0012869597707451629,\n",
       "  0.0012860657472577306,\n",
       "  0.0012851733789207747,\n",
       "  0.0012842826626700314,\n",
       "  0.001283393595446911,\n",
       "  0.001282506174198487,\n",
       "  0.0012816203958774825,\n",
       "  0.0012807362574422645,\n",
       "  0.0012798537558568299,\n",
       "  0.0012789728880907973,\n",
       "  0.0012780936511193944,\n",
       "  0.0012772160419234496,\n",
       "  0.0012763400574893802,\n",
       "  0.001275465694809184,\n",
       "  0.0012745929508804262,\n",
       "  0.0012737218227062315,\n",
       "  0.0012728523072952722,\n",
       "  0.001271984401661759,\n",
       "  0.0012711181028254301,\n",
       "  0.001270253407811542,\n",
       "  0.001269390313650857,\n",
       "  0.0012685288173796359,\n",
       "  0.0012676689160396255,\n",
       "  0.0012668106066780506,\n",
       "  0.0012659538863476012,\n",
       "  0.0012650987521064235,\n",
       "  0.0012642452010181123,\n",
       "  0.0012633932301516966,\n",
       "  0.0012625428365816331,\n",
       "  0.0012616940173877933,\n",
       "  0.0012608467696554563,\n",
       "  0.0012600010904752958,\n",
       "  0.0012591569769433734,\n",
       "  0.0012583144261611254,\n",
       "  0.0012574734352353558,\n",
       "  0.0012566340012782234,\n",
       "  0.0012557961214072345,\n",
       "  0.001254959792745232,\n",
       "  0.0012541250124203838,\n",
       "  0.001253291777566177,\n",
       "  0.0012524600853214033,\n",
       "  0.0012516299328301536,\n",
       "  0.0012508013172418044,\n",
       "  0.0012499742357110104,\n",
       "  0.0012491486853976944,\n",
       "  0.0012483246634670362,\n",
       "  0.0012475021670894644,\n",
       "  0.0012466811934406467,\n",
       "  0.0012458617397014791,\n",
       "  0.001245043803058076,\n",
       "  0.0012442273807017625,\n",
       "  0.001243412469829063,\n",
       "  0.001242599067641692,\n",
       "  0.001241787171346545,\n",
       "  0.0012409767781556876,\n",
       "  0.0012401678852863487,\n",
       "  0.001239360489960907,\n",
       "  0.0012385545894068848,\n",
       "  0.0012377501808569361,\n",
       "  0.00123694726154884,\n",
       "  0.0012361458287254885,\n",
       "  0.001235345879634877,\n",
       "  0.0012345474115300974,\n",
       "  0.0012337504216693262,\n",
       "  0.0012329549073158163,\n",
       "  0.0012321608657378876,\n",
       "  0.0012313682942089157,\n",
       "  0.0012305771900073266,\n",
       "  0.001229787550416583,\n",
       "  0.0012289993727251773,\n",
       "  0.0012282126542266223,\n",
       "  0.001227427392219441,\n",
       "  0.0012266435840071579,\n",
       "  0.00122586122689829,\n",
       "  0.0012250803182063366,\n",
       "  0.001224300855249771,\n",
       "  0.0012235228353520305,\n",
       "  0.0012227462558415084,\n",
       "  0.0012219711140515435,\n",
       "  0.0012211974073204118,\n",
       "  0.001220425132991317,\n",
       "  0.0012196542884123814,\n",
       "  0.0012188848709366368,\n",
       "  0.0012181168779220152,\n",
       "  0.0012173503067313411,\n",
       "  0.0012165851547323203,\n",
       "  0.0012158214192975318,\n",
       "  0.0012150590978044203,\n",
       "  0.0012142981876352836,\n",
       "  0.0012135386861772672,\n",
       "  0.0012127805908223538,\n",
       "  0.0012120238989673546,\n",
       "  0.0012112686080138991,\n",
       "  0.0012105147153684288,\n",
       "  0.0012097622184421856,\n",
       "  0.0012090111146512046,\n",
       "  0.0012082614014163051,\n",
       "  0.0012075130761630803,\n",
       "  0.0012067661363218908,\n",
       "  0.0012060205793278538,\n",
       "  0.0012052764026208346,\n",
       "  0.001204533603645439,\n",
       "  0.0012037921798510039,\n",
       "  0.0012030521286915871,\n",
       "  0.001202313447625961,\n",
       "  0.0012015761341176022,\n",
       "  0.001200840185634683,\n",
       "  0.0012001055996500646,\n",
       "  0.0011993723736412848,\n",
       "  0.0011986405050905522,\n",
       "  0.0011979099914847366,\n",
       "  0.0011971808303153611,\n",
       "  0.001196453019078592,\n",
       "  0.0011957265552752312,\n",
       "  0.0011950014364107081,\n",
       "  0.0011942776599950702,\n",
       "  0.0011935552235429743,\n",
       "  0.0011928341245736785,\n",
       "  0.0011921143606110345,\n",
       "  0.0011913959291834767,\n",
       "  0.0011906788278240174,\n",
       "  0.0011899630540702338,\n",
       "  0.0011892486054642637,\n",
       "  0.001188535479552794,\n",
       "  0.0011878236738870544,\n",
       "  0.001187113186022808,\n",
       "  0.0011864040135203427,\n",
       "  0.0011856961539444633,\n",
       "  0.0011849896048644825,\n",
       "  0.0011842843638542146,\n",
       "  0.0011835804284919637,\n",
       "  0.001182877796360519,\n",
       "  0.0011821764650471432,\n",
       "  0.0011814764321435671,\n",
       "  0.00118077769524598,\n",
       "  0.0011800802519550202,\n",
       "  0.0011793840998757697,\n",
       "  0.001178689236617743,\n",
       "  0.001177995659794881,\n",
       "  0.001177303367025542,\n",
       "  0.0011766123559324932,\n",
       "  0.001175922624142903,\n",
       "  0.0011752341692883323,\n",
       "  0.0011745469890047286,\n",
       "  0.0011738610809324144,\n",
       "  0.0011731764427160805,\n",
       "  0.0011724930720047792,\n",
       "  0.0011718109664519152,\n",
       "  0.0011711301237152373,\n",
       "  0.0011704505414568303,\n",
       "  0.0011697722173431078,\n",
       "  0.0011690951490448036,\n",
       "  0.0011684193342369634,\n",
       "  0.0011677447705989374,\n",
       "  0.0011670714558143728,\n",
       "  0.0011663993875712046,\n",
       "  0.0011657285635616476,\n",
       "  0.0011650589814821903,\n",
       "  0.0011643906390335851,\n",
       "  0.0011637235339208413,\n",
       "  0.0011630576638532173,\n",
       "  0.0011623930265442118,\n",
       "  0.0011617296197115575,\n",
       "  0.001161067441077211,\n",
       "  0.001160406488367348,\n",
       "  0.0011597467593123527,\n",
       "  0.0011590882516468119,\n",
       "  0.001158430963109506,\n",
       "  0.0011577748914434018,\n",
       "  0.0011571200343956445,\n",
       "  0.0011564663897175502,\n",
       "  0.0011558139551645983,\n",
       "  0.0011551627284964242,\n",
       "  0.001154512707476809,\n",
       "  0.001153863889873676,\n",
       "  0.00115321627345908,\n",
       "  0.0011525698560091997,\n",
       "  0.0011519246353043325,\n",
       "  0.001151280609128884,\n",
       "  0.0011506377752713624,\n",
       "  0.0011499961315243692,\n",
       "  0.0011493556756845935,\n",
       "  0.0011487164055528033,\n",
       "  0.0011480783189338375,\n",
       "  0.0011474414136366001,\n",
       "  0.0011468056874740507,\n",
       "  0.001146171138263198,\n",
       "  0.0011455377638250924,\n",
       "  0.0011449055619848186,\n",
       "  0.0011442745305714872,\n",
       "  0.0011436446674182282,\n",
       "  0.0011430159703621832,\n",
       "  0.0011423884372444978,\n",
       "  0.0011417620659103153,\n",
       "  0.001141136854208767,\n",
       "  0.0011405127999929673,\n",
       "  0.0011398899011200046,\n",
       "  0.0011392681554509344,\n",
       "  0.0011386475608507736,\n",
       "  0.0011380281151884897,\n",
       "  0.001137409816336997,\n",
       "  0.001136792662173147,\n",
       "  0.0011361766505777221,\n",
       "  0.0011355617794354285,\n",
       "  0.0011349480466348876,\n",
       "  0.001134335450068631,\n",
       "  0.0011337239876330898,\n",
       "  0.0011331136572285928,\n",
       "  0.0011325044567593529,\n",
       "  0.0011318963841334653,\n",
       "  0.0011312894372628964,\n",
       "  0.0011306836140634788,\n",
       "  0.0011300789124549047,\n",
       "  0.001129475330360716,\n",
       "  0.0011288728657082995,\n",
       "  0.0011282715164288803,\n",
       "  0.0011276712804575118,\n",
       "  0.0011270721557330708,\n",
       "  0.0011264741401982507,\n",
       "  0.0011258772317995528,\n",
       "  0.0011252814284872807,\n",
       "  0.0011246867282155323,\n",
       "  0.001124093128942194,\n",
       "  0.0011235006286289316,\n",
       "  0.0011229092252411854,\n",
       "  0.0011223189167481622,\n",
       "  0.001121729701122829,\n",
       "  0.0011211415763419045,\n",
       "  0.0011205545403858543,\n",
       "  0.0011199685912388823,\n",
       "  0.0011193837268889239,\n",
       "  0.0011187999453276408,\n",
       "  0.0011182172445504115,\n",
       "  0.001117635622556326,\n",
       "  0.0011170550773481804,\n",
       "  0.0011164756069324652,\n",
       "  0.0011158972093193644,\n",
       "  0.0011153198825227446,\n",
       "  0.0011147436245601491,\n",
       "  0.001114168433452793,\n",
       "  0.0011135943072255528,\n",
       "  0.0011130212439069628,\n",
       "  0.0011124492415292072,\n",
       "  0.0011118782981281124,\n",
       "  0.001111308411743143,\n",
       "  0.001110739580417391,\n",
       "  0.001110171802197573,\n",
       "  0.001109605075134021,\n",
       "  0.001109039397280677,\n",
       "  0.001108474766695085,\n",
       "  0.0011079111814383863,\n",
       "  0.0011073486395753109,\n",
       "  0.0011067871391741719,\n",
       "  0.0011062266783068591,\n",
       "  0.0011056672550488306,\n",
       "  0.0011051088674791094,\n",
       "  0.0011045515136802729,\n",
       "  0.0011039951917384493,\n",
       "  0.0011034398997433108,\n",
       "  0.0011028856357880646,\n",
       "  0.0011023323979694494,\n",
       "  0.0011017801843877272,\n",
       "  0.0011012289931466763,\n",
       "  0.001100678822353587,\n",
       "  0.0011001296701192524,\n",
       "  0.0010995815345579635,\n",
       "  0.0010990344137875032,\n",
       "  0.0010984883059291376,\n",
       "  0.0010979432091076124,\n",
       "  0.001097399121451144,\n",
       "  0.0010968560410914148,\n",
       "  0.0010963139661635656,\n",
       "  0.0010957728948061904,\n",
       "  0.0010952328251613288,\n",
       "  0.0010946937553744601,\n",
       "  0.0010941556835944973,\n",
       "  0.0010936186079737798,\n",
       "  0.0010930825266680683,\n",
       "  0.001092547437836538,\n",
       "  0.0010920133396417713,\n",
       "  0.0010914802302497526,\n",
       "  0.0010909481078298622,\n",
       "  0.0010904169705548688,\n",
       "  0.0010898868166009247,\n",
       "  0.001089357644147558,\n",
       "  0.0010888294513776678,\n",
       "  0.0010883022364775164,\n",
       "  0.0010877759976367254,\n",
       "  0.0010872507330482667,\n",
       "  0.0010867264409084585,\n",
       "  0.0010862031194169572,\n",
       "  0.0010856807667767535,\n",
       "  0.0010851593811941642,\n",
       "  0.0010846389608788276,\n",
       "  0.0010841195040436956,\n",
       "  0.001083601008905029,\n",
       "  0.0010830834736823911,\n",
       "  0.0010825668965986413,\n",
       "  0.001082051275879929,\n",
       "  0.0010815366097556873,\n",
       "  0.0010810228964586285,\n",
       "  0.001080510134224735,\n",
       "  0.001079998321293257,\n",
       "  0.0010794874559067022,\n",
       "  0.001078977536310834,\n",
       "  0.0010784685607546623,\n",
       "  0.001077960527490439,\n",
       "  0.001077453434773652,\n",
       "  0.0010769472808630192,\n",
       "  0.001076442064020481,\n",
       "  0.0010759377825111961,\n",
       "  0.0010754344346035362,\n",
       "  0.0010749320185690773,\n",
       "  0.0010744305326825959,\n",
       "  0.0010739299752220621,\n",
       "  0.0010734303444686348,\n",
       "  0.0010729316387066547,\n",
       "  0.0010724338562236385,\n",
       "  0.0010719369953102738,\n",
       "  0.0010714410542604128,\n",
       "  0.0010709460313710652,\n",
       "  0.0010704519249423953,\n",
       "  0.001069958733277714,\n",
       "  0.0010694664546834712,\n",
       "  0.0010689750874692554,\n",
       "  0.0010684846299477823,\n",
       "  0.0010679950804348927,\n",
       "  0.001067506437249544,\n",
       "  0.0010670186987138078,\n",
       "  0.0010665318631528592,\n",
       "  0.0010660459288949777,\n",
       "  0.0010655608942715348,\n",
       "  0.0010650767576169918,\n",
       "  0.0010645935172688946,\n",
       "  0.0010641111715678658,\n",
       "  0.0010636297188576008,\n",
       "  0.0010631491574848604,\n",
       "  0.0010626694857994673,\n",
       "  0.0010621907021542989,\n",
       "  0.0010617128049052811,\n",
       "  0.0010612357924113852,\n",
       "  0.001060759663034619,\n",
       "  0.001060284415140024,\n",
       "  0.0010598100470956676,\n",
       "  0.0010593365572726396,\n",
       "  0.0010588639440450443,\n",
       "  0.0010583922057899968,\n",
       "  0.0010579213408876166,\n",
       "  0.0010574513477210226,\n",
       "  0.0010569822246763257,\n",
       "  0.0010565139701426264,\n",
       "  0.0010560465825120067,\n",
       "  0.0010555800601795254,\n",
       "  0.0010551144015432125,\n",
       "  0.0010546496050040644,\n",
       "  0.0010541856689660369,\n",
       "  0.0010537225918360415,\n",
       "  0.0010532603720239387,\n",
       "  0.001052799007942533,\n",
       "  0.0010523384980075671,\n",
       "  0.001051878840637717,\n",
       "  0.001051420034254586,\n",
       "  0.0010509620772827003,\n",
       "  0.0010505049681495018,\n",
       "  0.0010500487052853445,\n",
       "  0.0010495932871234879,\n",
       "  0.001049138712100092,\n",
       "  0.0010486849786542137,\n",
       "  0.0010482320852277967,\n",
       "  0.0010477800302656716,\n",
       "  0.0010473288122155476,\n",
       "  0.0010468784295280073,\n",
       "  0.0010464288806565024,\n",
       "  0.0010459801640573473,\n",
       "  0.0010455322781897146,\n",
       "  0.0010450852215156293,\n",
       "  0.0010446389924999636,\n",
       "  0.0010441935896104324,\n",
       "  0.001043749011317587,\n",
       "  0.0010433052560948099,\n",
       "  0.0010428623224183105,\n",
       "  0.0010424202087671191,\n",
       "  0.0010419789136230813,\n",
       "  0.0010415384354708543,\n",
       "  0.0010410987727979002,\n",
       "  0.0010406599240944813,\n",
       "  0.0010402218878536546,\n",
       "  0.0010397846625712678,\n",
       "  0.001039348246745953,\n",
       "  0.0010389126388791214,\n",
       "  0.0010384778374749595,\n",
       "  0.001038043841040422,\n",
       "  0.0010376106480852285,\n",
       "  0.0010371782571218576,\n",
       "  0.0010367466666655414,\n",
       "  0.0010363158752342613,\n",
       "  0.0010358858813487419,\n",
       "  0.001035456683532447,\n",
       "  0.0010350282803115742,\n",
       "  0.0010346006702150485,\n",
       "  0.0010341738517745192,\n",
       "  0.001033747823524354,\n",
       "  0.0010333225840016339,\n",
       "  0.001032898131746148,\n",
       "  0.0010324744653003894,\n",
       "  0.0010320515832095492,\n",
       "  0.0010316294840215113,\n",
       "  0.0010312081662868491,\n",
       "  0.0010307876285588187,\n",
       "  0.0010303678693933543,\n",
       "  0.0010299488873490648,\n",
       "  0.0010295306809872263,\n",
       "  0.0010291132488717788,\n",
       "  0.001028696589569322,\n",
       "  0.0010282807016491076,\n",
       "  0.0010278655836830372,\n",
       "  0.0010274512342456563,\n",
       "  0.0010270376519141496,\n",
       "  0.0010266248352683348,\n",
       "  0.0010262127828906602,\n",
       "  0.001025801493366198,\n",
       "  0.001025390965282639,\n",
       "  0.00102498119723029,\n",
       "  0.0010245721878020674,\n",
       "  0.0010241639355934922,\n",
       "  0.0010237564392026853,\n",
       "  0.0010233496972303637,\n",
       "  0.0010229437082798349,\n",
       "  0.0010225384709569919,\n",
       "  0.0010221339838703083,\n",
       "  0.0010217302456308345,\n",
       "  0.0010213272548521921,\n",
       "  0.0010209250101505694,\n",
       "  0.0010205235101447168,\n",
       "  0.0010201227534559418,\n",
       "  0.0010197227387081038,\n",
       "  0.0010193234645276105,\n",
       "  0.001018924929543413,\n",
       "  0.0010185271323869998,\n",
       "  0.001018130071692393,\n",
       "  0.0010177337460961443,\n",
       "  0.0010173381542373295,\n",
       "  ...])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "theta = np.random.random(size=2)\n",
    "iterations = 40000\n",
    "alpha = 0.1\n",
    "theta, J_history = gradient_descent(X_train ,y_train, theta, alpha, iterations)\n",
    "theta, J_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-86125cd57f0fdb89",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "You can evaluate the learning process by monitoring the loss as training progress. In the following graph, we visualize the loss as a function of the iterations. This is possible since we are saving the loss value at every iteration in the `J_history` array. This visualization might help you find problems with your code. Notice that since the network converges quickly, we are using logarithmic scale for the number of iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a565f1f721f6377f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(iterations), J_history)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss as a function of iterations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3bdd058ecc5db0eb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Implement the pseudo-inverse function `pinv`. **Do not use `np.linalg.pinv`**, instead use only direct matrix multiplication as you saw in class (you can calculate the inverse of a matrix using `np.linalg.inv`). (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pinv(X, y):\n",
    "    \"\"\"\n",
    "    Calculate the optimal values of the parameters using the pseudoinverse\n",
    "    approach as you saw in class using the *training set*.\n",
    "\n",
    "    Input:\n",
    "    - X: Inputs  (n features over m instances).\n",
    "    - y: True labels (1 value over m instances).\n",
    "\n",
    "    Returns two values:\n",
    "    - theta: The optimal parameters of your model.\n",
    "\n",
    "    ########## DO NOT USE np.linalg.pinv ##############\n",
    "    \"\"\"\n",
    "    \n",
    "    ###########################################################################\n",
    "    # TODO: Implement the pseudoinverse algorithm.                            #\n",
    "    ###########################################################################\n",
    "    pinv_theta = np.matmul(np.linalg.inv(np.matmul(X.T, X)), X.T)\n",
    "    \n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return pinv_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.311273176476264"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J_pinv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ee89ac06af3087ae",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "theta_pinv = pinv(X_train ,y_train)\n",
    "J_pinv = compute_cost(X_train, y_train, theta_pinv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add the loss value for the theta calculated using the psuedo-inverse to our graph. This is another sanity check as the loss of our model should converge to the psuedo-inverse loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-639b53fc41479335",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(iterations), J_history)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss as a function of iterations')\n",
    "plt.hlines(y = J_pinv, xmin = 0, xmax = len(J_history), color='r',\n",
    "           linewidth = 1, linestyle = 'dashed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5043aa5363cbe5c9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "We can use a better approach for the implementation of `gradient_descent`. Instead of performing 40,000 iterations, we wish to stop when the improvement of the loss value is smaller than `1e-8` from one iteration to the next. Implement the function `efficient_gradient_descent`. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def efficient_gradient_descent(X, y, theta, alpha, num_iters):\n",
    "    \"\"\"\n",
    "    Learn the parameters of your model using the *training set*, but stop \n",
    "    the learning process once the improvement of the loss value is smaller \n",
    "    than 1e-8. This function is very similar to the gradient descent \n",
    "    function you already implemented.\n",
    "\n",
    "    Input:\n",
    "    - X: Inputs  (n features over m instances).\n",
    "    - y: True labels (1 value over m instances).\n",
    "    - theta: The parameters (weights) of the model being learned.\n",
    "    - alpha: The learning rate of your model.\n",
    "    - num_iters: The number of updates performed.\n",
    "\n",
    "    Returns two values:\n",
    "    - theta: The learned parameters of your model.\n",
    "    - J_history: the loss value for every iteration.\n",
    "    \"\"\"\n",
    "    \n",
    "    J_history = [] # Use a python list to save cost in every iteration\n",
    "    theta = theta.copy() # avoid changing the original thetas\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the gradient descent optimization algorithm.            #\n",
    "    ###########################################################################\n",
    "    pass\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return theta, J_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6e2524d07523d950",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "The learning rate is another factor that determines the performance of our model in terms of speed and accuracy. Complete the function `find_best_alpha`. Make sure you use the training dataset to learn the parameters (thetas) and use those parameters with the validation dataset to compute the cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_alpha(X_train, y_train, X_val, y_val, iterations):\n",
    "    \"\"\"\n",
    "    Iterate over provided values of alpha and train a model using the \n",
    "    *training* dataset. maintain a python dictionary with alpha as the \n",
    "    key and the loss on the *validation* set as the value.\n",
    "\n",
    "    Input:\n",
    "    - X_train, y_train, X_val, y_val: the training and validation data\n",
    "    - iterations: maximum number of iterations\n",
    "\n",
    "    Returns:\n",
    "    - alpha_dict: A python dictionary - {key (alpha) : value (validation loss)}\n",
    "    \"\"\"\n",
    "    \n",
    "    alphas = [0.00001, 0.00003, 0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 2, 3]\n",
    "    alpha_dict = {}\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the function.                                           #\n",
    "    ###########################################################################\n",
    "    pass\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return alpha_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a8b088fe7a10910a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "alpha_dict = find_best_alpha(X_train, y_train, X_val, y_val, 40000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5bd93130c022d3e1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Obtain the best learning rate from the dictionary `alpha_dict`. This can be done in a single line using built-in functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4f81cf375ac46b73",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "best_alpha = None\n",
    "###########################################################################\n",
    "#                            START OF YOUR CODE                           #\n",
    "###########################################################################\n",
    "pass\n",
    "###########################################################################\n",
    "#                             END OF YOUR CODE                            #\n",
    "###########################################################################\n",
    "print(best_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d16367ecb7183996",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Pick the best three alpha values you just calculated and provide **one** graph with three lines indicating the training loss as a function of iterations (Use 10,000 iterations). Note you are required to provide general code for this purpose (no hard-coding). Make sure the visualization is clear and informative. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-448638e817503ca3",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "#                            START OF YOUR CODE                           #\n",
    "###########################################################################\n",
    "pass\n",
    "###########################################################################\n",
    "#                             END OF YOUR CODE                            #\n",
    "###########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b73893d236bff1d5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "This is yet another sanity check. This function plots the regression lines of your model and the model based on the pseudoinverse calculation. Both models should exhibit the same trend through the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c7ee7d8763464371",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 7))\n",
    "plt.plot(X_train[:,1], y_train, 'ro', ms=1, mec='k')\n",
    "plt.ylabel('Price in USD')\n",
    "plt.xlabel('sq.ft')\n",
    "plt.plot(X_train[:, 1], np.dot(X_train, theta), 'o')\n",
    "plt.plot(X_train[:, 1], np.dot(X_train, theta_pinv), '-')\n",
    "\n",
    "plt.legend(['Training data', 'Linear regression', 'Best theta']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e77c602466fab37d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Part 2: Multivariate Linear Regression (30 points)\n",
    "\n",
    "In most cases, you will deal with databases that have more than one feature. It can be as little as two features and up to thousands of features. In those cases, we use a multiple linear regression model. The regression equation is almost the same as the simple linear regression equation:\n",
    "\n",
    "$$\n",
    "\\hat{y} = h_\\theta(\\vec{x}) = \\theta^T \\vec{x} = \\theta_0 + \\theta_1 x_1 + ... + \\theta_n x_n\n",
    "$$\n",
    "\n",
    "\n",
    "If you wrote vectorized code, this part should be straightforward. If your code is not vectorized, you should go back and edit your functions such that they support both multivariate and single variable regression. **Your code should not check the dimensionality of the input before running**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-15626dda8db26550",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Read comma separated data\n",
    "df = pd.read_csv('data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2dc0f4dc3491520c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Preprocessing\n",
    "\n",
    "Like in the single variable case, we need to create a numpy array from the dataframe. Before doing so, we should notice that some of the features are clearly irrelevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-a87b4027bd3bda4b",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "X = df.drop(columns=['price', 'id', 'date']).values\n",
    "y = df['price'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1aa12f54513b1efa",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Use the **same** `preprocess` function you implemented previously. Notice that proper vectorized implementation should work regardless of the dimensionality of the input. You might want to check that your code in the previous parts still works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f40a9df530db9399",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "X, y = preprocess(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and validation split\n",
    "np.random.seed(42)\n",
    "indices = np.random.permutation(X.shape[0])\n",
    "idx_train, idx_val = indices[:int(0.8*X.shape[0])], indices[int(0.8*X.shape[0]):]\n",
    "X_train, X_val = X[idx_train,:], X[idx_val,:]\n",
    "y_train, y_val = y[idx_train], y[idx_val]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using 3D visualization, we can still observe trends in the data. Visualizing additional dimensions requires advanced techniques we will learn later in the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0c68216a26a9b5af",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import mpl_toolkits.mplot3d.axes3d as p3\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "ax = p3.Axes3D(fig)\n",
    "xx = X_train[:, 1][:1000]\n",
    "yy = X_train[:, 2][:1000]\n",
    "zz = y_train[:1000]\n",
    "ax.scatter(xx, yy, zz, marker='o')\n",
    "ax.set_xlabel('bathrooms')\n",
    "ax.set_ylabel('sqft_living')\n",
    "ax.set_zlabel('price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-70fcd47d69caea00",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Use the bias trick again (add a column of ones as the zeroth column in the both the training and validation datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-2985911f4b7af3e1",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "#                            START OF YOUR CODE                           #\n",
    "###########################################################################\n",
    "pass\n",
    "###########################################################################\n",
    "#                             END OF YOUR CODE                            #\n",
    "###########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2b89288ff61c80ac",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Make sure the functions `compute_cost` (10 points), `gradient_descent` (15 points), and `pinv` (5 points) work on the multi-dimensional dataset. If you make any changes, make sure your code still works on the single variable regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-81ab741781b2f6ec",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "shape = X_train.shape[1]\n",
    "theta = np.ones(shape)\n",
    "J = compute_cost(X_train, y_train, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6f25fb05bd6c648a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "shape = X_train.shape[1]\n",
    "theta = np.random.random(shape)\n",
    "iterations = 40000\n",
    "theta, J_history = gradient_descent(X_train ,y_train, theta, best_alpha, iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-827d1de1293be51f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "theta_pinv = pinv(X_train ,y_train)\n",
    "J_pinv = compute_cost(X_train, y_train, theta_pinv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use visualization to make sure the code works well. Notice we use logarithmic scale for the number of iterations, since gradient descent converges after ~500 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4fa207b72d2445c2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(iterations), J_history)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss as a function of iterations - multivariate linear regression')\n",
    "plt.hlines(y = J_pinv, xmin = 0, xmax = len(J_history), color='r',\n",
    "           linewidth = 1, linestyle = 'dashed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cad652570cee3629",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Part 3: Polynomial Regression (10 points)\n",
    "\n",
    "Linear Regression allows us to explore linear relationships but if we need a model that describes non-linear dependencies we can also use Polynomial Regression. In order to perform polynomial regression, we create additional features using a function of the original features and use standard linear regression on the new features. For example, consider the following single variable $(x)$ cubic regression:\n",
    "\n",
    "$$ x_0 = 1, \\space x_1 = x, \\space x_2 = x^2, \\space x_3 = x^3$$\n",
    "\n",
    "And after using standard linear regression:\n",
    "\n",
    "$$ f(x) = \\theta_0 + \\theta_1 x + \\theta_2 x^2 +  \\theta_3 x^3$$\n",
    "\n",
    "As required. \n",
    "\n",
    "For this exercise, use polynomial regression by using all **quadratic** feature combinations: \n",
    "\n",
    "$$ 1, x, y, z, x^2, y^2, z^2, xy, xz, yz, ...$$\n",
    "\n",
    "and evaluate the MSE cost on the training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['price', 'id', 'date']\n",
    "all_features = df.drop(columns=columns_to_drop)\n",
    "all_features.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give an explanations to the results and compare them to regular linear regression. Do they make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use this Markdown cell for your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Adaptive Learning Rate (10 points)\n",
    "\n",
    "So far, we kept the learning rate alpha constant during training. However, changing alpha during training might improve convergence in terms of the global minimum found and running time. Implement the adaptive learning rate method based on the gradient descent algorithm above. \n",
    "\n",
    "**Your task is to find proper hyper-parameter values for the adaptive technique and compare this technique to the constant learning rate. Use clear visualizations of the validation loss and the learning rate as a function of the iteration**. \n",
    "\n",
    "Time based decay: this method reduces the learning rate every iteration according to the following formula:\n",
    "\n",
    "$$\\alpha = \\frac{\\alpha_0}{1 + D \\cdot t}$$\n",
    "\n",
    "Where $\\alpha_0$ is the original learning rate, $D$ is a decay factor and $t$ is the current iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here ###"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

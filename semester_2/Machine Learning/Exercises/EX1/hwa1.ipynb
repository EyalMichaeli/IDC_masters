{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-35def0d0f4b47a0a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Exercise 1: Linear Regression\n",
    "\n",
    "### This notebook is executed automatically. Failing to meet any of the submission requirements will results in a 25 point fine or your submission not being graded at all. Kindly reminder: the homework assignments grade is 50% of the final grade. \n",
    "\n",
    "### Make sure you restart the notebook and check the filename before submission. Appeals based on wrong filenames and errors due to syntax and execution errors will not be accepted.\n",
    "\n",
    "### Do not start the exercise until you fully understand the submission guidelines.\n",
    "\n",
    "## Read the following instructions carefully:\n",
    "\n",
    "1. This jupyter notebook contains all the step by step instructions needed for this exercise.\n",
    "1. Submission includes this notebook only with the exercise number and your ID as the filename. For example: `hw1_123456789_987654321.ipynb` if you submitted in pairs and `hw1_123456789.ipynb` if you submitted the exercise alone.\n",
    "1. Write **efficient vectorized** code whenever possible. Some calculations in this exercise take several minutes when implemented efficiently, and might take much longer otherwise. Unnecessary loops will result in point deduction.\n",
    "1. You are responsible for the correctness of your code and should add as many tests as you see fit. Tests will not be graded nor checked.\n",
    "1. Write your functions in this notebook only. **Do not create Python modules and import them**.\n",
    "1. You are allowed to use functions and methods from the [Python Standard Library](https://docs.python.org/3/library/) and [numpy](https://www.numpy.org/devdocs/reference/) only. **Do not import anything else.**\n",
    "1. Your code must run without errors. Make sure your `numpy` version is at least 1.15.4 and that you are using at least python 3.6. Changes of the configuration we provided are at your own risk. Any code that cannot run will not be graded.\n",
    "1. Write your own code. Cheating will not be tolerated.\n",
    "1. Answers to qualitative questions should be written in **markdown** cells (with $\\LaTeX$ support). Answers that will be written in commented code blocks will not be checked.\n",
    "\n",
    "## In this exercise you will perform the following:\n",
    "1. Load a dataset and perform basic data exploration using a powerful data science library called [pandas](https://pandas.pydata.org/pandas-docs/stable/).\n",
    "1. Preprocess the data for linear regression.\n",
    "1. Compute the cost and perform gradient descent in pure numpy in vectorized form.\n",
    "1. Fit a linear regression model using a single feature.\n",
    "1. Visualize your results using matplotlib.\n",
    "1. Perform multivariate linear regression.\n",
    "1. Perform polynomial regression.\n",
    "1. Experiment with adaptive learning rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I have read and understood the instructions: *** YOUR ID HERE ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5ed0076cec86f623",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np # used for scientific computing\n",
    "import pandas as pd # used for data analysis and manipulation\n",
    "import matplotlib.pyplot as plt # used for visualization and plotting\n",
    "\n",
    "np.random.seed(42) \n",
    "\n",
    "# make matplotlib figures appear inline in the notebook\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (14.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-916f46de8cde2ca7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Part 1: Data Preprocessing (10 Points)\n",
    "\n",
    "For the following exercise, we will use a dataset containing housing prices in King County, USA. The dataset contains 5,000 observations with 18 features and a single target value - the house price. \n",
    "\n",
    "First, we will read and explore the data using pandas and the `.read_csv` method. Pandas is an open source library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9ef8b2769c2c1949",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Read comma separated data\n",
    "df = pd.read_csv('data.csv') # Make sure this cell runs regardless of your absolute path.\n",
    "# df stands for dataframe, which is the default format for datasets in pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6966afc155aa6616",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Data Exploration\n",
    "A good practice in any data-oriented project is to first try and understand the data. Fortunately, pandas is built for that purpose. Start by looking at the top of the dataset using the `df.head()` command. This will be the first indication that you read your data properly, and that the headers are correct. Next, you can use `df.describe()` to show statistics on the data and check for trends and irregularities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "           id             date     price  bedrooms  bathrooms  sqft_living  \\\n0  7129300520  20141013T000000  221900.0         3       1.00         1180   \n1  6414100192  20141209T000000  538000.0         3       2.25         2570   \n2  5631500400  20150225T000000  180000.0         2       1.00          770   \n3  2487200875  20141209T000000  604000.0         4       3.00         1960   \n4  1954400510  20150218T000000  510000.0         3       2.00         1680   \n\n   sqft_lot  floors  waterfront  view  condition  grade  sqft_above  yr_built  \\\n0      5650     1.0           0     0          3      7        1180      1955   \n1      7242     2.0           0     0          3      7        2170      1951   \n2     10000     1.0           0     0          3      6         770      1933   \n3      5000     1.0           0     0          5      7        1050      1965   \n4      8080     1.0           0     0          3      8        1680      1987   \n\n   yr_renovated  zipcode      lat     long  sqft_living15  sqft_lot15  \n0             0    98178  47.5112 -122.257           1340        5650  \n1          1991    98125  47.7210 -122.319           1690        7639  \n2             0    98028  47.7379 -122.233           2720        8062  \n3             0    98136  47.5208 -122.393           1360        5000  \n4             0    98074  47.6168 -122.045           1800        7503  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>date</th>\n      <th>price</th>\n      <th>bedrooms</th>\n      <th>bathrooms</th>\n      <th>sqft_living</th>\n      <th>sqft_lot</th>\n      <th>floors</th>\n      <th>waterfront</th>\n      <th>view</th>\n      <th>condition</th>\n      <th>grade</th>\n      <th>sqft_above</th>\n      <th>yr_built</th>\n      <th>yr_renovated</th>\n      <th>zipcode</th>\n      <th>lat</th>\n      <th>long</th>\n      <th>sqft_living15</th>\n      <th>sqft_lot15</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7129300520</td>\n      <td>20141013T000000</td>\n      <td>221900.0</td>\n      <td>3</td>\n      <td>1.00</td>\n      <td>1180</td>\n      <td>5650</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>7</td>\n      <td>1180</td>\n      <td>1955</td>\n      <td>0</td>\n      <td>98178</td>\n      <td>47.5112</td>\n      <td>-122.257</td>\n      <td>1340</td>\n      <td>5650</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6414100192</td>\n      <td>20141209T000000</td>\n      <td>538000.0</td>\n      <td>3</td>\n      <td>2.25</td>\n      <td>2570</td>\n      <td>7242</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>7</td>\n      <td>2170</td>\n      <td>1951</td>\n      <td>1991</td>\n      <td>98125</td>\n      <td>47.7210</td>\n      <td>-122.319</td>\n      <td>1690</td>\n      <td>7639</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5631500400</td>\n      <td>20150225T000000</td>\n      <td>180000.0</td>\n      <td>2</td>\n      <td>1.00</td>\n      <td>770</td>\n      <td>10000</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>6</td>\n      <td>770</td>\n      <td>1933</td>\n      <td>0</td>\n      <td>98028</td>\n      <td>47.7379</td>\n      <td>-122.233</td>\n      <td>2720</td>\n      <td>8062</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2487200875</td>\n      <td>20141209T000000</td>\n      <td>604000.0</td>\n      <td>4</td>\n      <td>3.00</td>\n      <td>1960</td>\n      <td>5000</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>7</td>\n      <td>1050</td>\n      <td>1965</td>\n      <td>0</td>\n      <td>98136</td>\n      <td>47.5208</td>\n      <td>-122.393</td>\n      <td>1360</td>\n      <td>5000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1954400510</td>\n      <td>20150218T000000</td>\n      <td>510000.0</td>\n      <td>3</td>\n      <td>2.00</td>\n      <td>1680</td>\n      <td>8080</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>8</td>\n      <td>1680</td>\n      <td>1987</td>\n      <td>0</td>\n      <td>98074</td>\n      <td>47.6168</td>\n      <td>-122.045</td>\n      <td>1800</td>\n      <td>7503</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-5bd0d6844b64ea1a",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>condition</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.000000e+03</td>\n",
       "      <td>5.000000e+03</td>\n",
       "      <td>5000.0000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5.000000e+03</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.00000</td>\n",
       "      <td>5000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.630823e+09</td>\n",
       "      <td>5.394699e+05</td>\n",
       "      <td>3.3714</td>\n",
       "      <td>2.062150</td>\n",
       "      <td>2061.036800</td>\n",
       "      <td>1.615893e+04</td>\n",
       "      <td>1.432600</td>\n",
       "      <td>0.009400</td>\n",
       "      <td>0.243000</td>\n",
       "      <td>3.455000</td>\n",
       "      <td>7.595200</td>\n",
       "      <td>1753.151000</td>\n",
       "      <td>1966.660800</td>\n",
       "      <td>95.052800</td>\n",
       "      <td>98078.812600</td>\n",
       "      <td>47.559312</td>\n",
       "      <td>-122.215864</td>\n",
       "      <td>1976.84520</td>\n",
       "      <td>13451.164600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.870890e+09</td>\n",
       "      <td>3.873115e+05</td>\n",
       "      <td>0.9104</td>\n",
       "      <td>0.773592</td>\n",
       "      <td>923.727509</td>\n",
       "      <td>4.600220e+04</td>\n",
       "      <td>0.510793</td>\n",
       "      <td>0.096506</td>\n",
       "      <td>0.774643</td>\n",
       "      <td>0.677692</td>\n",
       "      <td>1.166537</td>\n",
       "      <td>818.390844</td>\n",
       "      <td>28.286855</td>\n",
       "      <td>425.234932</td>\n",
       "      <td>54.126332</td>\n",
       "      <td>0.139521</td>\n",
       "      <td>0.141807</td>\n",
       "      <td>674.73601</td>\n",
       "      <td>26514.749009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000102e+06</td>\n",
       "      <td>7.500000e+04</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>380.000000</td>\n",
       "      <td>6.090000e+02</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>380.000000</td>\n",
       "      <td>1900.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>98001.000000</td>\n",
       "      <td>47.155900</td>\n",
       "      <td>-122.514000</td>\n",
       "      <td>620.00000</td>\n",
       "      <td>660.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.154075e+09</td>\n",
       "      <td>3.179062e+05</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1410.000000</td>\n",
       "      <td>5.400000e+03</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1190.000000</td>\n",
       "      <td>1949.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>98033.000000</td>\n",
       "      <td>47.463675</td>\n",
       "      <td>-122.329000</td>\n",
       "      <td>1490.00000</td>\n",
       "      <td>5391.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.022900e+09</td>\n",
       "      <td>4.490000e+05</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1890.000000</td>\n",
       "      <td>7.875000e+03</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1530.000000</td>\n",
       "      <td>1968.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>98070.000000</td>\n",
       "      <td>47.572850</td>\n",
       "      <td>-122.235000</td>\n",
       "      <td>1820.00000</td>\n",
       "      <td>7800.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.345078e+09</td>\n",
       "      <td>6.500000e+05</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2500.000000</td>\n",
       "      <td>1.123400e+04</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2130.000000</td>\n",
       "      <td>1990.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>98118.000000</td>\n",
       "      <td>47.679200</td>\n",
       "      <td>-122.129000</td>\n",
       "      <td>2340.00000</td>\n",
       "      <td>10469.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.842300e+09</td>\n",
       "      <td>7.060000e+06</td>\n",
       "      <td>9.0000</td>\n",
       "      <td>6.750000</td>\n",
       "      <td>10040.000000</td>\n",
       "      <td>1.651359e+06</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>7680.000000</td>\n",
       "      <td>2015.000000</td>\n",
       "      <td>2015.000000</td>\n",
       "      <td>98199.000000</td>\n",
       "      <td>47.777600</td>\n",
       "      <td>-121.315000</td>\n",
       "      <td>5790.00000</td>\n",
       "      <td>434728.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id         price   bedrooms    bathrooms   sqft_living  \\\n",
       "count  5.000000e+03  5.000000e+03  5000.0000  5000.000000   5000.000000   \n",
       "mean   4.630823e+09  5.394699e+05     3.3714     2.062150   2061.036800   \n",
       "std    2.870890e+09  3.873115e+05     0.9104     0.773592    923.727509   \n",
       "min    1.000102e+06  7.500000e+04     0.0000     0.000000    380.000000   \n",
       "25%    2.154075e+09  3.179062e+05     3.0000     1.500000   1410.000000   \n",
       "50%    4.022900e+09  4.490000e+05     3.0000     2.000000   1890.000000   \n",
       "75%    7.345078e+09  6.500000e+05     4.0000     2.500000   2500.000000   \n",
       "max    9.842300e+09  7.060000e+06     9.0000     6.750000  10040.000000   \n",
       "\n",
       "           sqft_lot       floors   waterfront         view    condition  \\\n",
       "count  5.000000e+03  5000.000000  5000.000000  5000.000000  5000.000000   \n",
       "mean   1.615893e+04     1.432600     0.009400     0.243000     3.455000   \n",
       "std    4.600220e+04     0.510793     0.096506     0.774643     0.677692   \n",
       "min    6.090000e+02     1.000000     0.000000     0.000000     1.000000   \n",
       "25%    5.400000e+03     1.000000     0.000000     0.000000     3.000000   \n",
       "50%    7.875000e+03     1.000000     0.000000     0.000000     3.000000   \n",
       "75%    1.123400e+04     2.000000     0.000000     0.000000     4.000000   \n",
       "max    1.651359e+06     3.500000     1.000000     4.000000     5.000000   \n",
       "\n",
       "             grade   sqft_above     yr_built  yr_renovated       zipcode  \\\n",
       "count  5000.000000  5000.000000  5000.000000   5000.000000   5000.000000   \n",
       "mean      7.595200  1753.151000  1966.660800     95.052800  98078.812600   \n",
       "std       1.166537   818.390844    28.286855    425.234932     54.126332   \n",
       "min       3.000000   380.000000  1900.000000      0.000000  98001.000000   \n",
       "25%       7.000000  1190.000000  1949.000000      0.000000  98033.000000   \n",
       "50%       7.000000  1530.000000  1968.000000      0.000000  98070.000000   \n",
       "75%       8.000000  2130.000000  1990.000000      0.000000  98118.000000   \n",
       "max      13.000000  7680.000000  2015.000000   2015.000000  98199.000000   \n",
       "\n",
       "               lat         long  sqft_living15     sqft_lot15  \n",
       "count  5000.000000  5000.000000     5000.00000    5000.000000  \n",
       "mean     47.559312  -122.215864     1976.84520   13451.164600  \n",
       "std       0.139521     0.141807      674.73601   26514.749009  \n",
       "min      47.155900  -122.514000      620.00000     660.000000  \n",
       "25%      47.463675  -122.329000     1490.00000    5391.500000  \n",
       "50%      47.572850  -122.235000     1820.00000    7800.000000  \n",
       "75%      47.679200  -122.129000     2340.00000   10469.250000  \n",
       "max      47.777600  -121.315000     5790.00000  434728.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9b9bd1b387905904",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "We will start with one variable linear regression by extracting the target column and the `sqft_living` variable from the dataset. We use pandas and select both columns as separate variables and transform them into a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-c7cd243e8b5fe5aa",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "X = df['sqft_living'].values\n",
    "y = df['price'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-508e7e1a13f9bbe4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Preprocessing\n",
    "\n",
    "As the number of features grows, calculating gradients gets computationally expensive. We can speed this up by normalizing the input data to ensure all values are within the same range. This is especially important for datasets with high standard deviations or differences in the ranges of the attributes. Use [mean normalization](https://en.wikipedia.org/wiki/Feature_scaling) for the fearures (`X`) and the true labels (`y`).\n",
    "\n",
    "Implement the cost function `preprocess`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X, y):\n",
    "    \"\"\"\n",
    "    Perform mean normalization on the features and true labels.\n",
    "\n",
    "    Input:\n",
    "    - X: Inputs (n features over m instances).\n",
    "    - y: True labels.\n",
    "\n",
    "    Returns a two vales:\n",
    "    - X: The mean normalized inputs.\n",
    "    - y: The mean normalized labels.\n",
    "    \"\"\"\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the normalization function.                             #\n",
    "    ###########################################################################\n",
    "\n",
    "    X = (X - X.mean(axis=0)) / (np.max(X, axis=0) - np.min(X, axis=0))\n",
    "    y = (y - y.mean(axis=0)) / (np.max(y, axis=0) - np.min(y, axis=0))\n",
    "    \n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-9bb6a28b6b6932fa",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "X, y = preprocess(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will split the data into two datasets: \n",
    "1. The training dataset will contain 80% of the data and will always be used for model training.\n",
    "2. The validation dataset will contain the remaining 20% of the data and will be used for model evaluation. For example, we will pick the best alpha and the best features using the validation dataset, while still training the model using the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and validation split\n",
    "np.random.seed(42)\n",
    "indices = np.random.permutation(X.shape[0])\n",
    "idx_train, idx_val = indices[:int(0.8*X.shape[0])], indices[int(0.8*X.shape[0]):]\n",
    "X_train, X_val = X[idx_train], X[idx_val]\n",
    "y_train, y_val = y[idx_train], y[idx_val]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0c168d036748663e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Data Visualization\n",
    "Another useful tool is data visualization. Since this problem has only two parameters, it is possible to create a two-dimensional scatter plot to visualize the data. Note that many real-world datasets are highly dimensional and cannot be visualized naively. We will be using `matplotlib` for all data visualization purposes since it offers a wide range of visualization tools and is easy to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cbad8871e083093f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1008x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X_train, y_train, 'ro', ms=1, mec='k') # the parameters control the size, shape and color of the scatter plot\n",
    "plt.ylabel('Price in USD')\n",
    "plt.xlabel('sq.ft')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c50f0a0e569142ed",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Bias Trick\n",
    "\n",
    "Make sure that `X` takes into consideration the bias $\\theta_0$ in the linear model. Hint, recall that the predications of our linear model are of the form:\n",
    "\n",
    "$$\n",
    "\\hat{y} = h_\\theta(x) = \\theta^T x = \\theta_0 + \\theta_1 x_1\n",
    "$$\n",
    "\n",
    "Add columns of ones as the zeroth column of the features (do this for both the training and validation sets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-44853962dc1651df",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "#                            START OF YOUR CODE                           #\n",
    "###########################################################################\n",
    "X_train = np.column_stack([np.ones(X_train.shape[0]), X_train])\n",
    "X_val = np.column_stack([np.ones(X_val.shape[0]), X_val])\n",
    "\n",
    "###########################################################################\n",
    "#                             END OF YOUR CODE                            #\n",
    "###########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.07464149],\n",
       "       [ 1.        , -0.02081126],\n",
       "       [ 1.        ,  0.15931296],\n",
       "       ...,\n",
       "       [ 1.        , -0.08188787],\n",
       "       [ 1.        , -0.03116323],\n",
       "       [ 1.        , -0.02598725]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c7d7fd68c1b24943",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Part 2: Single Variable Linear Regression (40 Points)\n",
    "Simple linear regression is a linear regression model with a single explanatory varaible and a single target value. \n",
    "\n",
    "$$\n",
    "\\hat{y} = h_\\theta(x) = \\theta^T x = \\theta_0 + \\theta_1 x_1\n",
    "$$\n",
    "\n",
    "## Gradient Descent \n",
    "\n",
    "Our task is to find the best possible linear line that explains all the points in our dataset. We start by guessing initial values for the linear regression parameters $\\theta$ and updating the values using gradient descent. \n",
    "\n",
    "The objective of linear regression is to minimize the cost function $J$:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{n}(h_\\theta(x^{(i)})-y^{(i)})^2\n",
    "$$\n",
    "\n",
    "where the hypothesis (model) $h_\\theta(x)$ is given by a **linear** model:\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = \\theta^T x = \\theta_0 + \\theta_1 x_1\n",
    "$$\n",
    "\n",
    "$\\theta_j$ are parameters of your model. and by changing those values accordingly you will be able to lower the cost function $J(\\theta)$. One way to accopmlish this is to use gradient descent:\n",
    "\n",
    "$$\n",
    "\\theta_j = \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)})-y^{(i)})x_j^{(i)}\n",
    "$$\n",
    "\n",
    "In linear regresion, we know that with each step of gradient descent, the parameters $\\theta_j$ get closer to the optimal values that will achieve the lowest cost $J(\\theta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0f83af93c0436542",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Implement the cost function `compute_cost`. (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 2., 2.],\n",
       "       [2., 2., 2.],\n",
       "       [2., 2., 2.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.ones((3, 3)) * 2\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4., 4., 4.],\n",
       "       [4., 4., 4.],\n",
       "       [4., 4., 4.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, theta):\n",
    "    \"\"\"\n",
    "    Computes the average squared difference between an obserbation's actual and\n",
    "    predicted values for linear regression.  \n",
    "\n",
    "    Input:\n",
    "    - X: inputs  (n features over m instances).\n",
    "    - y: true labels (1 value over m instances).\n",
    "    - theta: the parameters (weights) of the model being learned.\n",
    "\n",
    "    Returns a single value:\n",
    "    - J: the cost associated with the current set of parameters (single number).\n",
    "    \"\"\"\n",
    "    \n",
    "    J = 0  # Use J for the cost.\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the MSE cost function.                                  #\n",
    "    ###########################################################################\n",
    "\n",
    "    y_hat = np.matmul(X, theta)\n",
    "    J = np.sum( (y_hat - y)**2 ) / (X.shape[0] * 2.0)\n",
    "    \n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4c1cfec24e144479",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5110382451954535"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta = np.array([-1, 2])\n",
    "J = compute_cost(X_train, y_train, theta)\n",
    "J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-afdc527b73d275bb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Implement the gradient descent function `gradient_descent`. (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, theta, alpha, num_iters):\n",
    "    \"\"\"\n",
    "    Learn the parameters of the model using gradient descent using \n",
    "    the *training set*. Gradient descent is an optimization algorithm \n",
    "    used to minimize some (loss) function by iteratively moving in \n",
    "    the direction of steepest descent as defined by the negative of \n",
    "    the gradient. We use gradient descent to update the parameters\n",
    "    (weights) of our model.\n",
    "\n",
    "    Input:\n",
    "    - X: Inputs  (n features over m instances).\n",
    "    - y: True labels (1 value over m instances).\n",
    "    - theta: The parameters (weights) of the model being learned.\n",
    "    - alpha: The learning rate of your model.\n",
    "    - num_iters: The number of updates performed.\n",
    "\n",
    "    Returns two values:\n",
    "    - theta: The learned parameters of your model.\n",
    "    - J_history: the loss value for every iteration.\n",
    "    \"\"\"\n",
    "    \n",
    "    J_history = [] # Use a python list to save cost in every iteration\n",
    "    theta = theta.copy() # avoid changing the original thetas\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the gradient descent optimization algorithm.            #\n",
    "    ###########################################################################\n",
    "    m = X.shape[0]\n",
    "    for _ in range(num_iters):\n",
    "        # cost\n",
    "        J = compute_cost(X, y, theta)\n",
    "        J_history.append(J)\n",
    "\n",
    "        y_hat = np.matmul(X, theta)\n",
    "\n",
    "        dJ_dtheta = np.matmul(X.T, (y_hat - y)) / m \n",
    "        \n",
    "        theta = theta - alpha * dJ_dtheta\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return theta, J_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-59b95cbea13e7fc1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.94301549e-04, 4.08751964e-01]),\n",
       " [0.07244667608564788,\n",
       "  0.059090085022239225,\n",
       "  0.04827077592584294,\n",
       "  0.03950666558098808,\n",
       "  0.03240726667985373,\n",
       "  0.026656284581135458,\n",
       "  0.021997520688276852,\n",
       "  0.01822345418871241,\n",
       "  0.0151659932649833,\n",
       "  0.01268898357762763,\n",
       "  0.010682140137849605,\n",
       "  0.009056132125619266,\n",
       "  0.007738601593335899,\n",
       "  0.006670938616607399,\n",
       "  0.005805669167039374,\n",
       "  0.005104339289727086,\n",
       "  0.004535801287455524,\n",
       "  0.004074825530251001,\n",
       "  0.00370097602140111,\n",
       "  0.003397699606160233,\n",
       "  0.0031515882309867743,\n",
       "  0.002951781373675797,\n",
       "  0.0027894820118883356,\n",
       "  0.0026575645577588676,\n",
       "  0.002550257285007331,\n",
       "  0.0024628850949646253,\n",
       "  0.002391661157105737,\n",
       "  0.0023335181379240577,\n",
       "  0.0022859714963540108,\n",
       "  0.0022470087530912473,\n",
       "  0.0022149997987645966,\n",
       "  0.002188624243573623,\n",
       "  0.002166812570509875,\n",
       "  0.002148698469478078,\n",
       "  0.0021335802279440355,\n",
       "  0.0021208894573672833,\n",
       "  0.0021101657616179248,\n",
       "  0.002101036218399421,\n",
       "  0.002093198759205237,\n",
       "  0.002086408707087106,\n",
       "  0.0020804678722500655,\n",
       "  0.0020752157194866548,\n",
       "  0.0020705222138004123,\n",
       "  0.002066282025362364,\n",
       "  0.002062409835526952,\n",
       "  0.0020588365347058916,\n",
       "  0.0020555061426467632,\n",
       "  0.0020523733138593033,\n",
       "  0.00204940131701119,\n",
       "  0.00204656039823905,\n",
       "  0.002043826455430692,\n",
       "  0.002041179964393988,\n",
       "  0.002038605109053879,\n",
       "  0.0020360890769121225,\n",
       "  0.002033621488369823,\n",
       "  0.0020311939344788,\n",
       "  0.0020287996025202793,\n",
       "  0.002026432972723705,\n",
       "  0.0020240895726090376,\n",
       "  0.0020217657780040557,\n",
       "  0.002019458651868412,\n",
       "  0.0020171658137411405,\n",
       "  0.002014885333993162,\n",
       "  0.0020126156481718267,\n",
       "  0.002010355487620016,\n",
       "  0.0020081038232776104,\n",
       "  0.002005859820160695,\n",
       "  0.0020036228004897154,\n",
       "  0.0020013922138232905,\n",
       "  0.0019991676128665987,\n",
       "  0.001996948633876169,\n",
       "  0.001994734980787768,\n",
       "  0.0019925264123599796,\n",
       "  0.001990322731760505,\n",
       "  0.0019881237781310644,\n",
       "  0.00198592941975496,\n",
       "  0.001983739548522805,\n",
       "  0.0019815540754497523,\n",
       "  0.0019793729270444534,\n",
       "  0.001977196042367907,\n",
       "  0.0019750233706511207,\n",
       "  0.001972854869365417,\n",
       "  0.001970690502659373,\n",
       "  0.0019685302400927404,\n",
       "  0.0019663740556109116,\n",
       "  0.0019642219267142374,\n",
       "  0.001962073833785166,\n",
       "  0.0019599297595432248,\n",
       "  0.001957789688603546,\n",
       "  0.001955653607119272,\n",
       "  0.0019535215024918968,\n",
       "  0.0019513933631366402,\n",
       "  0.001949269178292392,\n",
       "  0.0019471489378677655,\n",
       "  0.0019450326323163973,\n",
       "  0.0019429202525359306,\n",
       "  0.001940811789786192,\n",
       "  0.0019387072356229055,\n",
       "  0.0019366065818440025,\n",
       "  0.0019345098204461187,\n",
       "  0.0019324169435893626,\n",
       "  0.001930327943568766,\n",
       "  0.0019282428127911557,\n",
       "  0.0019261615437564182,\n",
       "  0.0019240841290423143,\n",
       "  0.0019220105612921788,\n",
       "  0.0019199408332049495,\n",
       "  0.0019178749375270898,\n",
       "  0.0019158128670460357,\n",
       "  0.00191375461458489,\n",
       "  0.0019117001729981138,\n",
       "  0.0019096495351680364,\n",
       "  0.0019076026940020225,\n",
       "  0.0019055596424301714,\n",
       "  0.001903520373403451,\n",
       "  0.0019014848798921827,\n",
       "  0.0018994531548848044,\n",
       "  0.0018974251913868702,\n",
       "  0.001895400982420233,\n",
       "  0.001893380521022376,\n",
       "  0.00189136380024587,\n",
       "  0.001889350813157929,\n",
       "  0.0018873415528400416,\n",
       "  0.001885336012387674,\n",
       "  0.0018833341849100206,\n",
       "  0.001881336063529799,\n",
       "  0.0018793416413830802,\n",
       "  0.0018773509116191444,\n",
       "  0.0018753638674003635,\n",
       "  0.0018733805019020963,\n",
       "  0.0018714008083126044,\n",
       "  0.0018694247798329772,\n",
       "  0.0018674524096770652,\n",
       "  0.0018654836910714265,\n",
       "  0.0018635186172552718,\n",
       "  0.001861557181480423,\n",
       "  0.0018595993770112705,\n",
       "  0.0018576451971247354,\n",
       "  0.0018556946351102348,\n",
       "  0.0018537476842696507,\n",
       "  0.0018518043379172973,\n",
       "  0.0018498645893798927,\n",
       "  0.001847928431996531,\n",
       "  0.001845995859118655,\n",
       "  0.001844066864110031,\n",
       "  0.0018421414403467216,\n",
       "  0.0018402195812170623,\n",
       "  0.001838301280121637,\n",
       "  0.0018363865304732527,\n",
       "  0.001834475325696917,\n",
       "  0.0018325676592298135,\n",
       "  0.0018306635245212797,\n",
       "  0.0018287629150327825,\n",
       "  0.0018268658242378967,\n",
       "  0.001824972245622282,\n",
       "  0.001823082172683658,\n",
       "  0.0018211955989317849,\n",
       "  0.0018193125178884398,\n",
       "  0.001817432923087392,\n",
       "  0.0018155568080743848,\n",
       "  0.0018136841664071102,\n",
       "  0.001811814991655187,\n",
       "  0.0018099492774001401,\n",
       "  0.0018080870172353775,\n",
       "  0.0018062282047661671,\n",
       "  0.0018043728336096171,\n",
       "  0.0018025208973946527,\n",
       "  0.0018006723897619936,\n",
       "  0.0017988273043641335,\n",
       "  0.001796985634865317,\n",
       "  0.0017951473749415186,\n",
       "  0.0017933125182804219,\n",
       "  0.0017914810585813949,\n",
       "  0.001789652989555472,\n",
       "  0.00178782830492533,\n",
       "  0.0017860069984252673,\n",
       "  0.0017841890638011831,\n",
       "  0.0017823744948105534,\n",
       "  0.0017805632852224127,\n",
       "  0.0017787554288173303,\n",
       "  0.0017769509193873913,\n",
       "  0.0017751497507361722,\n",
       "  0.0017733519166787221,\n",
       "  0.0017715574110415404,\n",
       "  0.001769766227662556,\n",
       "  0.0017679783603911066,\n",
       "  0.0017661938030879144,\n",
       "  0.0017644125496250713,\n",
       "  0.0017626345938860104,\n",
       "  0.0017608599297654916,\n",
       "  0.0017590885511695755,\n",
       "  0.0017573204520156067,\n",
       "  0.0017555556262321898,\n",
       "  0.00175379406775917,\n",
       "  0.0017520357705476115,\n",
       "  0.001750280728559778,\n",
       "  0.00174852893576911,\n",
       "  0.0017467803861602068,\n",
       "  0.0017450350737288026,\n",
       "  0.0017432929924817496,\n",
       "  0.0017415541364369936,\n",
       "  0.001739818499623556,\n",
       "  0.0017380860760815126,\n",
       "  0.0017363568598619736,\n",
       "  0.0017346308450270618,\n",
       "  0.0017329080256498933,\n",
       "  0.0017311883958145567,\n",
       "  0.0017294719496160943,\n",
       "  0.001727758681160479,\n",
       "  0.0017260485845645968,\n",
       "  0.0017243416539562237,\n",
       "  0.00172263788347401,\n",
       "  0.0017209372672674548,\n",
       "  0.0017192397994968897,\n",
       "  0.0017175454743334575,\n",
       "  0.0017158542859590916,\n",
       "  0.0017141662285664982,\n",
       "  0.001712481296359133,\n",
       "  0.001710799483551185,\n",
       "  0.0017091207843675533,\n",
       "  0.001707445193043829,\n",
       "  0.0017057727038262748,\n",
       "  0.001704103310971807,\n",
       "  0.0017024370087479733,\n",
       "  0.001700773791432934,\n",
       "  0.0016991136533154431,\n",
       "  0.0016974565886948285,\n",
       "  0.0016958025918809704,\n",
       "  0.0016941516571942854,\n",
       "  0.001692503778965703,\n",
       "  0.0016908589515366504,\n",
       "  0.0016892171692590283,\n",
       "  0.0016875784264951963,\n",
       "  0.0016859427176179496,\n",
       "  0.0016843100370105021,\n",
       "  0.0016826803790664668,\n",
       "  0.0016810537381898348,\n",
       "  0.0016794301087949578,\n",
       "  0.0016778094853065301,\n",
       "  0.0016761918621595651,\n",
       "  0.0016745772337993805,\n",
       "  0.0016729655946815779,\n",
       "  0.001671356939272023,\n",
       "  0.001669751262046827,\n",
       "  0.0016681485574923285,\n",
       "  0.0016665488201050725,\n",
       "  0.0016649520443917942,\n",
       "  0.0016633582248693983,\n",
       "  0.0016617673560649403,\n",
       "  0.001660179432515608,\n",
       "  0.0016585944487687032,\n",
       "  0.0016570123993816229,\n",
       "  0.0016554332789218383,\n",
       "  0.001653857081966881,\n",
       "  0.0016522838031043196,\n",
       "  0.0016507134369317428,\n",
       "  0.001649145978056743,\n",
       "  0.0016475814210968934,\n",
       "  0.0016460197606797333,\n",
       "  0.0016444609914427484,\n",
       "  0.0016429051080333514,\n",
       "  0.001641352105108866,\n",
       "  0.0016398019773365053,\n",
       "  0.001638254719393357,\n",
       "  0.0016367103259663617,\n",
       "  0.0016351687917522976,\n",
       "  0.0016336301114577606,\n",
       "  0.001632094279799146,\n",
       "  0.0016305612915026315,\n",
       "  0.0016290311413041584,\n",
       "  0.0016275038239494139,\n",
       "  0.0016259793341938127,\n",
       "  0.0016244576668024782,\n",
       "  0.0016229388165502256,\n",
       "  0.0016214227782215451,\n",
       "  0.0016199095466105816,\n",
       "  0.0016183991165211178,\n",
       "  0.0016168914827665568,\n",
       "  0.0016153866401699033,\n",
       "  0.0016138845835637467,\n",
       "  0.0016123853077902438,\n",
       "  0.0016108888077010994,\n",
       "  0.0016093950781575499,\n",
       "  0.0016079041140303456,\n",
       "  0.0016064159101997326,\n",
       "  0.0016049304615554354,\n",
       "  0.0016034477629966396,\n",
       "  0.0016019678094319747,\n",
       "  0.0016004905957794945,\n",
       "  0.0015990161169666635,\n",
       "  0.0015975443679303349,\n",
       "  0.0015960753436167372,\n",
       "  0.0015946090389814542,\n",
       "  0.0015931454489894103,\n",
       "  0.0015916845686148493,\n",
       "  0.0015902263928413212,\n",
       "  0.0015887709166616624,\n",
       "  0.0015873181350779797,\n",
       "  0.001585868043101632,\n",
       "  0.001584420635753215,\n",
       "  0.0015829759080625428,\n",
       "  0.0015815338550686298,\n",
       "  0.0015800944718196766,\n",
       "  0.0015786577533730508,\n",
       "  0.0015772236947952693,\n",
       "  0.001575792291161985,\n",
       "  0.0015743635375579654,\n",
       "  0.001572937429077078,\n",
       "  0.0015715139608222745,\n",
       "  0.0015700931279055723,\n",
       "  0.0015686749254480378,\n",
       "  0.0015672593485797693,\n",
       "  0.0015658463924398825,\n",
       "  0.0015644360521764919,\n",
       "  0.0015630283229466939,\n",
       "  0.0015616231999165515,\n",
       "  0.001560220678261076,\n",
       "  0.0015588207531642135,\n",
       "  0.0015574234198188232,\n",
       "  0.0015560286734266674,\n",
       "  0.0015546365091983882,\n",
       "  0.0015532469223534977,\n",
       "  0.0015518599081203565,\n",
       "  0.0015504754617361595,\n",
       "  0.0015490935784469191,\n",
       "  0.0015477142535074491,\n",
       "  0.0015463374821813492,\n",
       "  0.0015449632597409862,\n",
       "  0.0015435915814674805,\n",
       "  0.0015422224426506887,\n",
       "  0.0015408558385891866,\n",
       "  0.001539491764590256,\n",
       "  0.0015381302159698641,\n",
       "  0.001536771188052652,\n",
       "  0.0015354146761719148,\n",
       "  0.0015340606756695887,\n",
       "  0.0015327091818962328,\n",
       "  0.001531360190211014,\n",
       "  0.0015300136959816918,\n",
       "  0.0015286696945846002,\n",
       "  0.0015273281814046343,\n",
       "  0.001525989151835233,\n",
       "  0.0015246526012783637,\n",
       "  0.0015233185251445065,\n",
       "  0.0015219869188526378,\n",
       "  0.0015206577778302157,\n",
       "  0.0015193310975131625,\n",
       "  0.0015180068733458524,\n",
       "  0.0015166851007810905,\n",
       "  0.0015153657752801036,\n",
       "  0.0015140488923125206,\n",
       "  0.001512734447356355,\n",
       "  0.001511422435897996,\n",
       "  0.0015101128534321853,\n",
       "  0.0015088056954620087,\n",
       "  0.0015075009574988755,\n",
       "  0.001506198635062506,\n",
       "  0.0015048987236809135,\n",
       "  0.0015036012188903922,\n",
       "  0.001502306116235499,\n",
       "  0.0015010134112690404,\n",
       "  0.0014997230995520544,\n",
       "  0.0014984351766538003,\n",
       "  0.001497149638151737,\n",
       "  0.0014958664796315127,\n",
       "  0.0014945856966869478,\n",
       "  0.0014933072849200198,\n",
       "  0.0014920312399408484,\n",
       "  0.001490757557367682,\n",
       "  0.0014894862328268792,\n",
       "  0.0014882172619528965,\n",
       "  0.001486950640388273,\n",
       "  0.0014856863637836145,\n",
       "  0.0014844244277975784,\n",
       "  0.0014831648280968608,\n",
       "  0.00148190756035618,\n",
       "  0.001480652620258261,\n",
       "  0.0014794000034938218,\n",
       "  0.0014781497057615599,\n",
       "  0.0014769017227681336,\n",
       "  0.0014756560502281516,\n",
       "  0.0014744126838641557,\n",
       "  0.0014731716194066062,\n",
       "  0.001471932852593869,\n",
       "  0.0014706963791721987,\n",
       "  0.0014694621948957258,\n",
       "  0.001468230295526441,\n",
       "  0.001467000676834181,\n",
       "  0.001465773334596614,\n",
       "  0.001464548264599226,\n",
       "  0.0014633254626353045,\n",
       "  0.0014621049245059255,\n",
       "  0.0014608866460199387,\n",
       "  0.0014596706229939532,\n",
       "  0.0014584568512523229,\n",
       "  0.001457245326627132,\n",
       "  0.0014560360449581812,\n",
       "  0.0014548290020929737,\n",
       "  0.0014536241938866996,\n",
       "  0.0014524216162022233,\n",
       "  0.001451221264910068,\n",
       "  0.0014500231358884007,\n",
       "  0.0014488272250230228,\n",
       "  0.0014476335282073488,\n",
       "  0.001446442041342398,\n",
       "  0.0014452527603367785,\n",
       "  0.0014440656811066717,\n",
       "  0.0014428807995758204,\n",
       "  0.0014416981116755138,\n",
       "  0.0014405176133445744,\n",
       "  0.0014393393005293423,\n",
       "  0.0014381631691836622,\n",
       "  0.001436989215268871,\n",
       "  0.0014358174347537818,\n",
       "  0.0014346478236146702,\n",
       "  0.0014334803778352622,\n",
       "  0.0014323150934067188,\n",
       "  0.0014311519663276228,\n",
       "  0.001429990992603965,\n",
       "  0.0014288321682491309,\n",
       "  0.0014276754892838866,\n",
       "  0.001426520951736364,\n",
       "  0.0014253685516420495,\n",
       "  0.001424218285043769,\n",
       "  0.001423070147991674,\n",
       "  0.00142192413654323,\n",
       "  0.0014207802467631997,\n",
       "  0.0014196384747236315,\n",
       "  0.0014184988165038476,\n",
       "  0.0014173612681904267,\n",
       "  0.0014162258258771936,\n",
       "  0.001415092485665205,\n",
       "  0.001413961243662735,\n",
       "  0.0014128320959852633,\n",
       "  0.0014117050387554616,\n",
       "  0.0014105800681031785,\n",
       "  0.0014094571801654297,\n",
       "  0.0014083363710863806,\n",
       "  0.0014072176370173357,\n",
       "  0.0014061009741167258,\n",
       "  0.0014049863785500925,\n",
       "  0.0014038738464900765,\n",
       "  0.0014027633741164056,\n",
       "  0.0014016549576158785,\n",
       "  0.0014005485931823545,\n",
       "  0.0013994442770167389,\n",
       "  0.0013983420053269708,\n",
       "  0.0013972417743280102,\n",
       "  0.001396143580241823,\n",
       "  0.0013950474192973709,\n",
       "  0.0013939532877305975,\n",
       "  0.0013928611817844128,\n",
       "  0.001391771097708685,\n",
       "  0.0013906830317602236,\n",
       "  0.0013895969802027687,\n",
       "  0.001388512939306977,\n",
       "  0.0013874309053504104,\n",
       "  0.001386350874617521,\n",
       "  0.001385272843399641,\n",
       "  0.001384196807994968,\n",
       "  0.0013831227647085526,\n",
       "  0.0013820507098522867,\n",
       "  0.00138098063974489,\n",
       "  0.0013799125507118974,\n",
       "  0.001378846439085647,\n",
       "  0.001377782301205266,\n",
       "  0.0013767201334166598,\n",
       "  0.0013756599320724995,\n",
       "  0.0013746016935322074,\n",
       "  0.0013735454141619466,\n",
       "  0.0013724910903346074,\n",
       "  0.0013714387184297946,\n",
       "  0.0013703882948338172,\n",
       "  0.0013693398159396728,\n",
       "  0.0013682932781470378,\n",
       "  0.001367248677862254,\n",
       "  0.001366206011498315,\n",
       "  0.0013651652754748571,\n",
       "  0.0013641264662181442,\n",
       "  0.0013630895801610562,\n",
       "  0.001362054613743077,\n",
       "  0.001361021563410283,\n",
       "  0.00135999042561533,\n",
       "  0.0013589611968174405,\n",
       "  0.0013579338734823923,\n",
       "  0.0013569084520825068,\n",
       "  0.0013558849290966361,\n",
       "  0.0013548633010101515,\n",
       "  0.0013538435643149307,\n",
       "  0.0013528257155093462,\n",
       "  0.0013518097510982526,\n",
       "  0.0013507956675929775,\n",
       "  0.0013497834615113047,\n",
       "  0.0013487731293774661,\n",
       "  0.0013477646677221282,\n",
       "  0.00134675807308238,\n",
       "  0.0013457533420017226,\n",
       "  0.0013447504710300557,\n",
       "  0.0013437494567236656,\n",
       "  0.001342750295645215,\n",
       "  0.001341752984363731,\n",
       "  0.0013407575194545913,\n",
       "  0.0013397638974995143,\n",
       "  0.0013387721150865466,\n",
       "  0.001337782168810052,\n",
       "  0.0013367940552706985,\n",
       "  0.0013358077710754488,\n",
       "  0.0013348233128375462,\n",
       "  0.0013338406771765039,\n",
       "  0.0013328598607180945,\n",
       "  0.0013318808600943375,\n",
       "  0.0013309036719434863,\n",
       "  0.0013299282929100193,\n",
       "  0.001328954719644627,\n",
       "  0.0013279829488042,\n",
       "  0.0013270129770518197,\n",
       "  0.0013260448010567435,\n",
       "  0.001325078417494396,\n",
       "  0.0013241138230463564,\n",
       "  0.0013231510144003482,\n",
       "  0.0013221899882502265,\n",
       "  0.0013212307412959676,\n",
       "  0.001320273270243657,\n",
       "  0.001319317571805478,\n",
       "  0.0013183636426997013,\n",
       "  0.0013174114796506739,\n",
       "  0.0013164610793888055,\n",
       "  0.0013155124386505608,\n",
       "  0.0013145655541784448,\n",
       "  0.0013136204227209944,\n",
       "  0.0013126770410327655,\n",
       "  0.001311735405874323,\n",
       "  0.001310795514012228,\n",
       "  0.00130985736221903,\n",
       "  0.001308920947273251,\n",
       "  0.0013079862659593788,\n",
       "  0.0013070533150678538,\n",
       "  0.0013061220913950591,\n",
       "  0.001305192591743307,\n",
       "  0.0013042648129208316,\n",
       "  0.0013033387517417752,\n",
       "  0.0013024144050261786,\n",
       "  0.0013014917695999696,\n",
       "  0.0013005708422949526,\n",
       "  0.0012996516199487967,\n",
       "  0.0012987340994050267,\n",
       "  0.0012978182775130107,\n",
       "  0.0012969041511279493,\n",
       "  0.0012959917171108655,\n",
       "  0.0012950809723285934,\n",
       "  0.0012941719136537685,\n",
       "  0.001293264537964816,\n",
       "  0.0012923588421459384,\n",
       "  0.0012914548230871094,\n",
       "  0.0012905524776840585,\n",
       "  0.0012896518028382626,\n",
       "  0.0012887527954569364,\n",
       "  0.001287855452453018,\n",
       "  0.0012869597707451635,\n",
       "  0.0012860657472577313,\n",
       "  0.001285173378920775,\n",
       "  0.0012842826626700318,\n",
       "  0.0012833935954469117,\n",
       "  0.0012825061741984875,\n",
       "  0.0012816203958774832,\n",
       "  0.001280736257442265,\n",
       "  0.0012798537558568308,\n",
       "  0.0012789728880907977,\n",
       "  0.001278093651119395,\n",
       "  0.00127721604192345,\n",
       "  0.0012763400574893808,\n",
       "  0.0012754656948091847,\n",
       "  0.0012745929508804269,\n",
       "  0.0012737218227062321,\n",
       "  0.0012728523072952729,\n",
       "  0.0012719844016617597,\n",
       "  0.001271118102825431,\n",
       "  0.0012702534078115424,\n",
       "  0.0012693903136508577,\n",
       "  0.0012685288173796367,\n",
       "  0.0012676689160396263,\n",
       "  0.0012668106066780513,\n",
       "  0.0012659538863476017,\n",
       "  0.0012650987521064243,\n",
       "  0.001264245201018113,\n",
       "  0.0012633932301516972,\n",
       "  0.0012625428365816336,\n",
       "  0.0012616940173877942,\n",
       "  0.001260846769655457,\n",
       "  0.0012600010904752965,\n",
       "  0.001259156976943374,\n",
       "  0.0012583144261611263,\n",
       "  0.0012574734352353565,\n",
       "  0.0012566340012782242,\n",
       "  0.0012557961214072354,\n",
       "  0.0012549597927452322,\n",
       "  0.0012541250124203844,\n",
       "  0.0012532917775661776,\n",
       "  0.0012524600853214044,\n",
       "  0.0012516299328301542,\n",
       "  0.001250801317241805,\n",
       "  0.001249974235711011,\n",
       "  0.001249148685397695,\n",
       "  0.0012483246634670366,\n",
       "  0.0012475021670894653,\n",
       "  0.0012466811934406475,\n",
       "  0.0012458617397014796,\n",
       "  0.0012450438030580768,\n",
       "  0.001244227380701763,\n",
       "  0.0012434124698290638,\n",
       "  0.0012425990676416929,\n",
       "  0.0012417871713465453,\n",
       "  0.0012409767781556883,\n",
       "  0.0012401678852863491,\n",
       "  0.0012393604899609075,\n",
       "  0.0012385545894068853,\n",
       "  0.0012377501808569368,\n",
       "  0.0012369472615488407,\n",
       "  0.0012361458287254887,\n",
       "  0.0012353458796348774,\n",
       "  0.0012345474115300979,\n",
       "  0.0012337504216693267,\n",
       "  0.0012329549073158167,\n",
       "  0.0012321608657378876,\n",
       "  0.001231368294208916,\n",
       "  0.0012305771900073266,\n",
       "  0.001229787550416583,\n",
       "  0.0012289993727251773,\n",
       "  0.0012282126542266223,\n",
       "  0.001227427392219441,\n",
       "  0.0012266435840071579,\n",
       "  0.00122586122689829,\n",
       "  0.0012250803182063366,\n",
       "  0.001224300855249771,\n",
       "  0.0012235228353520303,\n",
       "  0.0012227462558415084,\n",
       "  0.0012219711140515435,\n",
       "  0.0012211974073204118,\n",
       "  0.001220425132991317,\n",
       "  0.0012196542884123814,\n",
       "  0.0012188848709366368,\n",
       "  0.0012181168779220152,\n",
       "  0.0012173503067313411,\n",
       "  0.0012165851547323203,\n",
       "  0.0012158214192975318,\n",
       "  0.0012150590978044203,\n",
       "  0.0012142981876352836,\n",
       "  0.0012135386861772672,\n",
       "  0.0012127805908223538,\n",
       "  0.0012120238989673546,\n",
       "  0.0012112686080138991,\n",
       "  0.0012105147153684288,\n",
       "  0.0012097622184421856,\n",
       "  0.0012090111146512046,\n",
       "  0.001208261401416305,\n",
       "  0.0012075130761630803,\n",
       "  0.0012067661363218908,\n",
       "  0.0012060205793278538,\n",
       "  0.0012052764026208346,\n",
       "  0.001204533603645439,\n",
       "  0.0012037921798510039,\n",
       "  0.0012030521286915871,\n",
       "  0.001202313447625961,\n",
       "  0.0012015761341176022,\n",
       "  0.001200840185634683,\n",
       "  0.0012001055996500646,\n",
       "  0.0011993723736412848,\n",
       "  0.0011986405050905522,\n",
       "  0.0011979099914847366,\n",
       "  0.0011971808303153611,\n",
       "  0.001196453019078592,\n",
       "  0.0011957265552752312,\n",
       "  0.0011950014364107081,\n",
       "  0.0011942776599950702,\n",
       "  0.0011935552235429743,\n",
       "  0.0011928341245736785,\n",
       "  0.0011921143606110345,\n",
       "  0.0011913959291834767,\n",
       "  0.0011906788278240174,\n",
       "  0.0011899630540702338,\n",
       "  0.0011892486054642637,\n",
       "  0.0011885354795527939,\n",
       "  0.0011878236738870544,\n",
       "  0.001187113186022808,\n",
       "  0.0011864040135203427,\n",
       "  0.0011856961539444633,\n",
       "  0.0011849896048644825,\n",
       "  0.0011842843638542146,\n",
       "  0.0011835804284919637,\n",
       "  0.001182877796360519,\n",
       "  0.0011821764650471432,\n",
       "  0.0011814764321435671,\n",
       "  0.00118077769524598,\n",
       "  0.0011800802519550202,\n",
       "  0.0011793840998757699,\n",
       "  0.001178689236617743,\n",
       "  0.001177995659794881,\n",
       "  0.001177303367025542,\n",
       "  0.0011766123559324932,\n",
       "  0.0011759226241429028,\n",
       "  0.0011752341692883325,\n",
       "  0.0011745469890047286,\n",
       "  0.0011738610809324144,\n",
       "  0.0011731764427160805,\n",
       "  0.0011724930720047792,\n",
       "  0.0011718109664519152,\n",
       "  0.0011711301237152373,\n",
       "  0.0011704505414568303,\n",
       "  0.0011697722173431078,\n",
       "  0.0011690951490448036,\n",
       "  0.0011684193342369634,\n",
       "  0.0011677447705989374,\n",
       "  0.0011670714558143728,\n",
       "  0.0011663993875712046,\n",
       "  0.0011657285635616476,\n",
       "  0.0011650589814821903,\n",
       "  0.0011643906390335851,\n",
       "  0.0011637235339208413,\n",
       "  0.0011630576638532173,\n",
       "  0.0011623930265442118,\n",
       "  0.0011617296197115575,\n",
       "  0.001161067441077211,\n",
       "  0.001160406488367348,\n",
       "  0.0011597467593123527,\n",
       "  0.0011590882516468119,\n",
       "  0.001158430963109506,\n",
       "  0.0011577748914434018,\n",
       "  0.0011571200343956445,\n",
       "  0.0011564663897175502,\n",
       "  0.0011558139551645983,\n",
       "  0.0011551627284964242,\n",
       "  0.001154512707476809,\n",
       "  0.001153863889873676,\n",
       "  0.00115321627345908,\n",
       "  0.0011525698560091997,\n",
       "  0.0011519246353043325,\n",
       "  0.001151280609128884,\n",
       "  0.0011506377752713624,\n",
       "  0.0011499961315243692,\n",
       "  0.0011493556756845935,\n",
       "  0.0011487164055528033,\n",
       "  0.0011480783189338375,\n",
       "  0.0011474414136366001,\n",
       "  0.0011468056874740507,\n",
       "  0.001146171138263198,\n",
       "  0.0011455377638250924,\n",
       "  0.0011449055619848186,\n",
       "  0.0011442745305714872,\n",
       "  0.0011436446674182284,\n",
       "  0.0011430159703621834,\n",
       "  0.0011423884372444982,\n",
       "  0.0011417620659103153,\n",
       "  0.0011411368542087671,\n",
       "  0.0011405127999929675,\n",
       "  0.0011398899011200048,\n",
       "  0.0011392681554509348,\n",
       "  0.0011386475608507738,\n",
       "  0.0011380281151884902,\n",
       "  0.0011374098163369972,\n",
       "  0.0011367926621731472,\n",
       "  0.0011361766505777226,\n",
       "  0.001135561779435429,\n",
       "  0.0011349480466348878,\n",
       "  0.001134335450068631,\n",
       "  0.0011337239876330902,\n",
       "  0.0011331136572285932,\n",
       "  0.0011325044567593533,\n",
       "  0.0011318963841334656,\n",
       "  0.0011312894372628964,\n",
       "  0.0011306836140634793,\n",
       "  0.001130078912454905,\n",
       "  0.0011294753303607164,\n",
       "  0.0011288728657083001,\n",
       "  0.0011282715164288805,\n",
       "  0.001127671280457512,\n",
       "  0.0011270721557330712,\n",
       "  0.001126474140198251,\n",
       "  0.001125877231799553,\n",
       "  0.001125281428487281,\n",
       "  0.0011246867282155325,\n",
       "  0.001124093128942194,\n",
       "  0.0011235006286289318,\n",
       "  0.0011229092252411856,\n",
       "  0.0011223189167481624,\n",
       "  0.0011217297011228292,\n",
       "  0.001121141576341905,\n",
       "  0.0011205545403858547,\n",
       "  0.0011199685912388823,\n",
       "  0.001119383726888924,\n",
       "  0.0011187999453276408,\n",
       "  0.0011182172445504115,\n",
       "  0.0011176356225563264,\n",
       "  0.0011170550773481804,\n",
       "  0.0011164756069324655,\n",
       "  0.0011158972093193648,\n",
       "  0.0011153198825227448,\n",
       "  0.0011147436245601495,\n",
       "  0.0011141684334527932,\n",
       "  0.001113594307225553,\n",
       "  0.001113021243906963,\n",
       "  0.0011124492415292074,\n",
       "  0.0011118782981281129,\n",
       "  0.0011113084117431431,\n",
       "  0.0011107395804173911,\n",
       "  0.0011101718021975732,\n",
       "  0.0011096050751340215,\n",
       "  0.001109039397280677,\n",
       "  0.0011084747666950852,\n",
       "  0.0011079111814383867,\n",
       "  0.0011073486395753113,\n",
       "  0.0011067871391741723,\n",
       "  0.0011062266783068593,\n",
       "  0.0011056672550488309,\n",
       "  0.0011051088674791094,\n",
       "  0.001104551513680273,\n",
       "  0.0011039951917384497,\n",
       "  0.0011034398997433108,\n",
       "  0.001102885635788065,\n",
       "  0.0011023323979694496,\n",
       "  0.0011017801843877274,\n",
       "  0.0011012289931466768,\n",
       "  0.0011006788223535875,\n",
       "  0.0011001296701192527,\n",
       "  0.001099581534557964,\n",
       "  0.0010990344137875034,\n",
       "  0.0010984883059291383,\n",
       "  0.0010979432091076126,\n",
       "  0.0010973991214511445,\n",
       "  0.001096856041091415,\n",
       "  0.0010963139661635658,\n",
       "  0.0010957728948061908,\n",
       "  0.0010952328251613293,\n",
       "  0.0010946937553744608,\n",
       "  0.0010941556835944975,\n",
       "  0.00109361860797378,\n",
       "  0.0010930825266680685,\n",
       "  0.0010925474378365382,\n",
       "  0.0010920133396417717,\n",
       "  0.001091480230249753,\n",
       "  0.0010909481078298624,\n",
       "  0.0010904169705548692,\n",
       "  0.0010898868166009249,\n",
       "  0.001089357644147558,\n",
       "  0.001088829451377668,\n",
       "  0.0010883022364775168,\n",
       "  0.0010877759976367256,\n",
       "  0.001087250733048267,\n",
       "  0.0010867264409084587,\n",
       "  0.0010862031194169574,\n",
       "  0.0010856807667767535,\n",
       "  0.0010851593811941644,\n",
       "  0.0010846389608788276,\n",
       "  0.0010841195040436958,\n",
       "  0.0010836010089050293,\n",
       "  0.0010830834736823914,\n",
       "  0.0010825668965986416,\n",
       "  0.0010820512758799293,\n",
       "  0.0010815366097556877,\n",
       "  0.0010810228964586285,\n",
       "  0.0010805101342247357,\n",
       "  0.0010799983212932572,\n",
       "  0.0010794874559067024,\n",
       "  0.0010789775363108341,\n",
       "  0.0010784685607546623,\n",
       "  0.0010779605274904391,\n",
       "  0.0010774534347736523,\n",
       "  0.0010769472808630194,\n",
       "  0.001076442064020481,\n",
       "  0.0010759377825111964,\n",
       "  0.0010754344346035364,\n",
       "  0.0010749320185690777,\n",
       "  0.001074430532682596,\n",
       "  0.0010739299752220623,\n",
       "  0.001073430344468635,\n",
       "  0.0010729316387066547,\n",
       "  0.0010724338562236389,\n",
       "  0.0010719369953102742,\n",
       "  0.001071441054260413,\n",
       "  0.0010709460313710654,\n",
       "  0.0010704519249423957,\n",
       "  0.001069958733277714,\n",
       "  0.0010694664546834715,\n",
       "  0.0010689750874692554,\n",
       "  0.0010684846299477825,\n",
       "  0.0010679950804348929,\n",
       "  0.0010675064372495443,\n",
       "  0.0010670186987138078,\n",
       "  0.0010665318631528599,\n",
       "  0.001066045928894978,\n",
       "  0.0010655608942715348,\n",
       "  0.0010650767576169922,\n",
       "  0.0010645935172688948,\n",
       "  0.0010641111715678662,\n",
       "  0.0010636297188576012,\n",
       "  0.0010631491574848609,\n",
       "  0.001062669485799468,\n",
       "  0.0010621907021542989,\n",
       "  0.0010617128049052816,\n",
       "  0.0010612357924113854,\n",
       "  0.0010607596630346193,\n",
       "  0.0010602844151400242,\n",
       "  0.0010598100470956679,\n",
       "  0.00105933655727264,\n",
       "  0.0010588639440450446,\n",
       "  0.001058392205789997,\n",
       "  0.001057921340887617,\n",
       "  0.0010574513477210226,\n",
       "  0.001056982224676326,\n",
       "  0.0010565139701426268,\n",
       "  0.0010560465825120069,\n",
       "  0.0010555800601795256,\n",
       "  0.0010551144015432125,\n",
       "  0.0010546496050040646,\n",
       "  0.001054185668966037,\n",
       "  0.0010537225918360418,\n",
       "  0.001053260372023939,\n",
       "  0.001052799007942533,\n",
       "  0.0010523384980075674,\n",
       "  0.0010518788406377174,\n",
       "  0.0010514200342545865,\n",
       "  0.0010509620772827008,\n",
       "  0.001050504968149502,\n",
       "  0.0010500487052853447,\n",
       "  0.001049593287123488,\n",
       "  0.0010491387121000927,\n",
       "  0.0010486849786542137,\n",
       "  0.001048232085227797,\n",
       "  0.0010477800302656718,\n",
       "  0.0010473288122155478,\n",
       "  0.0010468784295280076,\n",
       "  0.0010464288806565024,\n",
       "  0.0010459801640573475,\n",
       "  0.001045532278189715,\n",
       "  0.0010450852215156293,\n",
       "  0.0010446389924999638,\n",
       "  0.0010441935896104326,\n",
       "  0.0010437490113175871,\n",
       "  0.00104330525609481,\n",
       "  0.0010428623224183107,\n",
       "  0.0010424202087671193,\n",
       "  0.0010419789136230815,\n",
       "  0.0010415384354708545,\n",
       "  0.0010410987727979004,\n",
       "  0.0010406599240944815,\n",
       "  0.0010402218878536548,\n",
       "  0.001039784662571268,\n",
       "  0.0010393482467459531,\n",
       "  0.0010389126388791216,\n",
       "  0.0010384778374749595,\n",
       "  0.0010380438410404222,\n",
       "  0.001037610648085229,\n",
       "  0.0010371782571218578,\n",
       "  0.0010367466666655414,\n",
       "  0.0010363158752342617,\n",
       "  0.001035885881348742,\n",
       "  0.0010354566835324474,\n",
       "  0.0010350282803115744,\n",
       "  0.0010346006702150485,\n",
       "  0.0010341738517745194,\n",
       "  0.001033747823524354,\n",
       "  0.0010333225840016343,\n",
       "  0.0010328981317461483,\n",
       "  0.0010324744653003896,\n",
       "  0.0010320515832095494,\n",
       "  0.0010316294840215115,\n",
       "  0.0010312081662868493,\n",
       "  0.001030787628558819,\n",
       "  0.0010303678693933547,\n",
       "  0.001029948887349065,\n",
       "  0.0010295306809872265,\n",
       "  0.0010291132488717792,\n",
       "  0.0010286965895693221,\n",
       "  0.0010282807016491079,\n",
       "  0.0010278655836830374,\n",
       "  0.0010274512342456567,\n",
       "  0.0010270376519141498,\n",
       "  0.0010266248352683353,\n",
       "  0.0010262127828906604,\n",
       "  0.001025801493366198,\n",
       "  0.0010253909652826391,\n",
       "  0.0010249811972302902,\n",
       "  0.0010245721878020676,\n",
       "  0.0010241639355934922,\n",
       "  0.0010237564392026855,\n",
       "  0.0010233496972303642,\n",
       "  0.0010229437082798353,\n",
       "  0.001022538470956992,\n",
       "  0.0010221339838703085,\n",
       "  0.0010217302456308347,\n",
       "  0.0010213272548521921,\n",
       "  0.0010209250101505696,\n",
       "  0.001020523510144717,\n",
       "  0.0010201227534559418,\n",
       "  0.0010197227387081038,\n",
       "  0.001019323464527611,\n",
       "  0.0010189249295434134,\n",
       "  0.001018527132387,\n",
       "  0.001018130071692393,\n",
       "  0.0010177337460961446,\n",
       "  0.0010173381542373297,\n",
       "  ...])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "theta = np.random.random(size=2)\n",
    "iterations = 40000\n",
    "alpha = 0.1\n",
    "theta, J_history = gradient_descent(X_train ,y_train, theta, alpha, iterations)\n",
    "theta, J_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-86125cd57f0fdb89",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "You can evaluate the learning process by monitoring the loss as training progress. In the following graph, we visualize the loss as a function of the iterations. This is possible since we are saving the loss value at every iteration in the `J_history` array. This visualization might help you find problems with your code. Notice that since the network converges quickly, we are using logarithmic scale for the number of iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a565f1f721f6377f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1008x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(iterations), J_history)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss as a function of iterations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3bdd058ecc5db0eb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Implement the pseudo-inverse function `pinv`. **Do not use `np.linalg.pinv`**, instead use only direct matrix multiplication as you saw in class (you can calculate the inverse of a matrix using `np.linalg.inv`). (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pinv(X, y):\n",
    "    \"\"\"\n",
    "    Calculate the optimal values of the parameters using the pseudoinverse\n",
    "    approach as you saw in class using the *training set*.\n",
    "\n",
    "    Input:\n",
    "    - X: Inputs  (n features over m instances).\n",
    "    - y: True labels (1 value over m instances).\n",
    "\n",
    "    Returns two values:\n",
    "    - theta: The optimal parameters of your model.\n",
    "\n",
    "    ########## DO NOT USE np.linalg.pinv ##############\n",
    "    \"\"\"\n",
    "    \n",
    "    pinv_theta = []\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the pseudoinverse algorithm.                            #\n",
    "    ###########################################################################\n",
    "    pass\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return pinv_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ee89ac06af3087ae",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "theta_pinv = pinv(X_train ,y_train)\n",
    "J_pinv = compute_cost(X_train, y_train, theta_pinv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add the loss value for the theta calculated using the psuedo-inverse to our graph. This is another sanity check as the loss of our model should converge to the psuedo-inverse loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-639b53fc41479335",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(iterations), J_history)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss as a function of iterations')\n",
    "plt.hlines(y = J_pinv, xmin = 0, xmax = len(J_history), color='r',\n",
    "           linewidth = 1, linestyle = 'dashed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5043aa5363cbe5c9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "We can use a better approach for the implementation of `gradient_descent`. Instead of performing 40,000 iterations, we wish to stop when the improvement of the loss value is smaller than `1e-8` from one iteration to the next. Implement the function `efficient_gradient_descent`. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def efficient_gradient_descent(X, y, theta, alpha, num_iters):\n",
    "    \"\"\"\n",
    "    Learn the parameters of your model using the *training set*, but stop \n",
    "    the learning process once the improvement of the loss value is smaller \n",
    "    than 1e-8. This function is very similar to the gradient descent \n",
    "    function you already implemented.\n",
    "\n",
    "    Input:\n",
    "    - X: Inputs  (n features over m instances).\n",
    "    - y: True labels (1 value over m instances).\n",
    "    - theta: The parameters (weights) of the model being learned.\n",
    "    - alpha: The learning rate of your model.\n",
    "    - num_iters: The number of updates performed.\n",
    "\n",
    "    Returns two values:\n",
    "    - theta: The learned parameters of your model.\n",
    "    - J_history: the loss value for every iteration.\n",
    "    \"\"\"\n",
    "    \n",
    "    J_history = [] # Use a python list to save cost in every iteration\n",
    "    theta = theta.copy() # avoid changing the original thetas\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the gradient descent optimization algorithm.            #\n",
    "    ###########################################################################\n",
    "    pass\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return theta, J_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6e2524d07523d950",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "The learning rate is another factor that determines the performance of our model in terms of speed and accuracy. Complete the function `find_best_alpha`. Make sure you use the training dataset to learn the parameters (thetas) and use those parameters with the validation dataset to compute the cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_alpha(X_train, y_train, X_val, y_val, iterations):\n",
    "    \"\"\"\n",
    "    Iterate over provided values of alpha and train a model using the \n",
    "    *training* dataset. maintain a python dictionary with alpha as the \n",
    "    key and the loss on the *validation* set as the value.\n",
    "\n",
    "    Input:\n",
    "    - X_train, y_train, X_val, y_val: the training and validation data\n",
    "    - iterations: maximum number of iterations\n",
    "\n",
    "    Returns:\n",
    "    - alpha_dict: A python dictionary - {key (alpha) : value (validation loss)}\n",
    "    \"\"\"\n",
    "    \n",
    "    alphas = [0.00001, 0.00003, 0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 2, 3]\n",
    "    alpha_dict = {}\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the function.                                           #\n",
    "    ###########################################################################\n",
    "    pass\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return alpha_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a8b088fe7a10910a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "alpha_dict = find_best_alpha(X_train, y_train, X_val, y_val, 40000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5bd93130c022d3e1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Obtain the best learning rate from the dictionary `alpha_dict`. This can be done in a single line using built-in functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4f81cf375ac46b73",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "best_alpha = None\n",
    "###########################################################################\n",
    "#                            START OF YOUR CODE                           #\n",
    "###########################################################################\n",
    "pass\n",
    "###########################################################################\n",
    "#                             END OF YOUR CODE                            #\n",
    "###########################################################################\n",
    "print(best_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d16367ecb7183996",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Pick the best three alpha values you just calculated and provide **one** graph with three lines indicating the training loss as a function of iterations (Use 10,000 iterations). Note you are required to provide general code for this purpose (no hard-coding). Make sure the visualization is clear and informative. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-448638e817503ca3",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "#                            START OF YOUR CODE                           #\n",
    "###########################################################################\n",
    "pass\n",
    "###########################################################################\n",
    "#                             END OF YOUR CODE                            #\n",
    "###########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b73893d236bff1d5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "This is yet another sanity check. This function plots the regression lines of your model and the model based on the pseudoinverse calculation. Both models should exhibit the same trend through the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c7ee7d8763464371",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 7))\n",
    "plt.plot(X_train[:,1], y_train, 'ro', ms=1, mec='k')\n",
    "plt.ylabel('Price in USD')\n",
    "plt.xlabel('sq.ft')\n",
    "plt.plot(X_train[:, 1], np.dot(X_train, theta), 'o')\n",
    "plt.plot(X_train[:, 1], np.dot(X_train, theta_pinv), '-')\n",
    "\n",
    "plt.legend(['Training data', 'Linear regression', 'Best theta']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e77c602466fab37d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Part 2: Multivariate Linear Regression (30 points)\n",
    "\n",
    "In most cases, you will deal with databases that have more than one feature. It can be as little as two features and up to thousands of features. In those cases, we use a multiple linear regression model. The regression equation is almost the same as the simple linear regression equation:\n",
    "\n",
    "$$\n",
    "\\hat{y} = h_\\theta(\\vec{x}) = \\theta^T \\vec{x} = \\theta_0 + \\theta_1 x_1 + ... + \\theta_n x_n\n",
    "$$\n",
    "\n",
    "\n",
    "If you wrote vectorized code, this part should be straightforward. If your code is not vectorized, you should go back and edit your functions such that they support both multivariate and single variable regression. **Your code should not check the dimensionality of the input before running**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-15626dda8db26550",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Read comma separated data\n",
    "df = pd.read_csv('data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2dc0f4dc3491520c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Preprocessing\n",
    "\n",
    "Like in the single variable case, we need to create a numpy array from the dataframe. Before doing so, we should notice that some of the features are clearly irrelevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-a87b4027bd3bda4b",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "X = df.drop(columns=['price', 'id', 'date']).values\n",
    "y = df['price'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1aa12f54513b1efa",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Use the **same** `preprocess` function you implemented previously. Notice that proper vectorized implementation should work regardless of the dimensionality of the input. You might want to check that your code in the previous parts still works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f40a9df530db9399",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "X, y = preprocess(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and validation split\n",
    "np.random.seed(42)\n",
    "indices = np.random.permutation(X.shape[0])\n",
    "idx_train, idx_val = indices[:int(0.8*X.shape[0])], indices[int(0.8*X.shape[0]):]\n",
    "X_train, X_val = X[idx_train,:], X[idx_val,:]\n",
    "y_train, y_val = y[idx_train], y[idx_val]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using 3D visualization, we can still observe trends in the data. Visualizing additional dimensions requires advanced techniques we will learn later in the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0c68216a26a9b5af",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import mpl_toolkits.mplot3d.axes3d as p3\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "ax = p3.Axes3D(fig)\n",
    "xx = X_train[:, 1][:1000]\n",
    "yy = X_train[:, 2][:1000]\n",
    "zz = y_train[:1000]\n",
    "ax.scatter(xx, yy, zz, marker='o')\n",
    "ax.set_xlabel('bathrooms')\n",
    "ax.set_ylabel('sqft_living')\n",
    "ax.set_zlabel('price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-70fcd47d69caea00",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Use the bias trick again (add a column of ones as the zeroth column in the both the training and validation datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-2985911f4b7af3e1",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "#                            START OF YOUR CODE                           #\n",
    "###########################################################################\n",
    "pass\n",
    "###########################################################################\n",
    "#                             END OF YOUR CODE                            #\n",
    "###########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2b89288ff61c80ac",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Make sure the functions `compute_cost` (10 points), `gradient_descent` (15 points), and `pinv` (5 points) work on the multi-dimensional dataset. If you make any changes, make sure your code still works on the single variable regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-81ab741781b2f6ec",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "shape = X_train.shape[1]\n",
    "theta = np.ones(shape)\n",
    "J = compute_cost(X_train, y_train, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6f25fb05bd6c648a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "shape = X_train.shape[1]\n",
    "theta = np.random.random(shape)\n",
    "iterations = 40000\n",
    "theta, J_history = gradient_descent(X_train ,y_train, theta, best_alpha, iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-827d1de1293be51f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "theta_pinv = pinv(X_train ,y_train)\n",
    "J_pinv = compute_cost(X_train, y_train, theta_pinv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use visualization to make sure the code works well. Notice we use logarithmic scale for the number of iterations, since gradient descent converges after ~500 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4fa207b72d2445c2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(iterations), J_history)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss as a function of iterations - multivariate linear regression')\n",
    "plt.hlines(y = J_pinv, xmin = 0, xmax = len(J_history), color='r',\n",
    "           linewidth = 1, linestyle = 'dashed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cad652570cee3629",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Part 3: Polynomial Regression (10 points)\n",
    "\n",
    "Linear Regression allows us to explore linear relationships but if we need a model that describes non-linear dependencies we can also use Polynomial Regression. In order to perform polynomial regression, we create additional features using a function of the original features and use standard linear regression on the new features. For example, consider the following single variable $(x)$ cubic regression:\n",
    "\n",
    "$$ x_0 = 1, \\space x_1 = x, \\space x_2 = x^2, \\space x_3 = x^3$$\n",
    "\n",
    "And after using standard linear regression:\n",
    "\n",
    "$$ f(x) = \\theta_0 + \\theta_1 x + \\theta_2 x^2 +  \\theta_3 x^3$$\n",
    "\n",
    "As required. \n",
    "\n",
    "For this exercise, use polynomial regression by using all **quadratic** feature combinations: \n",
    "\n",
    "$$ 1, x, y, z, x^2, y^2, z^2, xy, xz, yz, ...$$\n",
    "\n",
    "and evaluate the MSE cost on the training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['price', 'id', 'date']\n",
    "all_features = df.drop(columns=columns_to_drop)\n",
    "all_features.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give an explanations to the results and compare them to regular linear regression. Do they make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use this Markdown cell for your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Adaptive Learning Rate (10 points)\n",
    "\n",
    "So far, we kept the learning rate alpha constant during training. However, changing alpha during training might improve convergence in terms of the global minimum found and running time. Implement the adaptive learning rate method based on the gradient descent algorithm above. \n",
    "\n",
    "**Your task is to find proper hyper-parameter values for the adaptive technique and compare this technique to the constant learning rate. Use clear visualizations of the validation loss and the learning rate as a function of the iteration**. \n",
    "\n",
    "Time based decay: this method reduces the learning rate every iteration according to the following formula:\n",
    "\n",
    "$$\\alpha = \\frac{\\alpha_0}{1 + D \\cdot t}$$\n",
    "\n",
    "Where $\\alpha_0$ is the original learning rate, $D$ is a decay factor and $t$ is the current iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here ###"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
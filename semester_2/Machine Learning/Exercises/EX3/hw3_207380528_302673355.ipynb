{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KkqdwZcO97qw"
   },
   "source": [
    "# Exercise 3: MAP Classifier\n",
    "\n",
    "In this assignment you will implement Baysian learning\n",
    "\n",
    "## Read the following instructions carefully:\n",
    "\n",
    "1. This jupyter notebook contains all the step by step instructions needed for this part of the exercise.\n",
    "2. Write vectorized code whenever possible.\n",
    "3. You are responsible for the correctness of your code and should add as many tests as you see fit. Tests will not be graded nor checked.\n",
    "4. Write your functions in this notebook only.\n",
    "5. You are allowed to use functions and methods from the [Python Standard Library](https://docs.python.org/3/library/) and [numpy](https://www.numpy.org/devdocs/reference/) only. \n",
    "6. Your code must run without errors. During the environment setup, you were given a specific version of `numpy` to install. Changes of the configuration we provided are at your own risk. Code that cannot run will also earn you the grade of 0.\n",
    "7. Write your own code. Cheating will not be tolerated. \n",
    "8. Submission includes this notebook and the answers to the theoretical part. Answers to qualitative questions should be written in markdown cells (with $\\LaTeX$ support).\n",
    "9. You can add additional functions.\n",
    "10. Submission: zip only the completed jupyter notebook and the PDF with your solution for the theory part. Do not include the data or any directories. Name the file `ID1_ID2.zip` and submit **only one copy of the assignment**.\n",
    "\n",
    "## In this exercise you will perform the following:\n",
    "1. Uderstand Conditional Independence concept \n",
    "1. Implement density estimation using MLE\n",
    "1. Implement a Naive Bayes Classifier based on Uni-Normal distribution\n",
    "1. Implement a Full Bayes Classifier based on Multi-Normal distribution\n",
    "1. Implement a Discrete Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 596,
     "status": "ok",
     "timestamp": 1649263726391,
     "user": {
      "displayName": "Yarden Rachamim",
      "userId": "05474227465087296318"
     },
     "user_tz": -180
    },
    "id": "S7n52AXs97q6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as st \n",
    "import math\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "sns.set(style=\"darkgrid\")\n",
    "sns.set(font_scale=1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wIlV22zUVJ7p"
   },
   "source": [
    "# Conditional independence  \n",
    "Define 3 random variables (RV) $X, Y, C$ s.t.:  \n",
    "1. $X, Y$ and $C$ are all binary\n",
    "2. The following conditions hold:  \n",
    "    * P(X=0) = 0.3\n",
    "    * P(Y=0) = 0.3\n",
    "    * P(C=0) = 0.5\n",
    "3. $X$ and $Y$ are not independent\n",
    "4. $X$ and $Y$ are conditionaly independent given $C$ $(X \\perp\\!\\!\\!\\perp Y |C)$\n",
    "\n",
    "In order to define those RV you need to fill the distributions (represent as python dictionaries) below\n",
    "and then write a function that prove that conditions 3 (`is_X_Y_depndendent`) and 4 (`is_X_Y_given_C_independent`) holds.\n",
    "\n",
    "Rcall that:   \n",
    "1. $P(X|Y) = \\frac{P(X, Y)}{P(Y)}$  \n",
    "2. $P(X, Y|C) = \\frac{P(X, Y, C)}{P(C)}$\n",
    "3. $(X \\perp\\!\\!\\!\\perp Y |C)$   iff  \n",
    "$\\forall x, y,c$: $p(X=x,Y=y|C=c)=p(X=x|C=c)p(Y=y|C=c)$  \n",
    "\n",
    "Make sure that all the probabilities are valid! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 614,
     "status": "ok",
     "timestamp": 1649263838473,
     "user": {
      "displayName": "Yarden Rachamim",
      "userId": "05474227465087296318"
     },
     "user_tz": -180
    },
    "id": "-K57RiKSbKCi"
   },
   "outputs": [],
   "source": [
    "# You need to fill the None value with valid probabilities\n",
    "X = {0: 0.3, 1: 0.7}  # P(X=x)\n",
    "Y = {0: 0.3, 1: 0.7}  # P(Y=y)\n",
    "C = {0: 0.5, 1: 0.5}  # P(C=c)\n",
    "\n",
    "# TODO: eyal - I think that for X, Y to be not independent we simply need to provide one value (for example: (0, 0)) to NOT be the multiplication of p(x=0) times p(y=0). \n",
    "                # And of course probs vector needs to sum to 1.\n",
    "X_Y = {\n",
    "    (0, 0): 0.25,\n",
    "    (0, 1): 0.25,\n",
    "    (1, 0): 0.25,\n",
    "    (1, 1): 0.25\n",
    "}  # P(X=x, Y=y)\n",
    "\n",
    "X_C = {\n",
    "    (0, 0): 0.25,\n",
    "    (0, 1): 0.25,\n",
    "    (1, 0): 0.25,\n",
    "    (1, 1): 0.25\n",
    "}  # P(X=x, C=y)\n",
    "\n",
    "Y_C = {\n",
    "    (0, 0): 0.25,\n",
    "    (0, 1): 0.25,\n",
    "    (1, 0): 0.25,\n",
    "    (1, 1): 0.25\n",
    "}  # P(Y=y, C=c)\n",
    "\n",
    "X_Y_C = {\n",
    "    (0, 0, 0): 0.125, # P(X=x, Y=y, C=c) = P(X=x, Y=y | C=c) * p(C=c) = P(X=x | C=c) * P(Y=y | C=c) * p(C=c) = P(X=0 | C=0) * P(Y=0 | C=0) * p(C=0) = \n",
    "    # 0.5 * 0.5 * 0.5 = 0.125\n",
    "    # explanation for the above calculation: P(A, B) = P(A | B) * P(B) (because of the fact that P(A | B) = P(A, B) / P(B))\n",
    "    # TODO: not sure if we need to write this down or not.\n",
    "    (0, 0, 1): 0.125,\n",
    "    (0, 1, 0): 0.125,\n",
    "    (0, 1, 1): 0.125,\n",
    "    (1, 0, 0): 0.125,\n",
    "    (1, 0, 1): 0.125,\n",
    "    (1, 1, 0): 0.125,\n",
    "    (1, 1, 1): 0.125\n",
    "}  # P(X=x, Y=y, C=c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1649263839191,
     "user": {
      "displayName": "Yarden Rachamim",
      "userId": "05474227465087296318"
     },
     "user_tz": -180
    },
    "id": "QExLsNKrdegA"
   },
   "outputs": [],
   "source": [
    "# You may assume that X, Y and C have the same support (e.g. they are defined on the same space)\n",
    "# Note: since python suffer from numerical instability you may want to use np.isclose instead of the `==` operator\n",
    "def is_X_Y_depndendent(X, Y, X_Y):\n",
    "    \"\"\"\n",
    "    return True iff X and Y are depndendent\n",
    "    \"\"\"\n",
    "    # let's calculate for each values, the probabilty that it'd have if X & Y are indepndendent\n",
    "    for X_Y_values in X_Y:\n",
    "        x_val, y_val = X_Y_values\n",
    "        prob_x, prob_y = X[x_val], Y[y_val]\n",
    "        prob_if_indepndendent = prob_x * prob_y\n",
    "        real_prob = X_Y[X_Y_values]\n",
    "\n",
    "        if not np.isclose(prob_if_indepndendent, real_prob):\n",
    "            # it's enough that just one of the values shows this to be depndendent\n",
    "            print(f\"X and Y are depndendent! \\nThe probability for X={x_val} & Y={y_val} if they were independent is: {prob_if_indepndendent}, but it's: {real_prob} \\n\")\n",
    "            return True\n",
    "        \n",
    "    print(\"It seems that X and Y are independent!\")\n",
    "    return False\n",
    "\n",
    "\n",
    "def is_X_Y_given_C_independent(X, Y, C, X_C, Y_C, X_Y_C):\n",
    "    \"\"\"\n",
    "    return True iff X_given_C and Y_given_C are indepndendent\n",
    "    \"\"\"\n",
    "    # let's calculate for each values, the probabilty that it'd have if X & Y are depndendent given c\n",
    "    for X_Y_C_values in X_Y_C:\n",
    "        x_val, y_val, c_val = X_Y_C_values\n",
    "        prob_x, prob_y, prob_c = X[x_val], Y[y_val], C[c_val]\n",
    "\n",
    "        p_x_given_c = X_C[(x_val, c_val)] / prob_c\n",
    "        p_y_given_c = Y_C[(y_val, c_val)] / prob_c\n",
    "\n",
    "        prob_if_indepndendent = p_x_given_c * p_y_given_c * prob_c\n",
    "        real_prob = X_Y_C[X_Y_C_values]\n",
    "\n",
    "        if not np.isclose(prob_if_indepndendent, real_prob):\n",
    "            # it's enough that just one of the values shows this to be depndendent\n",
    "            print(f\"X and Y are depndendent given C! \\nThe probability for X={x_val} & Y={y_val} given C={c_val} if they were independent is: {prob_if_indepndendent}, but it's: {real_prob} \\n\")\n",
    "            return False\n",
    "    \n",
    "    print(\"It seems that X & Y are independent given C!\")\n",
    "    # we are supposed to get here in our example\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X and Y are depndendent! \n",
      "The probability for X=0 & Y=0 if they were independent is: 0.09, but it's: 0.25 \n",
      "\n",
      "It seems that X & Y are independent given C!\n"
     ]
    }
   ],
   "source": [
    "assert is_X_Y_depndendent(X, Y, X_Y)\n",
    "assert is_X_Y_given_C_independent(X, Y, C, X_C, Y_C, X_Y_C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZJBM6CCAyRB"
   },
   "source": [
    "# Maximum Likelihood estimation  \n",
    "\n",
    "In probability theory and statistics, the Poisson distribution is a discrete probability distribution that expresses the probability of a given number of events occurring in a fixed interval of time or space if these events occur with a known constant mean rate and independently of the time since the last event.  \n",
    "The paramter of this distribution is the rate of events in that given time interval, annotated $\\lambda$  \n",
    "if $X$~$Pois(\\lambda)$  \n",
    "then $p(X=k|\\lambda) = \\frac{\\lambda^ke^{-\\lambda}}{k!}$  \n",
    "Where $X$ is a RV $\\lambda$ is the rate and $p$ is the pmf\n",
    "\n",
    "Implement the function `poisson_log_pmf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ae0FUmlqFPA1"
   },
   "outputs": [],
   "source": [
    "def poisson_log_pmf(k, rate):\n",
    "    \"\"\"\n",
    "    k: A discrete instance\n",
    "    rate: poisson rate parameter (lambda)\n",
    "\n",
    "    return the log pmf value for instance k given the rate\n",
    "    \"\"\"\n",
    "    nominator = rate ** k * np.e ** -rate\n",
    "    denominator = math.factorial(k)\n",
    "    return np.log(nominator / denominator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "weRAiHJxIbuh"
   },
   "source": [
    "In the file poisson_1000_samples.csv there are 1000 points drawn from some poisson distribution with a fixed paramter $\\lambda$  \n",
    "\n",
    "In the following section you are going to find a rate that maximizes the likelihood function. You will do this in 2 diffrent ways:\n",
    "1. Iterative (`possion_iterative_mle`): given a list of possible rates (`rates`), calculate the log likelihood value for each rate and return the rate that has the maximum value\n",
    "2. Analytic (`possion_analytic_mle`): read the following blog: https://www.statology.org/mle-poisson-distribution/. This blog demonstrate how to derive the MLE of a poisson distribution. Understande the process and implement the function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 342
    },
    "executionInfo": {
     "elapsed": 311,
     "status": "error",
     "timestamp": 1649264049923,
     "user": {
      "displayName": "Yarden Rachamim",
      "userId": "05474227465087296318"
     },
     "user_tz": -180
    },
    "id": "14_MylZP-15d",
    "outputId": "014abf33-e03c-4ef7-b787-eb1a00d0815b"
   },
   "outputs": [],
   "source": [
    "poisson_samples = pd.read_csv('data/poisson_1000_samples.csv').values.flatten()\n",
    "rates = np.linspace(1e-20, 20, num=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 295,
     "status": "ok",
     "timestamp": 1649264030874,
     "user": {
      "displayName": "Yarden Rachamim",
      "userId": "05474227465087296318"
     },
     "user_tz": -180
    },
    "id": "Z8C_qIhKA9ZK"
   },
   "outputs": [],
   "source": [
    "def get_poisson_log_likelihoods(samples, rates):\n",
    "    \"\"\"\n",
    "    samples: set of univariate discrete observations\n",
    "    rates: an iterable of rates to calculate log-likelihood by.\n",
    "\n",
    "    return: 1d numpy array, where each value represent that log-likelihood value of rates[i]\n",
    "    \"\"\"\n",
    "    # TODO: this doesn't work well (the break shouldn't be there)\n",
    "    log_likelihoods_per_rate = list()\n",
    "    for rate in rates:\n",
    "        samples_log_likelihoods = [poisson_log_pmf(k=k, rate=rate) for k in samples]\n",
    "\n",
    "        # because it's a log likelihoopd, the multiplication can be presented as summation: \n",
    "        # log( p(D | lambda) ) = log( p(x1 | lambda) * p(x2 | lambda) ... ) = log( p(x1 | lambda) ) + log( p(x2 | lambda) ) them.\n",
    "        log_likelihood = 0.0\n",
    "        for sample_log_likelihood in samples_log_likelihoods:\n",
    "            log_likelihood = log_likelihood + sample_log_likelihood\n",
    "\n",
    "        log_likelihoods_per_rate.append(log_likelihood)\n",
    "\n",
    "    return log_likelihoods_per_rate\n",
    "\n",
    "    \n",
    "def possion_iterative_mle(samples, rates):\n",
    "    \"\"\"\n",
    "    samples: set of univariate discrete observations\n",
    "    rate: a rate to calculate log-likelihood by.\n",
    "\n",
    "    return: the rate that maximizes the likelihood \n",
    "    \"\"\"\n",
    "    rate = 0.\n",
    "    log_likelihoods = get_poisson_log_likelihoods(samples, rates)\n",
    "    # Your code goes here\n",
    "    max_log_likelihood = np.max(log_likelihoods)\n",
    "    rate_index = log_likelihoods.index(max_log_likelihood)\n",
    "    rate = rates[rate_index]\n",
    "\n",
    "    # End of your code\n",
    "    return rate\n",
    "\n",
    "def possion_analytic_mle(samples):\n",
    "    \"\"\"\n",
    "    samples: set of univariate discrete observations\n",
    "\n",
    "    return: the rate that maximizes the likelihood\n",
    "    \"\"\"\n",
    "    # the poisson analytical MLE, as described in the blog, is simply the mean of the samples:\n",
    "    samples_mean = np.mean(samples)\n",
    "    return samples_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "executionInfo": {
     "elapsed": 779,
     "status": "error",
     "timestamp": 1649264034100,
     "user": {
      "displayName": "Yarden Rachamim",
      "userId": "05474227465087296318"
     },
     "user_tz": -180
    },
    "id": "i55OIUH1MIUh",
    "outputId": "b593730f-e29a-4ba0-fd56-0f2a8bc4f29d"
   },
   "outputs": [],
   "source": [
    "x = rates\n",
    "y = get_poisson_log_likelihoods(poisson_samples, rates)\n",
    "\n",
    "iterative_rate = possion_iterative_mle(poisson_samples, rates)\n",
    "analytic_rate = possion_analytic_mle(poisson_samples)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.axvline(x=iterative_rate, linestyle=':', c='r', label=f\"iterative: {iterative_rate:.2f}\")\n",
    "plt.axvline(x=analytic_rate, linestyle='--', c='g', label=f\"analytic: {analytic_rate:.2f}\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kL1Y_nmu97q9"
   },
   "source": [
    "\n",
    "# Normal Naive Bayes Classifier Vs Normal Full Bayes Classifier\n",
    "In the following section we are going to compare 2 models on a given dataset. <br>\n",
    "The 2 classifiers we are going to test are:\n",
    "1. Naive Bayes classifer.<br>\n",
    "1. Full Bayes classifier.<br>\n",
    "Recall that a Naive Bayes classifier makes the following assumption :<br> \n",
    "## $$ p(x_1, x_2, ..., x_n|A_j) = \\Pi p(x_i | A_j) $$\n",
    "But the full Bayes classifier will not make this assumption.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4Trl8uU97q-"
   },
   "source": [
    "### The Data Story"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Z8oRyIG97rA"
   },
   "source": [
    "In a faraway land called **Randomistan** there is a rare animal called the **Randomammal**.<br> \n",
    "We have gathered data about this unique animal to help the **randomian** researchers in observing this beast. <br>\n",
    "For a 1000 days straight we have measured the temperature and the humidity in Randomistan and whether the Randomammal was spotted or not. <br>\n",
    "The well known randomian **Bob** is a bit of a lazy researcher so he likes to keep things simple, and so he assumes that the temperature and the humidity are independent given the class. <br>\n",
    "**Alice** on the other hand is a hard working researcher and does not make any assumptions, she's young and is trying to gain some fame in the randomian community.\n",
    "\n",
    "The dataset contains 2 features (**Temperature**, **Humidity**) alongside a binary label (**Spotted**) for each instance.<br>\n",
    "\n",
    "We are going to test 2 different classifiers :\n",
    "* Naive Bayes Classifier (Bob)\n",
    "* Full Bayes Classifier. (Alice)\n",
    "\n",
    "Both of our researchers assume that our features are normally distributed. But while Bob with his Naive classifier will assume that the features are independent, Alice and her Full Bayes classifier will not make this assumption.<br><br>\n",
    "Let's start off by loading the data (train, test) into a pandas dataframe and then converting them\n",
    "into numpy arrays.<br>\n",
    "The datafiles are :\n",
    "- randomammal_train.csv\n",
    "- randomammal_test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wml9l2VJ97rB"
   },
   "outputs": [],
   "source": [
    "# Load the train and test set into a pandas dataframe and convert them into a numpy array.\n",
    "# The columns order: ['Temp', 'Humidity', 'Spotted']\n",
    "train_set = pd.read_csv('data/randomammal_train.csv').values\n",
    "test_set = pd.read_csv('data/randomammal_test.csv').values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R0Smujya97rD"
   },
   "source": [
    "# Data Visualization\n",
    "Draw a scatter plot of the training data where __x__=Temerature and **y**=Humidity. <br>\n",
    "Use color to distinguish points from different classes.<br>\n",
    "Stop for a minute to think about Alice and Bob's approaches and which one you expect to work better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e3GuXpOj97rF"
   },
   "outputs": [],
   "source": [
    "plt.style.use('seaborn')\n",
    "sns.set(style=\"darkgrid\")\n",
    "sns.set(font_scale=1.2)\n",
    "\n",
    "# Your code goes here.\n",
    "plt.figure(figsize=(10, 6))\n",
    "mask_not_spotted = train_set[:, -1] == 0\n",
    "mask_spotted = train_set[:, -1] == 1\n",
    "\n",
    "plt.scatter(x=train_set[mask_not_spotted][:, 0], y=train_set[mask_not_spotted][:, 1], marker='o', color='blue', alpha=0.3)\n",
    "plt.scatter(x=train_set[mask_spotted][:, 0], y=train_set[mask_spotted][:, 1], marker='o', color='red', alpha=0.3)\n",
    "\n",
    "plt.title('Scatter Plot Humidity & Temperature'.title())\n",
    "plt.xlabel('Temperature')\n",
    "plt.ylabel('Humidity')\n",
    "plt.legend(['Not Spotted', 'Spotted'], loc = 'upper right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mtalzG-N97rG"
   },
   "source": [
    "## Bob's Naive Model\n",
    "\n",
    "Start with implementing the [normal distribution](https://en.wikipedia.org/wiki/Normal_distribution) probability density function in the next cell: \n",
    "$$ \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\cdot e ^{-\\frac{(x - \\mu)^2}{2\\sigma^2}} $$\n",
    "Where :\n",
    "* $\\mu$ is the distribution mean.\n",
    "* $\\sigma$ is the distribution standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g0MLP1cR_0d-"
   },
   "outputs": [],
   "source": [
    "def normal_pdf(x, mean, std):\n",
    "    \"\"\"\n",
    "    Calculate normal desnity function for a given x, mean and standrad deviation.\n",
    " \n",
    "    Input:\n",
    "    - x: A value we want to compute the distribution for.\n",
    "    - mean: The mean value of the distribution.\n",
    "    - std:  The standard deviation of the distribution.\n",
    " \n",
    "    Returns the normal distribution pdf according to the given mean and std for the given x.    \n",
    "    \"\"\"\n",
    "    nomimnator = np.exp( -( (x-mean)**2 ) / ( 2 * std **2 ) )\n",
    "    denomimnator = np.sqrt( 2 * np.pi * std**2 )\n",
    "    return nomimnator / denomimnator\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zLQsBEja97rH"
   },
   "source": [
    "Implement the **NaiveNormalClassDistribution** in the next cell and build a distribution object for each class.\n",
    "Recall that when using the naive assumption, we assume our features are indepenent given the class. Meaning:\n",
    "$$ P(x_1, x_2 | Y) = p(x_1 | Y) \\cdot p(x_2 | Y)$$\n",
    "\n",
    "\n",
    "Since we assume our features are normally distributed we need to find the mean and std for each feature in order for us to compute those probabilites. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BSJRUYiZ97rI"
   },
   "outputs": [],
   "source": [
    "class NaiveNormalClassDistribution():\n",
    "    def __init__(self, dataset, class_value):\n",
    "        \"\"\"\n",
    "        A class which encapsulates the relevant parameters(mean, std) for a class conditinoal normal distribution.\n",
    "        The mean and std are computed from a given data set.\n",
    "        \n",
    "        Input\n",
    "        - dataset: The dataset as a 2d numpy array, assuming the class label is the last column\n",
    "        - class_value : The class to calculate the parameters for.\n",
    "        \"\"\"\n",
    "        self.class_value_mask = np.where(dataset[: , -1] == class_value)\n",
    "        self.means = np.mean(dataset[self.class_value_mask][: , :-1], axis=0)\n",
    "        self.stds = np.std(dataset[self.class_value_mask][: , :-1], axis=0)\n",
    "        self.dataset = dataset\n",
    "        self.class_value = class_value\n",
    "\n",
    "    def get_prior(self):\n",
    "        \"\"\"\n",
    "        Returns the prior porbability of the class according to the dataset distribution.\n",
    "        \"\"\"\n",
    "        return np.sum(self.dataset[: , -1] == self.class_value) / len(self.dataset)\n",
    "    \n",
    "    def get_instance_likelihood(self, x):\n",
    "        \"\"\"\n",
    "        Returns the likelihhod porbability of the instance under the class according to the dataset distribution.\n",
    "        \"\"\"\n",
    "        xs_likelihoods = [normal_pdf(x_i, mean, std) for x_i, mean, std in zip(x, self.means, self.stds)]\n",
    "\n",
    "        # the instance likelihood is the multiplication of the likelihood of all the features (thanks to the NAIVENESS)        \n",
    "        return np.prod(xs_likelihoods)\n",
    "\n",
    "    def get_instance_posterior(self, x):\n",
    "        \"\"\"\n",
    "        Returns the posterior porbability of the instance under the class according to the dataset distribution.\n",
    "        * Ignoring p(x)\n",
    "        \"\"\"\n",
    "        instance_likelihood = self.get_instance_likelihood(x)\n",
    "        prior = self.get_prior()\n",
    "        return instance_likelihood * prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RDlv-Qu897rK"
   },
   "outputs": [],
   "source": [
    "# Build the a NaiveNormalClassDistribution for each class.\n",
    "naive_normal_CD_0 = NaiveNormalClassDistribution(train_set, 0)\n",
    "naive_normal_CD_1 = NaiveNormalClassDistribution(train_set, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E3S8TaXg97rK"
   },
   "source": [
    "Implement the **MAPClassifier** class and build a MAPClassifier object containing the 2 distribution objects you just made above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fh-E75DE97rL"
   },
   "outputs": [],
   "source": [
    "class MAPClassifier():\n",
    "    def __init__(self, ccd0 , ccd1):\n",
    "        \"\"\"\n",
    "        A Maximum a posteriori classifier. \n",
    "        This class will hold 2 class distributions, one for class 0 and one for class 1, and will predict an instance\n",
    "        by the class that outputs the highest posterior probability for the given instance.\n",
    "    \n",
    "        Input\n",
    "            - ccd0 : An object contating the relevant parameters and methods for the distribution of class 0.\n",
    "            - ccd1 : An object contating the relevant parameters and methods for the distribution of class 1.\n",
    "        \"\"\"\n",
    "        self.dist0 = ccd0\n",
    "        self.dist1 = ccd1\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Predicts the instance class using the 2 distribution objects given in the object constructor.\n",
    "    \n",
    "        Input\n",
    "            - An instance to predict.\n",
    "        Output\n",
    "            - 0 if the posterior probability of class 0 is higher and 1 otherwise.\n",
    "        \"\"\"\n",
    "        if self.dist0.get_instance_posterior(x) >= self.dist1.get_instance_posterior(x):\n",
    "            return 0\n",
    "        \n",
    "        # if got here, then class 1 has higher posterior\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yQfcw22U97rL"
   },
   "outputs": [],
   "source": [
    "naive_normal_classifier = MAPClassifier(naive_normal_CD_0, naive_normal_CD_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fK3PKXpf97rM"
   },
   "source": [
    "### Evaluate model\n",
    "Implement the **compute_accuracy** function in the next cell. Use it and the 2 distribution objects you created to compute the accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eA39OpAp97rM"
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(testset, map_classifier):\n",
    "    \"\"\"\n",
    "    Compute the accuracy of a given a testset using a MAP classifier object.\n",
    "    \n",
    "    Input\n",
    "        - testset: The testset for which to compute the accuracy (Numpy array). where the class label is the last column\n",
    "        - map_classifier : A MAPClassifier object capable of prediciting the class for each instance in the testset.\n",
    "        \n",
    "    Ouput\n",
    "        - Accuracy = #Correctly Classified / #testset size\n",
    "    \"\"\"\n",
    "    preds = [map_classifier.predict(row) for row in testset[: , :-1]]\n",
    "    return np.sum(preds == testset[: , -1]) / len(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 272,
     "status": "ok",
     "timestamp": 1649236759557,
     "user": {
      "displayName": "Yarden Rachamim",
      "userId": "05474227465087296318"
     },
     "user_tz": -180
    },
    "id": "X-p0Oo2A97rM",
    "outputId": "6ad81ff3-d37a-406a-83dd-0d061ffed43e"
   },
   "outputs": [],
   "source": [
    "# Compute the naive model accuracy and store it in the naive accuracy variable.\n",
    "naive_accuracy = compute_accuracy(test_set, naive_normal_classifier)\n",
    "naive_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_mGpmGM97rN"
   },
   "source": [
    "## Alice's Full Model\n",
    "\n",
    "Start with Implementing the [multivariate normal](https://en.wikipedia.org/wiki/Multivariate_normal_distribution) distribution probability density function in the next cell.\n",
    "\n",
    "## $$ (2\\pi)^{-\\frac{d}{2}} det(\\Sigma )^{-\\frac{1}{2}} \\cdot e ^{-\\frac{1}{2}(x-\\mu)^T \\Sigma ^ {-1} (x - \\mu) }$$\n",
    "\n",
    "Where : \n",
    "* $\\mu$ is the distribution mean vector. (length 2 in our case)\n",
    "* $\\Sigma$ Is the distribution covarince matrix. (size 2x2 in our case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xs = np.ones(3)\n",
    "mean_vec = [0, 0, 0]\n",
    "cov_i_matrix = np.eye(3)\n",
    "\n",
    "y = st.multivariate_normal.pdf(xs, mean_vec, cov_i_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bZTlpgi7Ojal"
   },
   "outputs": [],
   "source": [
    "def multi_normal_pdf(x, mean, cov):\n",
    "    \"\"\"\n",
    "    Calculate multi variable normal desnity function for a given x, mean and covarince matrix.\n",
    " \n",
    "    Input:\n",
    "    - x: A value we want to compute the distribution for.\n",
    "    - mean: The mean vector of the distribution.\n",
    "    - cov:  The covariance matrix of the distribution.\n",
    " \n",
    "    Returns the normal distribution pdf according to the given mean and var for the given x.    \n",
    "    \"\"\"\n",
    "    return ( 2 * np.pi) ** ( -len(x) / 2)  *  ( np.linalg.det(cov) ** -0.5 )  *  np.exp( -0.5 * (x-mean).T @ np.linalg.inv(cov) @ (x-mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_y = multi_normal_pdf(xs, mean_vec, cov_i_matrix)\n",
    "assert np.isclose(y, pdf_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oIo8Ji8Z97rN"
   },
   "source": [
    "Implement the **MultiNormalClassDistribution** and build a distribution object for each class.\n",
    "\n",
    "In the full bayes model we will not make any simplyfing assumptions, meaning, we will use a multivariate normal distribution. <br>\n",
    "And so, we'll need to compute the mean of each feature and to compute the covariance between the features to build the covariance matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rJDo4AeC97rO"
   },
   "outputs": [],
   "source": [
    "class MultiNormalClassDistribution():\n",
    "    \n",
    "    \n",
    "    def __init__(self, dataset, class_value):\n",
    "        \"\"\"\n",
    "        A class which encapsulate the relevant parameters(mean, cov matrix) for a class conditinoal multi normal distribution.\n",
    "        The mean and cov matrix (You can use np.cov for this!) will be computed from a given data set.\n",
    "        \n",
    "        Input\n",
    "        - dataset: The dataset as a numpy array\n",
    "        - class_value : The class to calculate the parameters for.\n",
    "        \"\"\"\n",
    "        self.class_value_mask = np.where(dataset[: , -1] == class_value)\n",
    "        self.means = np.mean(dataset[self.class_value_mask][: , :-1], axis=0)\n",
    "        self.cov = np.cov(dataset[self.class_value_mask][: , :-1].T)\n",
    "        self.dataset = dataset\n",
    "        self.class_value = class_value\n",
    "\n",
    "    def get_prior(self):\n",
    "        \"\"\"\n",
    "        Returns the prior porbability of the class according to the dataset distribution.\n",
    "        \"\"\"\n",
    "        return np.sum(self.dataset[: , -1] == self.class_value) / len(self.dataset)\n",
    "    \n",
    "    def get_instance_likelihood(self, x):\n",
    "        \"\"\"\n",
    "        Returns the likelihood of the instance under the class according to the dataset distribution.\n",
    "        \"\"\"\n",
    "        return multi_normal_pdf(x, self.means, self.cov)\n",
    "    \n",
    "    def get_instance_posterior(self, x):\n",
    "        \"\"\"\n",
    "        Returns the posterior porbability of the instance under the class according to the dataset distribution.\n",
    "        * Ignoring p(x)\n",
    "        \"\"\"\n",
    "        instance_likelihood = self.get_instance_likelihood(x)\n",
    "        prior = self.get_prior()\n",
    "        return instance_likelihood * prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yRxCmrfI97rO"
   },
   "outputs": [],
   "source": [
    "# Build the a MultiNormalClassDistribution for each class.\n",
    "multi_normal_CD_0 = MultiNormalClassDistribution(train_set, 0)\n",
    "multi_normal_CD_1 = MultiNormalClassDistribution(train_set, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wMYfYFiF97rO"
   },
   "source": [
    "build a MAPClassifier object contating the 2 distribution objects you just made above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0dM1VNHn97rP"
   },
   "outputs": [],
   "source": [
    "multi_normal_classifier = MAPClassifier(multi_normal_CD_0, multi_normal_CD_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_fmTzieM97rP"
   },
   "source": [
    "### Evaluate model\n",
    "Use the **compute_accuracy** function and the 2 distribution objects you created to compute the accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 401,
     "status": "ok",
     "timestamp": 1649238247985,
     "user": {
      "displayName": "Yarden Rachamim",
      "userId": "05474227465087296318"
     },
     "user_tz": -180
    },
    "id": "9Ihutafq97rP",
    "outputId": "9ba3c303-f2e1-44b5-f780-df7896cc5584"
   },
   "outputs": [],
   "source": [
    "# Compute the naive model accuracy and store it in the naive accuracy variable.\n",
    "full_accuracy = compute_accuracy(test_set, multi_normal_classifier)\n",
    "full_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_7u-ec397rQ"
   },
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_XxF0vm797rQ"
   },
   "source": [
    "Use a plot bar to showcase the models accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "executionInfo": {
     "elapsed": 393,
     "status": "ok",
     "timestamp": 1649238258489,
     "user": {
      "displayName": "Yarden Rachamim",
      "userId": "05474227465087296318"
     },
     "user_tz": -180
    },
    "id": "tCi0JFha97rQ",
    "outputId": "d1a6baa6-9149-40e9-af8b-52caff47a5df"
   },
   "outputs": [],
   "source": [
    "# Bar plot of accuracy of each model side by side.\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(x=['Naive', 'Full'], height=[naive_accuracy, full_accuracy])\n",
    "plt.title(\"Naive vs Full accuracy comparison\", fontdict={\"fontsize\": 20})\n",
    "plt.ylabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GUxx4QO697rR"
   },
   "source": [
    "# Comparing Max a posteriori, prior, and likelihood results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JvoTqYI397rR"
   },
   "source": [
    "For each of the classifiers above (naive Bayes and full Bayes, in which we compare posterior probabilities), we explore how classifiers would perform if we compare (1) only prior probabilities or (2) only likelihoods. \n",
    "\n",
    "In this section, you will implement MaxPrior and MaxLikelihood classifiers similarly to MAPClassifier, and then visualize the performance of the three models (MAP, MaxPrior, and MaxLikelihood) for each of the examples of above (naive Bayes and full Bayes).\n",
    "\n",
    "For example, your visualization can be a graph where accuracy is the y-axis, \"MaxPrior\", \"MaxLikelihood\", and \"MAP\" are the x-axis values, and at each x-value, there will be two bars - one for the naive Bayes, and one for the full Bayes.  \n",
    "\n",
    "Other graphs (that make sense / are intuitive) will be accepted as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FpIKZphj97rS"
   },
   "source": [
    "Implement the **MaxPrior** class and build a MaxPrior object like you did above with the **MAPClassifier**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2M5xSt4897rS"
   },
   "outputs": [],
   "source": [
    "class MaxPrior():\n",
    "    def __init__(self, ccd0 , ccd1):\n",
    "        \"\"\"\n",
    "        A Maximum prior classifier. \n",
    "        This class will hold 2 class distributions, one for class 0 and one for class 1, and will predicit an instance\n",
    "        by the class that outputs the highest prior probability for the given instance.\n",
    "    \n",
    "        Input\n",
    "            - ccd0 : An object contating the relevant parameters and methods for the distribution of class 0.\n",
    "            - ccd1 : An object contating the relevant parameters and methods for the distribution of class 1.\n",
    "        \"\"\"\n",
    "        self.dist0 = ccd0\n",
    "        self.dist1 = ccd1\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Predicts the instance class using the 2 distribution objects given in the object constructor.\n",
    "    \n",
    "        Input\n",
    "            - An instance to predict.\n",
    "        Output\n",
    "            - 0 if the posterior probability of class 0 is higher and 1 otherwise.\n",
    "        \"\"\"\n",
    "        if self.dist0.get_prior() >= self.dist1.get_prior():\n",
    "            return 0\n",
    "            \n",
    "        return 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2jOncByj97rS"
   },
   "source": [
    "Implement the **MaxLikelihood** class and build a MaxLikelihood object like you did above with the **MAPClassifier**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uk7culTl97rT"
   },
   "outputs": [],
   "source": [
    "class MaxLikelihood():\n",
    "    def __init__(self, ccd0 , ccd1):\n",
    "        \"\"\"\n",
    "        A Maximum Likelihood classifier. \n",
    "        This class will hold 2 class distributions, one for class 0 and one for class 1, and will predicit an instance\n",
    "        by the class that outputs the highest likelihood probability for the given instance.\n",
    "    \n",
    "        Input\n",
    "            - ccd0 : An object contating the relevant parameters and methods for the distribution of class 0.\n",
    "            - ccd1 : An object contating the relevant parameters and methods for the distribution of class 1.\n",
    "        \"\"\"\n",
    "        self.dist0 = ccd0\n",
    "        self.dist1 = ccd1\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Predicts the instance class using the 2 distribution objects given in the object constructor.\n",
    "    \n",
    "        Input\n",
    "            - An instance to predict.\n",
    "        Output\n",
    "            - 0 if the posterior probability of class 0 is higher and 1 otherwise.\n",
    "        \"\"\"\n",
    "        if self.dist0.get_instance_likelihood(x) >= self.dist1.get_instance_likelihood(x):\n",
    "            return 0\n",
    "            \n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H2r0piw097rT"
   },
   "source": [
    "### Run and evaluate the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KpKqIqTy97rU"
   },
   "source": [
    "Repeat the process you did for the MAPClassifier, now for the MaxPrior and MaxLikelihood classifiers:\n",
    "1. Feed the naive_normal distributions and the multi_normal distributions you made for each class into the new models you made in this section\n",
    "2. Evaluate the accuracies\n",
    "3. Plot the results as described in the beginning of this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o7zQccmN97rV"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "####### YOUR CODE HERE ########\n",
    "# you may add cells as needed #\n",
    "###############################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wMYfYFiF97rO"
   },
   "source": [
    "build the classifiers objects containing the 2 distribution objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0dM1VNHn97rP"
   },
   "outputs": [],
   "source": [
    "# max prior classifiers\n",
    "prior_naive_classifier = MaxPrior(naive_normal_CD_0, naive_normal_CD_1)\n",
    "prior_full_classifier = MaxPrior(multi_normal_CD_0, multi_normal_CD_1)\n",
    "\n",
    "prior_naive_accuracy = compute_accuracy(test_set, prior_naive_classifier)\n",
    "prior_full_accuracy = compute_accuracy(test_set, prior_full_classifier)\n",
    "print(f\"For the MaxPrior model, the naive accuracy is {prior_naive_accuracy:.3f} and the full accuracy is {prior_full_accuracy:.3f}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max likelihood classifiers\n",
    "likelihood_naive_classifier = MaxLikelihood(naive_normal_CD_0, naive_normal_CD_1)\n",
    "likelihood_full_classifier = MaxLikelihood(multi_normal_CD_0, multi_normal_CD_1)\n",
    "\n",
    "likelihood_naive_accuracy = compute_accuracy(test_set, likelihood_naive_classifier)\n",
    "likelihood_full_accuracy = compute_accuracy(test_set, likelihood_full_classifier)\n",
    "print(f\"For the MaxLikelihood model, the naive accuracy is {likelihood_naive_accuracy:.3f} and the full accuracy is {likelihood_full_accuracy:.3f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_XxF0vm797rQ"
   },
   "source": [
    "Use a plot bar to showcase the models accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "executionInfo": {
     "elapsed": 393,
     "status": "ok",
     "timestamp": 1649238258489,
     "user": {
      "displayName": "Yarden Rachamim",
      "userId": "05474227465087296318"
     },
     "user_tz": -180
    },
    "id": "tCi0JFha97rQ",
    "outputId": "d1a6baa6-9149-40e9-af8b-52caff47a5df"
   },
   "outputs": [],
   "source": [
    "acc_naive = [prior_naive_accuracy, likelihood_naive_accuracy, naive_accuracy]\n",
    "acc_full = [prior_full_accuracy, likelihood_full_accuracy, full_accuracy]\n",
    "\n",
    "x = np.arange(3)\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "sns.set(style=\"darkgrid\")\n",
    "sns.set(font_scale=1.5)\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "plt.bar(x, acc_naive, color=\"cornflowerblue\", width=0.25)\n",
    "plt.bar(x + 0.25, acc_full, color=\"mediumaquamarine\", width=0.25)\n",
    "\n",
    "plt.legend(['Naive', 'Full'])\n",
    "plt.xticks([i + 0.125 for i in range(3)], ['MaxPrior', 'MaxLikelihood', 'MAP'])\n",
    "plt.title(\"Models accuracy by model & distribution type\".title())\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2TWhBD4997rV"
   },
   "source": [
    "# Discrete Naive Bayes Classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UKakgRD797rV"
   },
   "source": [
    "We will now build a discrete naive Bayes based classifier using **Laplace** smoothing.\n",
    "In the recitation, we saw how to compute the probability for each attribute value under each class:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fNv3VdNY97rW"
   },
   "source": [
    "$$ P(x_j | A_i) = \\frac{n_{ij} + 1}{n_i + |V_j|} $$\n",
    "Where:\n",
    "* $n_{ij}$ The number of training instances with the class $A_i$ and the value $x_j$ in the relevant attribute.\n",
    "* $n_i$ The number of training instances with the class $A_i$\n",
    "* $|V_j|$ The number of possible values of the relevant attribute.\n",
    "\n",
    "In order to compute the likelihood we assume:\n",
    "$$ P(x| A_i) = \\prod\\limits_{j=1}^{n}P(x_j|A_i) $$\n",
    "\n",
    "And to classify an instance we will choose : \n",
    "$$\\arg\\!\\max\\limits_{i} P(A_i) \\cdot P(x | A_i)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "95Y9WEKO97rW"
   },
   "source": [
    "## Data\n",
    "We will try to predict breast cancer again only this time from a different dataset, \n",
    "<br> you can read about the dataset here : [Breast Cancer Dataset](https://archive.ics.uci.edu/ml/datasets/breast+cancer)<br>\n",
    "Load the training set and test set provided for you in the data folder.\n",
    " - breast_trainset.csv\n",
    " - breast_testset.csv\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ldj_5a3d97rX"
   },
   "outputs": [],
   "source": [
    "# Load the train and test set into a pandas dataframe and convert them into a numpy array.\n",
    "train_set = pd.read_csv('data/breast_trainset.csv').values\n",
    "test_set = pd.read_csv('data/breast_testset.csv').values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TXzx4U0097rX"
   },
   "source": [
    "## Build A Discrete Naive Bayes Distribution for each class\n",
    "Implement the **DiscreteNBClassDistribution** in the next cell and build a distribution object for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(train_set[: , -1] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hS-DkveU97rX"
   },
   "outputs": [],
   "source": [
    "EPSILLON = 1e-6 # if a certain value only occurs in the test set, the probability for that value will be EPSILLON.\n",
    "\n",
    "class DiscreteNBClassDistribution():\n",
    "    def __init__(self, dataset, class_value):\n",
    "        \"\"\"\n",
    "        A class which computes and encapsulate the relevant probabilites for a discrete naive bayes \n",
    "        distribution for a specific class. The probabilites are computed with laplace smoothing.\n",
    "        \n",
    "        Input\n",
    "        - dataset: The dataset as a numpy array.\n",
    "        - class_value: Compute the relevant parameters only for instances from the given class.\n",
    "        \"\"\"\n",
    "        self.class_value_mask = np.where(dataset[: , -1] == class_value)\n",
    "        self.n_i = len(dataset[self.class_value_mask])\n",
    "        \n",
    "        self.dataset = dataset\n",
    "        self.class_value = class_value\n",
    "\n",
    "    def get_prior(self):\n",
    "        \"\"\"\n",
    "        Returns the prior porbability of the class according to the dataset distribution.\n",
    "        \"\"\"\n",
    "        return np.sum(self.dataset[: , -1] == self.class_value) / len(self.dataset)\n",
    "    \n",
    "    def get_instance_likelihood(self, x):\n",
    "        \"\"\"\n",
    "        Returns the likelihood of the instance under the class according to the dataset distribution.\n",
    "        \"\"\"\n",
    "        xs_likelihoods = list()\n",
    "        for j, x_j in enumerate(x):\n",
    "            one_class_dataset = self.dataset[self.class_value_mask]\n",
    "            n_ij = np.sum(one_class_dataset[: , j] == x_j)  # filter for x_j value feature as well, then count it for n_ij\n",
    "            v_j = len(np.unique(self.dataset[: , j]))\n",
    "            likelihood_to_append = (n_ij + 1) / (self.n_i + v_j)\n",
    "\n",
    "            if n_ij == 0:\n",
    "                likelihood_to_append = EPSILLON\n",
    "                \n",
    "            xs_likelihoods.append( likelihood_to_append )\n",
    "\n",
    "        # the instance likelihood is the multiplication of the likelihood of all the features (thanks to the NAIVENESS)\n",
    "        return np.prod(xs_likelihoods)\n",
    "    \n",
    "    def get_instance_posterior(self, x):\n",
    "        \"\"\"\n",
    "        Returns the posterior porbability of the instance under the class according to the dataset distribution.\n",
    "        * Ignoring p(x)\n",
    "        \"\"\"\n",
    "        instance_likelihood = self.get_instance_likelihood(x)\n",
    "        prior = self.get_prior()\n",
    "        return instance_likelihood * prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uiJVXw5h97rY"
   },
   "outputs": [],
   "source": [
    "discrete_naive_CD_0 = DiscreteNBClassDistribution(train_set, 0)\n",
    "discrete_naive_CD_1 = DiscreteNBClassDistribution(train_set, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c7Zfpdyt97rY"
   },
   "source": [
    "build a MAPClassifier object contating the 2 distribution objects you just made above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R46vXMqS97rZ"
   },
   "outputs": [],
   "source": [
    "discrete_naive_classifier = MAPClassifier(discrete_naive_CD_0, discrete_naive_CD_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bKtJw1Ty97rZ"
   },
   "source": [
    "Use the **compute_accuracy** function and the 2 distribution objects you created to compute the accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_accuracy(test_set, discrete_naive_classifier)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ProbabilisticModels.ipynb",
   "provenance": [
    {
     "file_id": "1bCEDw-NC2JWZstuBhGlo7VcB188Ft2K5",
     "timestamp": 1649263512426
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

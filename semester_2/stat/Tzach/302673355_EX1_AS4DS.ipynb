{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e061f88",
   "metadata": {},
   "source": [
    "\n",
    "**Name**: Tzach Larboni\n",
    "\n",
    "**Student ID**: 302673355\n",
    "\n",
    "**Date**: 19.03.2022\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca50075",
   "metadata": {},
   "source": [
    "##### Advanced Statistics for Data Science (Spring 2022)\n",
    "# Home Assignment 1\n",
    "#### Topics:\n",
    "- The Linear Model\n",
    "- Linear Least Squares\n",
    "- Exploratory data analysis\n",
    "- Normal, chisquared, t, and F distributions\n",
    "\n",
    "#### Due: 03/22/2022 by 18:30\n",
    "\n",
    "$\\newcommand{\\Id}{{\\mathbf{I}}}  \n",
    "\\newcommand{\\SSE}{\\mathsf{SSE}}\n",
    "\\newcommand{\\SSR}{\\mathsf{SSR}}\n",
    "\\newcommand{\\MSE}{\\mathsf{MSE}}\n",
    "\\newcommand{\\simiid}{\\overset{iid}{\\sim}}\n",
    "\\newcommand{\\ex}{\\mathbb E}\n",
    "\\newcommand{\\var}{\\mathrm{Var}}\n",
    "\\newcommand{\\Cov}[2]{{\\mathrm{Cov}  \\left(#1, #2 \\right)}}\n",
    "\\newcommand{\\one}[1]{\\mathbf 1 {\\left\\{#1\\right\\}}}\n",
    "\\newcommand{\\SE}[1]{\\mathrm{SE} \\left[#1\\right]}\n",
    "\\newcommand{\\reals}{\\mathbb R}\n",
    "\\newcommand{\\Ncal}{\\mathcal N}\n",
    "\\newcommand{\\abs}[1]{\\ensuremath{\\left\\vert#1\\right\\vert}}\n",
    "\\newcommand{\\rank}{\\operatorname{rank}}\n",
    "\\newcommand{\\tr}{\\operatorname{Tr}}\n",
    "\\newcommand{\\diag}{\\operatorname{diag}}\n",
    "\\newcommand{\\sign}{\\operatorname{sign}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfb0bc1",
   "metadata": {},
   "source": [
    "#### Instructions:\n",
    "- Write your name, Student ID, and date in the cell below. \n",
    "- Submit a copy of this notebook with code filled in the relevant places as the solution of coding excercises.\n",
    "- For theoretic excercises, you can either write your solution in the notebook using $\\LaTeX$ or submit additional notes.\n",
    "\n",
    "<hr>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d828516",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9249beb6",
   "metadata": {},
   "source": [
    "## Problem 1 (Making a Model)\n",
    "A project management team want to model the number of two types of users, called type $A$ and type $B$, over time. The model they want to use has the overall number of users growing linearly over time. At time $t=0$ (not necessarily the website launching date) the linear model should have expected number of users equal in the two groups. At time $t=100$, they introduce a new feature. They think this could change the slope at time $t=100$ but only for users of type $A$ (it changes the *slope*, hence would not put a jump discontinuity into the expected number of users).\n",
    "\n",
    "1. Write a linear model that the team can use. Be sure to say what features $\\{Z_{ij}\\}$ go into the model.\n",
    "2. Maybe the intervention did not do anything. Which parameter in your model would then be zero? (make sure you have such a parameter in your model). \n",
    "\n",
    "*The point*: as data scientistis/statistician, you have to turn the domain person's intuition into a model. They may not come to you talking about $x$s and $y$s and $\\beta$ and $\\epsilon$. Also, the model might be something they want to disprove. \n",
    "\n",
    "<hr> \n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d376bbfc",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1. Write a linear model that the team can use. Be sure to say what features $\\{Z_{ij}\\}$ go into the model.\n",
    "\n",
    "#### Answer \n",
    "The model at hand is a two-group two-phase linear regression model, in which after $t=100$ the slope of the model changes.\n",
    "The relevant features:\n",
    "1. $x_1$ = isTypeA. a dummy variable that tracks whether the user is from Type A or not - \n",
    "$$\n",
    "x_1 = \\left\\{\n",
    "    \\begin{array}\\\\\n",
    "        1 & \\mbox{if } \\ x \\in {TypeA}* \\\\\n",
    "        0 & \\mbox{else.}\n",
    "    \\end{array}\n",
    "\\right.\n",
    "$$\n",
    "2. $x_2$ = Time $t$\n",
    "3. $x_3$ = An interaction varialbe between $[t - 100]_+$ and $TypeA$, such that $Z_+ := max(0,100-t)$. Meaning, \n",
    "\n",
    "$$\n",
    "x_3 = \\left\\{\n",
    "    \\begin{array}\\\\\n",
    "        0 & \\mbox{if } \\ t \\lt 100 \\\\\n",
    "        1 & \\mbox{otherwise}\n",
    "    \\end{array}\n",
    "\\right.\n",
    "$$\n",
    "4. $y$ = The total number of users.\n",
    "\n",
    "To put it in a linear regression model form:\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 = \\beta_0 + \\beta_1 \\dot isTypeA \\beta_2 \\dot t + \\beta_3 \\dot isTypeA \\dot [t-100]_+\n",
    "$$\n",
    "\n",
    "\n",
    "At $t=0$, I would set $beta_1 x_1 = 0.5$ - meaning, half of the users are $TypeA$ users. <br> If the intervention had really changed the slope of $TypeA$ group, it would be reflected in $\\beta_3$ coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef17e832",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2. Maybe the intervention did not do anything. Which parameter in your model would then be zero? (make sure you have such a parameter in your model).\n",
    "\n",
    "#### Answer \n",
    "\n",
    "If the intervention did not do anything, the coefficient $\\beta_3$, that its corresponding feature only starts to work at $t=100$, would be zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb7b8f4",
   "metadata": {},
   "source": [
    "## Problem 2 (Linerization trick in Sinusiodal Regression)\n",
    "1. Consider the model:\n",
    "$$\n",
    "\\mathbb E[Y|X=x] = \\beta_0 + \\beta_1 \\sin\\left(2\\pi  (x - \\beta_2) \\right)\n",
    "$$\n",
    "with predictor $x$, response varaible $y$, and parameters $\\beta_0$, $\\beta_1$, and $\\beta_2$. Is this model linear?\n",
    "2. Consider instead the model:\n",
    "$$\n",
    "\\mathbb E[Y|X=x] = \\beta_0' + \\beta_1' \\sin\\left(2\\pi \\cdot x\\right) + \\beta_2' \\cos(2 \\pi \\cdot x),\n",
    "$$\n",
    "with predictor $x$, response varaible $y$, and parameters $\\beta_0'$, $\\beta_1'$, and $\\beta_2'$. Is this model linear?\n",
    "3. Show that the model from 1 can be written as the model in 2 in the sense that $(\\beta_0', \\beta_1', \\beta_2')$ are a function of $(\\beta_0, \\beta_1', \\beta_2')$. Find this function.\n",
    "\n",
    "<hr> \n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61031f64",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1. No, the model is not linear. Not all of the coefficients $\\beta_i$ , $\\Sigma_{i=0}^{3}$ are parameters of features.\n",
    "2. Yes. All of the coefficients, $\\beta_0$, $\\beta_1$, $\\beta_2$ are in the power of 1 and are parameters of features, hence the model is linear.\n",
    "3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54768c63",
   "metadata": {},
   "source": [
    "## Problem 3 (EDA)\n",
    "For this problem, you will need to go through the exploratory data analysis (EDA) notebook in the course page\n",
    "\n",
    "1. Download the Red Wine Quality Dataset from UCI repository (the file winequality-red.csv at https://archive.ics.uci.edu/ml/datasets/Wine+Quality). \n",
    "2. Filter columns to cosnider only the five variables: $\\texttt{density}$, $\\texttt{alcohol}$, $\\texttt{pH}$, $\\texttt{volatile}$, $\\texttt{acidity}$, and the target variable $\\texttt{quality}$.\n",
    "3. Evaluate mean, varaince, median, q1, q3, IQR, and absolute range of each variable.\n",
    "4. Indicate whether there are outlayers, mark and remove those; explain your logic. \n",
    "5. Illustrate the covaraince matrix of all variable. Indicate variables most correlated with the target variable. \n",
    "6. Illustrate a \"pairs plot\", i.e. a matrix of scatterplots in which each cell involves two variables except the diagonal.\n",
    "\n",
    "<hr>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fa4121-d679-4d2e-8d79-9ba0cda6c3af",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8701e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee95188a-9470-4bfc-bc26-a46b6d05275f",
   "metadata": {},
   "source": [
    "1. Download the Red Wine Quality Dataset from UCI repository (the file winequality-red.csv at https://archive.ics.uci.edu/ml/datasets/Wine+Quality). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89326801",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"winequality-red.csv\", delimiter=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafd08f2-430e-49a8-a5da-cb6e75e174ef",
   "metadata": {},
   "source": [
    "2. Filter columns to cosnider only the five variables: $\\texttt{density}$, $\\texttt{alcohol}$, $\\texttt{pH}$, $\\texttt{volatile}$, $\\texttt{acidity}$, and the target variable $\\texttt{quality}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6c983d-09f4-470b-a885-54a8f164dfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"density\", \"alcohol\", \"pH\", \"volatile acidity\", \"quality\"]\n",
    "df = df.filter(cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeae7d58-22b4-48b1-a7fe-278ffad74541",
   "metadata": {},
   "source": [
    "3. Evaluate mean, varaince, median, q1, q3, IQR, and absolute range of each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57500019-1f16-4ab7-8224-a7ec6421f60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = {}\n",
    "stats_cols = ['Mean', 'Variance', 'Median', 'Q1', 'Q3', 'IQR', 'AbsoluteRange']\n",
    "for col in cols[:len(cols)-1]:\n",
    "    mean = df.describe()[col]['mean']\n",
    "    var = df.describe()[col]['std']**2\n",
    "    median = df.describe()[col]['50%']\n",
    "    first_q = df.describe()[col]['25%']\n",
    "    third_q = df.describe()[col]['75%']\n",
    "    iqr = third_q - first_q\n",
    "    abs_range = df.describe()[col]['max'] - df.describe()[col]['min']\n",
    "    stats.update({col: [mean, var, median, first_q, third_q, iqr, abs_range]})\n",
    "stats_df = pd.DataFrame.from_dict(stats, orient='index', columns=stats_cols)\n",
    "print(stats_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27285e2b-729e-4f54-b1ba-f698b6467eb4",
   "metadata": {},
   "source": [
    "4. Indicate whether there are outlayers, mark and remove those; explain your logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d8e1a2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def clean_outliers(df ,column_to_clean, print_df=True, z_score_threshold=3):\n",
    "    \"\"\"\n",
    "    Uses z-score test to clean a DF from outliers. Takes in a numerical column.\n",
    "    The default z-score threshold is 3, to capture 99.8% of a normally distributed data, but can be altered.\n",
    "    The add_z_score_col flag adds a fitting z-score column to the DF as a default, but can be altered.\n",
    "    Calculation of the z-score is done manually.\n",
    "    \"\"\"\n",
    "    s = df[column_to_clean]\n",
    "    z_score = (s - np.mean(s)) / np.std(s)\n",
    "    mask = abs(z_score) > z_score_threshold\n",
    "    print(f\"For column {column_to_clean}, there are {len(df[mask])} outliers.\")\n",
    "    if print_df:\n",
    "        print(df[column_to_clean][mask])\n",
    "    return df[~mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedd4433-c8b2-44e4-a64b-acd4273bb268",
   "metadata": {},
   "outputs": [],
   "source": [
    "unfiltered_len_df = len(df)\n",
    "for col in cols:\n",
    "    df = clean_outliers(df, col)\n",
    "print(f\"In total, {unfiltered_len_df - len(df)} rows were cleaned.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728d70be-0432-454a-ad21-7297df91a7c0",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "5. Illustrate the covaraince matrix of all variable. Indicate variables most correlated with the target variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bab4cd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cov_matrix = df.cov()\n",
    "print(cov_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44e1f4f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_matrix(matrix, title):\n",
    "    mask = np.triu(np.ones_like(matrix, dtype=np.bool))\n",
    "    f, ax = plt.subplots(figsize=(16, 12))\n",
    "    sns.heatmap(matrix, mask=mask, vmax=.99, vmin=-0.99, center=0,\n",
    "                square=True, linewidths=.5, cbar_kws={\"shrink\": .9}, annot=True)\n",
    "    plt.title(f\"{title}\".title(), fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c74903-64bc-41d7-ac7d-e840eb5c63fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_matrix(cov_matrix, \"covariance matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d2a386",
   "metadata": {},
   "source": [
    "As visible from the covariance matrix, alcohol has the highest covariance with the target. <br> To find the highest correlation, we'll create the correlation matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cdd785-173b-429a-98b1-fb3a40d30b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aec8df6-e0b1-48a6-8a28-ee5945def725",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_matrix(corr, \"corrleation matix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3218aa78-67cb-42b4-ab71-a5098d9a712b",
   "metadata": {},
   "source": [
    "$Alcohol$ has the highest Pearson corrleation coefficient with the target "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e84b05-55a7-4673-8e2b-d8d49a2b7b28",
   "metadata": {},
   "source": [
    "6. Illustrate a \"pairs plot\", i.e. a matrix of scatterplots in which each cell involves two variables except the diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a193d60",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] =  [15, 15]\n",
    "\n",
    "pd.plotting.scatter_matrix(df, hist_kwds={'bins':30})\n",
    "plt.show()\n",
    "plt.rcParams[\"figure.figsize\"] = (10,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b659c9",
   "metadata": {},
   "source": [
    "## Problem 4 (Least squares)\n",
    "\n",
    "1. Consider the Red Wine Quality Dataset of the previous question. Fit a least squares model to ``quality`` based on the other variables and a constant term. Report on the vector of regression coefficients $\\hat{\\beta}$. Make sure to indicate which entry is associated with each predictor. \n",
    "2. Plot the response values $y$ and the fitted response $\\hat{y} = Z^\\top \\hat{\\beta}$\n",
    "3. Illustrate the histogram of the residual vector $\\hat{\\epsilon}$; what is the average of this vector?\n",
    "4. Write a code that verifies: \n",
    " - $H = H^\\top$ \n",
    " - $H^2 = H$\n",
    " - $(I-H)^2 = I-H$\n",
    " - $\\hat{\\epsilon}^\\top \\hat{y} = 0$ \n",
    " - $\\lambda_i(H) \\in \\{0,1\\}$, where $\\lambda_i$ is the $i$-th eigenvalue of $H$\n",
    " - $\\|y\\|^2 = \\|\\hat{\\epsilon}\\|^2 + \\|\\hat{y}\\|^2$\n",
    " - $SS_{Tot} = SS_{Fit} + SS_{Res}$, for the sum of squares decomposition  \n",
    "<hr>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477593e1-9993-45b0-8e62-1fbd4e4c2d8c",
   "metadata": {},
   "source": [
    "1. Consider the Red Wine Quality Dataset of the previous question. Fit a least squares model to ``quality`` based on the other variables and a constant term. Report on the vector of regression coefficients $\\hat{\\beta}$. Make sure to indicate which entry is associated with each predictor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a129326b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = df['density'].values\n",
    "x2 = df['alcohol'].values\n",
    "x3 = df['pH'].values\n",
    "x4 = df['volatile acidity'].values\n",
    "y = df['quality'].values\n",
    "\n",
    "Z = np.vstack([np.ones_like(x1), x1, x2, x3, x4]).T\n",
    "A = np.dot(np.linalg.inv(np.dot(Z.T,Z)), Z.T)\n",
    "beta_hat = np.dot(A, y)\n",
    "print(f\"The vector beta hat is {beta_hat}. \\nThe intersect is {beta_hat[0]:.3f}. \\nThe coefficient for Density is {beta_hat[1]:.3f}. \\nThe coefficient for Alcohol is {beta_hat[2]:.3f}. \\nThe coefficient for pH is {beta_hat[3]:.3f}. \\nThe coefficient for Volatile Acidity is {beta_hat[4]:.3f}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1096962",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y_hat = np.matmul(Z, beta_hat)\n",
    "plt.plot(y)\n",
    "plt.plot(y_hat)\n",
    "plt.title(r\"y and $\\hat{y}$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7a4df9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "epsilon_hat = y - y_hat\n",
    "sns.histplot(epsilon_hat)\n",
    "plt.title(\"Epsilon Histogram\")\n",
    "plt.show()\n",
    "print(f\"The average of the vector epsilon is {np.mean(epsilon_hat):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f9ace0-1fd6-4b9f-9d9b-e6ad6e3f732d",
   "metadata": {},
   "source": [
    "4. Write a code that verifies: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44ce2ef-f928-4072-baff-3ea31686e5f0",
   "metadata": {},
   "source": [
    " - $H = H^\\top$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e466ee-5e1c-4b85-b65e-2e60395a8f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "H = np.dot(np.dot(Z, np.linalg.inv(np.dot(Z.T, Z))), Z.T)\n",
    "np.allclose(H, H.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e64da30-0509-44bb-99cb-2bf23b4aa9a8",
   "metadata": {},
   "source": [
    " - $H^2 = H$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc87a8d3-e27c-42ff-aa27-d169ab69a9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "H_squared = np.dot(H, H)\n",
    "np.allclose(H, H_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83425e2a-8a5e-44f5-9549-c8bb2a862c12",
   "metadata": {},
   "source": [
    " - $(I-H)^2 = I-H$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d45e91-4030-4b84-bbb5-e48d79050230",
   "metadata": {},
   "outputs": [],
   "source": [
    "I = np.identity(H.shape[1])\n",
    "np.allclose(np.matmul(I-H, I-H), I-H)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482af558-45d1-4a2f-a5d2-32ca3788ffa3",
   "metadata": {},
   "source": [
    " - $\\hat{\\epsilon}^\\top \\hat{y} = 0$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb2a33e-7076-4a66-ba00-7fff6064adc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(np.round(np.dot(epsilon_hat.T, y_hat),5), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7326dc58-f9d4-444f-b13f-b271da845661",
   "metadata": {},
   "source": [
    " - $\\lambda_i(H) \\in \\{0,1\\}$, where $\\lambda_i$ is the $i$-th eigenvalue of $H$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e6ddf1-209f-4a1b-b483-64f1f8165585",
   "metadata": {},
   "outputs": [],
   "source": [
    "all(np.round(eigval.real,5)>=0 and np.round(eigval.real,5)<=1 for eigval in np.linalg.eigvals(H))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95fb729-f077-4de7-b8dc-a60ae4d7f95b",
   "metadata": {},
   "source": [
    " - $\\|y\\|^2 = \\|\\hat{\\epsilon}\\|^2 + \\|\\hat{y}\\|^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05d4572-026b-48cd-8faa-d0c428b21be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_norm_squared = np.dot(y,y)\n",
    "y_hat_norm_squared = np.dot(y_hat, y_hat)\n",
    "\n",
    "np.allclose(y_norm_squared, epsilon_hat_norm_squared + y_hat_norm_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70add7f1-52a0-4efc-94e7-e9f98bba7c36",
   "metadata": {},
   "source": [
    " - $SS_{Tot} = SS_{Fit} + SS_{Res}$, for the sum of squares decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05776634-beb5-48ad-9fd0-c2609dd1d954",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_mean = np.mean(y)\n",
    "SS_tot = np.sum((y - y_mean)**2)\n",
    "SS_fit = np.sum((y_hat - y_mean)**2)\n",
    "SS_res = np.sum((y - y_hat)**2)\n",
    "np.allclose(SS_tot, SS_fit + SS_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79eedfd",
   "metadata": {},
   "source": [
    "$\\newcommand{\\var}{\\mathrm{Var}}$\n",
    "## Problem 5 (Weighted least squares)\n",
    "In some cases we may want to weigh differently the controbution of each coordinate to the loss function. For example, this may be because not all meaasuremetns are equally reliable (e.g., new measurements may be more reliable than old ones). One way to account for this difference in reliability is by considering a *weighted* least squares problem, in which we seek $(\\beta_1,\\ldots,\\beta_p) \\in \\reals^p$ to minimize \n",
    "$$\n",
    "S(\\beta_1,\\ldots,\\beta_p) := \n",
    "\\sum_{i=1}^n w_i^2\\left(y_i - \\sum_{j=1}^p z_{ij} \\beta_j \\right)^2 = (\\epsilon W)^\\top (\\epsilon W),\n",
    "$$\n",
    "where $W = \\diag(w_1,\\ldots,w_n)$, $w_i>0$ for all $i=1,\\ldots,n$ are the weights associated with the reliability of each coordinate. \n",
    "\n",
    "1. Write the corresponding normal equations in a matrix form\n",
    "2. Find the optimal vector of weights $\\hat{\\beta}$ minimizing $S(\\beta_1,\\ldots,\\beta_p)$ in terms of $W$, $Z$, and $y$. \n",
    "3. What is the hat matrix? make sure that your expression reduces to the hat matrix from class when $W$ is the identity matrix. \n",
    "\n",
    "<hr>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58482f8d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1. Write the corresponding normal equations in a matrix form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c793242",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Answer\n",
    "$$\n",
    "\\hat{\\epsilon}^T Z W^T = 0 \\Leftrightarrow  Z^T W \\hat{\\epsilon}^T = 0 \\Leftrightarrow Z^T W Z\\hat{\\beta} = Z^T W y\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecfe6f3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2. Find the optimal vector of weights $\\hat{\\beta}$ minimizing $S(\\beta_1,\\ldots,\\beta_p)$ in terms of $W$, $Z$, and $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99a7ad4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Answer\n",
    "$$\n",
    "\\hat{S}(\\beta_1, ..., \\beta_p) := \\hat{\\beta} := (Z^T W Z)^{-1}Z^T W y\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6775a706",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "3. What is the hat matrix? make sure that your expression reduces to the hat matrix from class when $W$ is the identity matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40048cff",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Answer\n",
    "\n",
    "The general hat matrix is $H=Z(Z^T Z)^{-1} Z^T$, and given a weight matrix $W$, the hat matrix will be:\n",
    "$$\n",
    "H = Z(Z^T W Z)^{-1} Z\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b2296d-815b-431c-bc39-213ab53649fb",
   "metadata": {},
   "source": [
    "$\\newcommand{\\var}{\\mathrm{Var}}$\n",
    "## Problem 6 (Sampling from $\\mathcal N(0,1)$, $\\chi^2$, $t$, and $F$)\n",
    "In the following excerise, you can only sample using repeated calls to ``random.random()``. \n",
    "You can use the ``scipy.stats`` package *only* to illustrate PDFs. You should illustrate histograms with the number of bins between 5%-10% of the number of samples $n=1,000$ in the input to the histogram. For example, use $100$ bins equally spaced between the range of the samples, so that you'll get a nice comparison between the empirical distribution and the theoretic distribution represented by the PDF. Make sure that the histogram is normalized to approximate the density of the simulated random variable. Set the seed ``random.seed(my_id)`` where ``my_id`` is your ID number with leading zeros removed. \n",
    "\n",
    "1. Implement the functions ``sample_unif``, ``sample_normal_clt``, and ``sample_normal``. Illustrate the histogram of $1000$ samples from ``sample_normal_clt`` and $1000$ samples from ``sample_normal``. Choose the input to ``sample_normal_clt`` so that the two histogram look alike. \n",
    "2. Implement the functions ``sample_chisq``, ``sample_t``, and ``sample_f``. You should use ``sample_normal`` repeatedly. \n",
    "3. Use ``sample_normal`` to sample $1000$ times from $\\mathcal N(0,1)$ and use one figure to illustrate the histogram of the samples and the true PDF of $\\mathcal N(0,1)$\n",
    "4. Use ``sample_chisq`` to sample $1000$ times from $\\chi^2_k$ and use one figure to illustrate the histogram of the samples and the true PDF of $\\chi^2_k$; for $k=5$ and $k=10$.\n",
    "5. Use ``sample_t`` to sample $1000$ times from $t_k$ (the $t$-distribution with $k$ degrees of freedom) and use one figure to illustrate the histogram of the samples and the true PDF of $t_k$; for $k=5$ and $k=10$.\n",
    "6. Use ``sample_F`` to sample $1000$ times from $F_{k_1,k_2}$ and use one figure to illustrate the histogram of the samples and the true PDF of $F_{k_1,k_2}$; for $(k_1, k_2)= (10,5)$ and $(k_1, k_2)= (50,45)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e6ccbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import scipy.stats as st\n",
    "\n",
    "def sample_unif(n: int) -> float:\n",
    "    \"\"\"\n",
    "    return n independent samples from the uniform distribution over (0,1)\n",
    "    \"\"\"\n",
    "\n",
    "    return np.array([random.random() for _ in range(n)])\n",
    "\n",
    "\n",
    "def sample_normal_clt(n: int) -> float:\n",
    "    \"\"\"\n",
    "    Approximately sample from the standard normal distribution using the CLT \n",
    "    \n",
    "    Args:\n",
    "    -----\n",
    "    :n:    number of samples from a non-normal distribution\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    \n",
    "    :z:    random sample approximately from the standard normal distribution\n",
    "    \n",
    "    \"\"\"\n",
    "    samples = [np.sum(sample_unif(n)) for _ in range(n)] # Take n samples from the uniform distribtuion, as was previously implemented\n",
    "    return (samples - np.mean(samples)) / np.std(samples)\n",
    "\n",
    "#     # sample n non-normal (in this case, uniform) samples, sum them and then normalize them\n",
    "#     sums = np.array([np.sum(sample_unif(n)) for _ in range(n)])\n",
    "#     z_samples = (sums - sums.mean()) / sums.std()\n",
    "#     \"\"\"\n",
    "#     I could do the following and sample only 1 but it took a lot of time, so I just return n smaples.\n",
    "#     z = standard_normal[int(random.random() * len(standard_normal))]\n",
    "#     \"\"\"\n",
    "#     return z_samples\n",
    "\n",
    "\n",
    "def sample_normal() -> float:\n",
    "    \"\"\"\n",
    "    Sample from the standard normal distribution using a single sample\n",
    "    from the uniform distribution. You should use the normal quantile function\n",
    "    ``norm.ppf``\n",
    "    \n",
    "    Args:\n",
    "    -----\n",
    "    :n:    number of samples from a non-normal distribution\n",
    "    \n",
    "    Return:\n",
    "    -------\n",
    "    :z:    random sample from the standard normal distribution\n",
    "    \n",
    "    \"\"\"\n",
    "    n = 1\n",
    "    return st.norm.ppf(q=sample_unif(n))\n",
    "\n",
    "\n",
    "def sample_chisq(k: int) -> float:\n",
    "    \"\"\"\n",
    "    Sample from the chisquared distribution with k degrees of freedom\n",
    "    \n",
    "    Args:\n",
    "    -----\n",
    "    :k:    number of degrees of freedom (DoF)\n",
    "    \n",
    "    Return:\n",
    "    -------\n",
    "    :x:    random sample from the chisquared distribution with k degrees of freedom\n",
    "    \n",
    "    \"\"\"\n",
    "    return np.sum(np.array([sample_normal() for _ in range(k)]) ** 2)\n",
    "\n",
    "\n",
    "def sample_t(k: int) -> float:\n",
    "    \"\"\"\n",
    "    Sample from the t distribution with k degrees of freedom\n",
    "    \n",
    "    Args:\n",
    "    -----\n",
    "    :k:    number of degrees of freedom\n",
    "    \n",
    "    Return:\n",
    "    -------\n",
    "    :x:    random sample from the t distribution\n",
    "    \n",
    "    \"\"\"\n",
    "    z = sample_normal()\n",
    "    x = sample_chisq(k)\n",
    "    return z / np.sqrt(x/k)\n",
    "\n",
    "def sample_f(k1: int, k2:int) -> float:\n",
    "    \"\"\"\n",
    "    Sample from the F distribution with k1 over k2 degrees of freedom\n",
    "    \n",
    "    Args:\n",
    "    -----\n",
    "    :k1:    number of degrees of freedom numerator\n",
    "    :k2:    number of degrees of freedom denominator\n",
    "    \n",
    "    Return:\n",
    "    -------\n",
    "    :x:    random sample from the F distribution\n",
    "    \n",
    "    \"\"\"\n",
    "    chisq_1 = 1/k1 * sample_chisq(k1)\n",
    "    chisq_2 = 1/k2 * sample_chisq(k2)\n",
    "    return chisq_1/chisq_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa84a3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_ID = 302673355 # your personal ID number with leading zeros removed\n",
    "random.seed(MY_ID)\n",
    "\n",
    "# Your code to items 2-4 goes here:\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93fef3b-7ae8-4203-9aae-88c610d7076a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "num_bins = int(n * 0.08)\n",
    "sample_from_normal_clt = [sample_normal_clt(n)]\n",
    "sample_from_normal = np.array([sample_normal() for _ in range(n)])\n",
    "plt.hist(sample_from_normal_clt, bins=num_bins, color=\"blue\")\n",
    "plt.hist(sample_from_normal, bins=num_bins, color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68c6974-d788-4d4e-8d96-b31249172124",
   "metadata": {},
   "source": [
    "4. Use ``sample_chisq`` to sample $1000$ times from $\\chi^2_k$ and use one figure to illustrate the histogram of the samples and the true PDF of $\\chi^2_k$; for $k=5$ and $k=10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23352a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dofs = [5,10]\n",
    "for dof in dofs:\n",
    "    sample = [sample_chisq(dof) for _ in range(n)]\n",
    "    true_pdf_chi_squared = st.chi2.rvs(dof, size=n)\n",
    "    plt.hist(sample, bins=num_bins, color=\"blue\")\n",
    "    plt.hist(true_pdf_chi_squared, bins=num_bins, color=\"red\")    \n",
    "    plt.title(f\"chi squared with {dof} Degrees of Freedom. $N = {n}$\".title())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e225345-30fb-49fd-b7c5-d6072a7f688c",
   "metadata": {},
   "source": [
    "5. Use ``sample_t`` to sample $1000$ times from $t_k$ (the $t$-distribution with $k$ degrees of freedom) and use one figure to illustrate the histogram of the samples and the true PDF of $t_k$; for $k=5$ and $k=10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7a210c-0f5c-414b-bed0-87e4d1f5765b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dofs = [5,10]\n",
    "for dof in dofs:\n",
    "    sample = np.array([sample_t(dof) for _ in range(n)])\n",
    "    true_pdf_t = st.t.rvs(dof, size=1000)\n",
    "    plt.hist(sample, bins=num_bins, color=\"blue\")\n",
    "    plt.hist(true_pdf_t, bins=num_bins, color=\"red\")    \n",
    "    plt.title(f\"T distribution with {dof} Degrees of Freedom. $N = {n}$\".title())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c778a53-0497-427d-a0d5-acc686464a45",
   "metadata": {},
   "source": [
    "6. Use ``sample_F`` to sample $1000$ times from $F_{k_1,k_2}$ and use one figure to illustrate the histogram of the samples and the true PDF of $F_{k_1,k_2}$; for $(k_1, k_2)= (10,5)$ and $(k_1, k_2)= (50,45)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557f7781-cb0e-42b5-93bc-f3153f423497",
   "metadata": {},
   "outputs": [],
   "source": [
    "dofs = [[10,5], [50,45]]\n",
    "for dof in dofs:\n",
    "    sample = [sample_f(dof[0], dof[1]) for _ in range(n)]\n",
    "    true_pdf_f = st.f.rvs(dof[0], dof[1], size=n)\n",
    "    plt.hist(sample, bins = num_bins, color = \"blue\")\n",
    "    plt.hist(true_pdf_f, bins = num_bins, color = \"red\") \n",
    "    plt.title(f\"F distribution with $k_1$ = {dof[0]}, $k_2$ = {dof[1]} Degrees of Freedom. $N = {n}$\".title())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41acabdd-2140-40c8-a906-8fea3e704c3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c7c2837",
   "metadata": {},
   "source": [
    "#### Advanced Statistics for Data Science (Spring 2022)\n",
    "# Home Assignment 3\n",
    "#### Topics:\n",
    "- Statistical Estimation\n",
    "- Hypothesis Testing in one and two samples\n",
    "\n",
    "#### Due: 25/04/2022 by 18:30\n",
    "\n",
    "#### Instructions:\n",
    "- Write your name, Student ID, and date in the cell below. \n",
    "- Submit a copy of this notebook with code filled in the relevant places as the solution of coding excercises.\n",
    "- For theoretic excercises, you can either write your solution in the notebook using $\\LaTeX$ or submit additional notes.\n",
    "\n",
    "<hr>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972793a2",
   "metadata": {},
   "source": [
    "\n",
    "**Name**: Tzach Larboni\n",
    "\n",
    "**Student ID**: 302673355\n",
    "\n",
    "**Date**: 24.04.2022\n",
    "\n",
    "$\n",
    "\\newcommand{\\Id}{{\\mathbf{I}}}  \n",
    "\\newcommand{\\SSE}{\\mathsf{SSE}}\n",
    "\\newcommand{\\SSR}{\\mathsf{SSR}}\n",
    "\\newcommand{\\MSE}{\\mathsf{MSE}}\n",
    "\\newcommand{\\simiid}{\\overset{iid}{\\sim}}\n",
    "\\newcommand{\\ex}{\\mathbb E}\n",
    "\\newcommand{\\var}{\\mathrm{Var}}\n",
    "\\newcommand{\\Cov}[2]{{\\mathrm{Cov}  \\left(#1, #2 \\right)}}\n",
    "\\newcommand{\\one}[1]{\\mathbf 1 {\\left\\{#1\\right\\}}}\n",
    "\\newcommand{\\SE}[1]{\\mathrm{SE} \\left[#1\\right]}\n",
    "\\newcommand{\\reals}{\\mathbb R}\n",
    "\\newcommand{\\Ncal}{\\mathcal N}\n",
    "\\newcommand{\\abs}[1]{\\ensuremath{\\left\\vert#1\\right\\vert}}\n",
    "\\newcommand{\\rank}{\\operatorname{rank}}\n",
    "\\newcommand{\\tr}{\\operatorname{Tr}}\n",
    "\\newcommand{\\diag}{\\operatorname{diag}}\n",
    "\\newcommand{\\sign}{\\operatorname{sign}}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50517ab",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df1289d1-23e6-4b47-95e4-ef8304d5501a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as st\n",
    "import scipy.linalg\n",
    "import math\n",
    "import statsmodels.formula.api as smf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93acbbf2",
   "metadata": {},
   "source": [
    "## Problem 1 (Variance Estimation)\n",
    "\n",
    "Consider the variance estimate\n",
    "$$\n",
    "s^2 = \\frac{1}{n-1} \\sum_{i=1}^n(y_i - \\bar{y})^2. \n",
    "$$\n",
    "If $Y_i \\simiid \\Ncal(\\mu,\\sigma^2)$, then \n",
    "$$\n",
    "\\frac{n-1}{\\sigma^2}s^2 \\sim \\chi^2_{n-1}.\n",
    "$$\n",
    "1. Use this information to derive a $1-\\alpha$ coinfidence interval for $\\sigma^2$ (express $L$ and $U$ in terms of $s^2$, $n$, and the relevant quantiles of the $\\chi^2$ distribution). \n",
    "2. For $n = 2,\\ldots,10$ and $\\alpha=0.05$, report on the lower ($L$) and upper ($U$) values of the coinfidence interval in terms of $s^2$. \n",
    "3. How large $n$ must be to obtain a $0.95$ coinfidence interval of size $0.1s^2$? \n",
    "\n",
    "The point: the number of degrees of freedom needed for a reasonable ($10\\%$ range) estimate of the variance can be very large. Sometimes, much larger than our data permit.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8145013-a56e-44e9-858b-1b571fc3749c",
   "metadata": {},
   "source": [
    "### Answer\n",
    "##### Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59b32be-2f58-4ff9-9072-24fa3492470e",
   "metadata": {},
   "source": [
    "We know that $$\\frac{n-1}{\\sigma^2}s^2 \\sim \\chi^2_{n-1} $$\n",
    "Hence $$ \\chi^2_{1-\\frac{\\alpha}{2}} < \\frac{n-1}{\\sigma^2}s^2 < \\chi^2_\\frac{\\alpha}{2}$$\n",
    "If we clear the fractions we'll get $$\\frac{n-1}{\\chi^2_{1-\\frac{\\alpha}{2}}} s^2 < \\sigma^2 < \\frac{n-1}{\\chi^2_\\frac{\\alpha}{2}} s^2$$\n",
    "\n",
    "Meaning, $$L =  \\frac{n-1}{\\chi^2_{1-\\frac{\\alpha}{2}}} s^2 $$\n",
    "$$U = \\frac{n-1}{\\chi^2_\\frac{\\alpha}{2}} s^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8f0f4c-9b08-40af-a100-2bf5254242bc",
   "metadata": {},
   "source": [
    "#### Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b4447cb-358e-4363-9fb6-be4c787e730d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = np.linspace(2, 10, num=9)\n",
    "alpha = 0.05\n",
    "\n",
    "def get_c_i(n, alpha, should_print=False):\n",
    "    num = dof = n - 1\n",
    "    u_den = st.chi2.ppf(alpha/2, dof)\n",
    "    l_den = st.chi2.ppf(1 - alpha/2, dof)\n",
    "    if should_print:\n",
    "        print(f\"For n = {n}:\\n   L = {num/l_den:.3f} * s^2 \\n   U = {num/u_den:.3f} * s^2\")\n",
    "    return (num/l_den, num/u_den)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97665e6a-2360-4ad7-bdc1-4a3f81f43805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n = 2.0:\n",
      "   L = 0.199 * s^2 \n",
      "   U = 1018.258 * s^2\n",
      "For n = 3.0:\n",
      "   L = 0.271 * s^2 \n",
      "   U = 39.498 * s^2\n",
      "For n = 4.0:\n",
      "   L = 0.321 * s^2 \n",
      "   U = 13.902 * s^2\n",
      "For n = 5.0:\n",
      "   L = 0.359 * s^2 \n",
      "   U = 8.257 * s^2\n",
      "For n = 6.0:\n",
      "   L = 0.390 * s^2 \n",
      "   U = 6.015 * s^2\n",
      "For n = 7.0:\n",
      "   L = 0.415 * s^2 \n",
      "   U = 4.849 * s^2\n",
      "For n = 8.0:\n",
      "   L = 0.437 * s^2 \n",
      "   U = 4.142 * s^2\n",
      "For n = 9.0:\n",
      "   L = 0.456 * s^2 \n",
      "   U = 3.670 * s^2\n",
      "For n = 10.0:\n",
      "   L = 0.473 * s^2 \n",
      "   U = 3.333 * s^2\n"
     ]
    }
   ],
   "source": [
    "for n in ns:\n",
    "    get_c_i(n, alpha, should_print=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c010e240-3ad0-4298-b3d2-a40d481ec5a5",
   "metadata": {},
   "source": [
    "#### Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b967c1a0-cf33-40b8-8deb-c0375d17a6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To obstain a cofidence interval in the size of 0.1*s^2, we need to have at least 3082 samples\n"
     ]
    }
   ],
   "source": [
    "ns = np.array(range(2,1_000_000))\n",
    "for n in ns:\n",
    "    l,u = get_c_i(n, alpha)\n",
    "    if u-l <= 0.1:\n",
    "        print(f\"To obstain a cofidence interval in the size of 0.1*s^2, we need to have at least {n} samples\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e3f051",
   "metadata": {},
   "source": [
    "## Problem 2 (Correlated Data)\n",
    "\n",
    "Suppose that $Y_1,\\ldots,Y_n$ has each mean $\\mu$ and variance $\\sigma^2$, but \n",
    "$$\n",
    "\\rho_{ij} := \\mathrm{Corr}(Y_i,Y_j) = \\begin{cases}\n",
    "1 & i=j \\\\\n",
    "\\rho & |i-j| = 1 \\\\\n",
    "0 & |i-j| > 1\n",
    "\\end{cases}.\n",
    "$$\n",
    "This situation arise when an observation $i$ may depend to some extent on only the previous observation‚Äôs white noise: an one-lag \"holdeover effect\". This is also known as the \"lag-1 moving average\" model (MA(1)). \n",
    "\n",
    "1. Show that:\n",
    "  - $$\\mathrm{Var}(\\bar{Y}) = \\frac{\\sigma^2}{n}(1+ 2\\rho \\frac{n-1}{n})$$\n",
    "Namely, positive correlation increases varaince. Hint: use that $\\mathrm{Var}(U+V) = \\mathrm{Var}(U) + \\mathrm{Var}(V) + 2 \\mathrm{Cov}(U,V)$ and induction or recursive computation over $n$. Another option is to write $Y = \\Sigma^{1/2}Z$ where $Z\\sim \\Ncal(0,I)$ and $\\Sigma^{1/2}$ is symmetric with  $\\Sigma^{1/2}\\Sigma^{1/2} = \\Sigma$ has the desired covariance stracture.  \n",
    "\n",
    "  - $$\\qquad \\ex[{s^2}] = \\sigma^2(1 - 2\\rho/n)$$\n",
    "  where $s^2$ is the standard varince estiamte. \n",
    "Namely, with positive correlation the \"variety\" in the data is smaller. \n",
    "\n",
    "  - **(Bonus)** The t-statistic statisfies\n",
    "$$\n",
    "t = \\sqrt{n} \\frac{\\bar{Y}-\\mu}{s} \\to \\Ncal(0,1 + 2 \\rho),\\quad n \\to \\infty\n",
    "$$\n",
    "Hint: you may use the following version of Slutsky's Theorem: for two sequences of RV U_n and V_n, if $U_n \\overset{D}{\\to} U$ and $V_n \\overset{p}{\\to} c$ (constant), then $ V_n U_n \\overset{D}{\\to} cU$\n",
    "2. Verify your answer to the first two items in 1 using simulations. Use `nMonte = 10000` problem instances. In each instance, use a sample size of `n = 10` with $\\sigma=1$ and $\\rho \\in \\{\\pm 0.1, \\pm 0.3, \\pm0.5\\}$. The function `genrate_correlated_data` below generates noramlly distributed data satisfying the correlation model above. \n",
    "\n",
    "3. Suppose $\\rho>0$\n",
    " - Derive a $1-\\alpha$ confidence interval based on $s$ and the $t$-distribution with $n-1$ DoF. Does your interval \n",
    " contains the value of $\\mu$ more or less often than $1-\\alpha$? Verify using a simulation with `nMonte = 10000` problem instances of sampes size `n=100`. Also use $\\alpha=0.05$, $\\rho=.25$, $\\sigma =1$, and $\\mu_0=2$. \n",
    " \n",
    " - Suppose that we reject $H_0\\,:\\,\\mu = \\mu_0$ whenever $t$ exceeds the critical value $t_{n-1}^{1-\\alpha/2}$. Would our P-value be too small or too large? Would we reject more or less often then $\\alpha$ if the null $\\mu = \\mu_0$ is true? Verify using a simulation with `nMonte = 10000` problem instances of sampes size `n=100`. Also use $\\alpha=0.05$, $\\rho=.25$, $\\sigma =1$, and $\\mu_0=2$. \n",
    " \n",
    " - Would your answer to the preivous two items change if $\\rho < 0$? how?\n",
    "\n",
    "The point: correlation in our data is bad because it makes us make wrong descisions. The effect of correlation is much worst than non-nomrality since the latter diminishes with $n$ due to the CLT. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d03b95b-4874-49f2-84ad-a9510f12d561",
   "metadata": {},
   "source": [
    "### Answer\n",
    "##### Part 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cad765-d883-4d1e-a3bf-b39c3e223631",
   "metadata": {},
   "source": [
    "We'll prove by induction:\n",
    "\n",
    "1. Base case  \n",
    "If $n=2$, then $$Var(\\bar{Y}) = Var(\\frac{Y_1+Y_2}{2}) = Var(\\frac{Y_1}{2}) + Var(\\frac{Y_2}{2}) + 2Cov(\\frac{Y_1}{2}, \\frac{Y_2}{2})$$ \n",
    "Since $Cov(X,Y) = \\rho \\sigma_X \\sigma_Y $, we'll plug in:\n",
    "$$ =\\frac{1}{4}[Var(Y_1) + Var(Y_2) + 2\\rho Std(\\frac{Y_1}{2})Std(\\frac{Y_2}{2}) $$\n",
    "$$ =\\frac{\\sigma^2}{2} + 2\\rho \\frac{\\sigma^2}{4}$$\n",
    "$$ =\\frac{\\sigma^2}{2}(1 + 2\\rho \\frac{1}{2})$$\n",
    "$$ =\\frac{\\sigma^2}{n}( 1 + {2\\rho} \\frac{n-1}{n})$$\n",
    "\n",
    "2. Induction hypothesis:\n",
    "$$Var(\\bar{Y_n}) = \\frac{\\sigma^2}{n}(1+ 2\\rho \\frac{n-1}{n})$$\n",
    "\n",
    "3. Induction step:\n",
    "$$ Var(\\bar(Y_{n+1} = Var(\\frac{1}{n+1}\\Sigma_{i=1}^{n+1}Y_i)$$\n",
    "We'll multiply and divide by $n$\n",
    "$$ = Var(\\frac{n}{n+1} * \\frac{1}{n} \\Sigma_{i=1}^{n+1}Y_i)$$\n",
    "$$ = (\\frac{n}{n+1})^2 Var(\\frac{1}{n} \\Sigma_{i=1}^{n+1}Y_i)$$\n",
    "$$ = (\\frac{n}{n+1})^2 Var(\\frac{1}{n} \\Sigma_{i=1}^{n}Y_i + \\frac{1}{n}Y_{n+1}) $$\n",
    "$$ = (\\frac{n}{n+1})^2 [Var(\\bar{Y_n}) + Var(\\frac{1}{n} Y_{n+1}) + 2Cov(\\bar{Y_n},\\frac{1}{n} Y_{n+1})] $$\n",
    "$$ = (\\frac{n}{n+1})^2 [\\frac{\\sigma^2}{n}(1+ 2\\rho \\frac{n-1}{n}) + \\frac{1}{n^2} \\sigma^2 + 2Cov(\\bar{Y_n},\\frac{1}{n} Y_{n+1})] $$\n",
    "\n",
    "Let's examine $Cov(\\bar{Y_n},\\frac{1}{n} Y_{n+1})$. Since $\\bar{Y_n} = \\frac{Y_1 + Y+2 + ... + Y_n}{n}$ we can deduce\n",
    "$$ Cov(\\bar{Y_n},\\frac{1}{n} Y_{n+1}) = \\frac{1}{n^2}[Cov(Y_1,Y_{n+1}) + Cov(Y_2,Y_{n+1}) + ... + Cov(Y_n,Y_{n+1}) $$\n",
    "$$ = \\frac{1}{n^2} (0 + 0 + ... \\rho \\sigma^2)$$\n",
    "$$ = \\frac{\\rho \\sigma^2}{n^2} $$\n",
    "\n",
    "Plug it back in:\n",
    "$$ = (\\frac{n}{n+1})^2 [\\frac{\\sigma^2}{n}(1+ 2\\rho \\frac{n-1}{n}) + \\frac{1}{n^2} \\sigma^2 +2\\frac{\\rho \\sigma^2}{n^2}] $$\n",
    "$$ = (\\frac{n}{n+1})^2 * \\frac{\\sigma^2}{n}[1 + 2\\rho \\frac{n-1}{n} + \\frac{1}{n} + 2\\frac{\\rho}{n}]$$\n",
    "$$ = (\\frac{n\\sigma^2}{(n+1)^2}) (\\frac{n + 2n\\rho - 2\\rho + 1 +2\\rho}{n}) $$\n",
    "$$ = (\\frac{\\sigma^2}{(n+1)^2}) (n + 2n\\rho + 1) $$\n",
    "$$ = (\\frac{\\sigma^2}{n+1}) (\\frac{n + 1 + 2n\\rho}{n + 1})$$\n",
    "$$ = (\\frac{\\sigma^2}{n+1}) (1 + \\frac{2n\\rho}{n + 1})$$\n",
    "$$Q.E.D$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae3070a-784f-4e59-83ca-2a85c4f9cbbf",
   "metadata": {},
   "source": [
    "##### Part 1.2\n",
    "We know that by definition:\n",
    "$$\\ex[{s^2}] = \\ex[\\frac{\\Sigma_{i=1}^{n}(Y_i-\\bar{Y})^2}{n-1}] $$\n",
    "$$ = \\frac{1}{n-1} \\ex[\\Sigma_{i=1}^{n}[(Y_i-\\mu)- (\\bar{Y}-\\mu)] ^2 ]$$\n",
    "$$ = \\frac{1}{n-1}[(\\Sigma_{i=1}^{n}Var(Y_i)) - nVar(\\bar{Y})] $$\n",
    "We'll plug in the answer from the previous part:\n",
    "$$ \\ex[{s^2}] = \\frac{1}{n-1}[n\\sigma^2 - n \\frac{\\sigma^2}{n}(1+\\frac{2\\rho(n-1)}{n})] $$\n",
    "$$ = \\sigma^2 \\frac{1}{n-1} (n - 1 - \\frac{2\\rho(n-1)}{n}$$\n",
    "$$ = \\sigma^2 (\\frac{n-1-2\\rho(n-1)}{(n-1)n} $$\n",
    "$$ = \\frac{\\sigma^2(n-1)(1-2\\rho}{n(n-1)}$$\n",
    "$$ = \\sigma^2 \\frac{1-2\\rho}{n}$$\n",
    "$$ Q.E.D$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920454a3-9a53-44bc-8e2c-53cfae61b54c",
   "metadata": {},
   "source": [
    "#### Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "210ea7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genrate_correlated_data(n: int, rho: float, mu: float, sigma: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate samples from the model:\n",
    "    Yi ~ N(mu, sigma^2) and Corr(Yi,Yj) = ( i == j ) + rho * ( abs( i - j ) == 1 )\n",
    "    \n",
    "    Args:\n",
    "    -----\n",
    "    :n:     sample size\n",
    "    :rho:   desired one lag correlation between samples\n",
    "    :mu:    mean\n",
    "    :sigma: standard deviation\n",
    "    \n",
    "    \"\"\"\n",
    "    assert sigma > 0\n",
    "    \n",
    "    # build desired covariance matrix\n",
    "    Sig = np.zeros((n,n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i==j : \n",
    "                Sig[i,j] = 1\n",
    "            if np.abs(i-j) == 1:\n",
    "                Sig[i,j] = rho\n",
    "                Sig[j,i] = rho\n",
    "                \n",
    "    # get matrix square root of covariance matrix:\n",
    "    Sig_sqrt = np.linalg.cholesky(sigma**2 * Sig)\n",
    "    \n",
    "    # sample from the standard normal dist. and transform \n",
    "    # so that the result is a normal vector with the desired \n",
    "    # covaraince structure\n",
    "    return mu + Sig_sqrt @ np.random.randn(n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ba7bab-dc96-42c2-81f2-b47aeb2dcfe7",
   "metadata": {},
   "source": [
    "  - $$\\mathrm{Var}(\\bar{Y}) = \\frac{\\sigma^2}{n}(1+ 2\\rho \\frac{n-1}{n})$$\n",
    "\n",
    "  - $$\\qquad \\ex[{s^2}] = \\sigma^2(1 - 2\\rho/n)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "716de51f-e658-4de0-b087-518e91d430a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nMonte = 10000\n",
    "n = 10\n",
    "sigma = 1\n",
    "mu = 0\n",
    "rhos = [0.1, -0.1, 0.3, -0.3, 0.5, -0.5]\n",
    "\n",
    "def calc_theoretic_y_bar_variance(sigma, n, rho):\n",
    "    return (sigma ** 2 / n) * (1 + 2 * rho * (n - 1)/n)\n",
    "    \n",
    "def calc_theoretic_mean_sample_var(sigma, rho, n):\n",
    "    return sigma **2 * (1 - 2 * rho / n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b9b755-4978-465c-a59e-714642c4c7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for rho in rhos:    \n",
    "    data = [genrate_correlated_data(n=n, rho=rho, mu=mu, sigma=sigma) for _ in range(nMonte)]\n",
    "    data = np.asarray(data)\n",
    "    \n",
    "    y_bars = np.mean(data, axis=1).reshape(-1,1)\n",
    "    sigma_squared = (1 / (n - 1)) * np.sum((data - y_bars) ** 2, axis=1)\n",
    "    \n",
    "    empirical_y_bars_var = np.var(y_bars)\n",
    "    empirical_sigma_squared_mean = np.mean(sigma_squared) \n",
    "\n",
    "    theoretical_y_bar_var = calc_theoretic_y_bar_variance(sigma, n, rho)\n",
    "    theoretical_sigma_squared_mean = calc_theoretic_mean_sample_var(sigma=sigma, rho=rho, n=n)\n",
    "\n",
    "    print(f\"For rho {rho}:\\n   The theoretical y bar variance is {theoretical_y_bar_var:.3f}, while the emprical is {empirical_y_bars_var:.3f}.\\n   The theoretical sigma squared mean is {theoretical_sigma_squared_mean:.3f}, while the emprical is {empirical_sigma_squared_mean:.3f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6181297-2337-4690-8887-5b5d68c66ee6",
   "metadata": {},
   "source": [
    "#### Part 3\n",
    "\n",
    "3. Suppose $\\rho>0$\n",
    " - Derive a $1-\\alpha$ confidence interval based on $s$ and the $t$-distribution with $n-1$ DoF. Does your interval \n",
    " contains the value of $\\mu$ more or less often than $1-\\alpha$? Verify using a simulation with `nMonte = 10000` problem instances of sampes size `n=100`. Also use $\\alpha=0.05$, $\\rho=.25$, $\\sigma =1$, and $\\mu_0=2$. \n",
    " \n",
    " - Suppose that we reject $H_0\\,:\\,\\mu = \\mu_0$ whenever $t$ exceeds the critical value $t_{n-1}^{1-\\alpha/2}$. Would our P-value be too small or too large? Would we reject more or less often then $\\alpha$ if the null $\\mu = \\mu_0$ is true? Verify using a simulation with `nMonte = 10000` problem instances of sampes size `n=100`. Also use $\\alpha=0.05$, $\\rho=.25$, $\\sigma =1$, and $\\mu_0=2$. \n",
    " \n",
    " - Would your answer to the preivous two items change if $\\rho < 0$? how?\n",
    "\n",
    "The point: correlation in our data is bad because it makes us make wrong descisions. The effect of correlation is much worst than non-nomrality since the latter diminishes with $n$ due to the CLT. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b32e04-6d43-44a4-9a39-926bfba49861",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "alpha = 0.05\n",
    "rho_positive = 0.25\n",
    "rho_negative = -1 * rho_positive\n",
    "sigma = 1\n",
    "mu_0 = 2\n",
    "nMonte = 100_0\n",
    "\n",
    "def calc_hyptohesis(n, alpha, rho, sigma, mu, mu_0, nMonte):\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    for _ in range(nMonte):\n",
    "        data = genrate_correlated_data(n=n, rho=rho, mu=mu_0, sigma=sigma)\n",
    "        y_bar_i = np.mean(data)\n",
    "        s = np.sqrt((1 / (n - 1)) * np.sum((data - y_bar_i) ** 2))\n",
    "        t_alpha_bar = st.t.ppf(1 - alpha / 2, df=n-1)\n",
    "\n",
    "        l = mu_0 - (s / np.sqrt(n)) * t_alpha_bar\n",
    "        u = mu_0 + (s / np.sqrt(n)) * t_alpha_bar\n",
    "        t_stat = (y_bar_i - mu_0) / (s/ np.sqrt(n))\n",
    "        df = df.append(dict(\n",
    "            true_mu = mu,\n",
    "            alpha = alpha,\n",
    "            t_stat = t_stat,\n",
    "            n = n,\n",
    "            s = s,\n",
    "            l = l,\n",
    "            u = u,\n",
    "            reject_h_0 = t_stat > st.t.ppf(1 - alpha / 2, df= n - 1),\n",
    "            mu_in_ci = l <= y_bar_i <= u\n",
    "            ), ignore_index=True)\n",
    "    \n",
    "    print(f\"For rho = {rho}, Mu appears in the CI in {df['mu_in_ci'].value_counts()[1.0]/len(df):.2f}% of the times, while we expected {1-alpha}% of the times.\")\n",
    "    print(f\"If we were to reject H0 if t exceeded the critical value mentioned, we would reject H0 {df['reject_h_0'].value_counts()[1.0]/len(df):.2f}% of the time. The difference is {df['reject_h_0'].value_counts()[1.0]/len(df) - alpha:.2f}.\")\n",
    "    \n",
    "    return None\n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd7614a-22f1-4dc1-9b9d-c6005147fdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_hyptohesis(n=n, alpha=alpha, rho=rho_positive, sigma=sigma, mu=0, mu_0=2, nMonte=nMonte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb6ceb1-0c37-41d7-89fc-fa318698f4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_hyptohesis(n=n, alpha=alpha, rho=rho_negative, sigma=sigma, mu=0, mu_0=2, nMonte=nMonte)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9d8ddd-8a38-4829-a0e5-43b3e0a54eda",
   "metadata": {},
   "source": [
    "It seems like when $\\rho > 0$ we are rejecting more than $\\alpha$ by a little bit, and if $\\rho < 0$ we are rejecting more than $\\alpha$ by quite a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5581e1ca",
   "metadata": {},
   "source": [
    "## Problem 3 (Regression and Hypothesis Testing)\n",
    "\n",
    "The dataset for this problem is available in the file temp_TLV_beach.csv, which was taken directly from the meterological service website (https://ims.data.gov.il/ims/1). \n",
    "\n",
    "We consider monitoring changes in rainfall/precipitation over the years at Station 136320 located at Tel-Aviv beach area. \n",
    "To do so, we will set up a standard linear model with $p = 3$ features, where for dates (times) $t \\in \\{0,1,\\ldots,366\\}$ (we have 366 for leap years) we set\n",
    "$$\n",
    "y_t = \\beta_0 + \\beta_1 \\cos( 2\\pi(t/365)) + \\beta_2 \\sin( 2\\pi(t/365)) + \\epsilon_t,\\qquad t=1,\\ldots,n. \n",
    "\\label{eq:model} \\tag{2}\n",
    "$$\n",
    "(note that the dataset does not contain measurments from all days in the range)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c92dbd",
   "metadata": {},
   "source": [
    "1) Set $y_t = \\texttt{Rainfall}$. \n",
    " - Plot $y_t$ versus $t=$`Date` and identify winter times.\n",
    "\n",
    "- Find the LS regression coefficients $\\beta$; plot the fitted response $\\hat{y}_t$ over time along with the original response $y_t$. \n",
    "\n",
    " - Test whether the fitted model significantly improves on the trival model $y_t = \\beta'_0 + \\epsilon_t$.\n",
    " \n",
    " - For each parameter $p$, report the P-value for testing $H_0\\,:\\,\\hat{\\beta}_p = 0$ and indicate whether this parameter is \n",
    "significantly different than $0$ at level $\\alpha = 0.01$. \n",
    "(for this item, you can either evaluate everything from the formulas provded in class or use a statistical package like `statsmodels`)\n",
    "\n",
    "You may use the code below to format the `Date` column correctly and extract other relevant information from it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20e28fd-62d8-48ec-8a6f-257bbbc032f3",
   "metadata": {},
   "source": [
    "### Answer\n",
    "#### Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dde4de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('rainfall_TLV_beach.csv')\n",
    "\n",
    "data['Date'] = pd.to_datetime(data.Date, format=\"%d-%m-%Y\")\n",
    "data['DayOfYear'] = data.Date.dt.day_of_year\n",
    "data['Month'] = data.Date.dt.month\n",
    "data['Year'] = data.Date.dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dffffd-d32a-49ca-9813-b3c35c8383aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [15, 8]\n",
    "y_t = data['Rainfall']\n",
    "sns.scatterplot(y=y_t, x=data['Date'])\n",
    "plt.title('Rainfall per date'.title())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b063dc6b-8760-4954-8f81-310dad00ebf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(y=y_t, x=data['Month'])\n",
    "plt.title('Rainfall per day of the year'.title())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d219325-78b0-4e2c-83de-315dd03394cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"It looks like the months {sorted(data['Month'][data['Rainfall'] > 30].value_counts().keys())} are the winter months, as they contain the rainest days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cdaa12-42c9-4342-8ac5-07d20b0bb1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['X_1'] = np.cos(2* np.pi * data['DayOfYear']/365)\n",
    "data['X_2'] = np.sin(2* np.pi * data['DayOfYear']/365)    \n",
    "\n",
    "\n",
    "X = data[['X_1', 'X_2']]\n",
    "Z = np.c_[np.ones_like(y_t), X]\n",
    "\n",
    "beta_hat = np.linalg.inv(Z.T @ Z) @ Z.T @ y_t\n",
    "\n",
    "print(f\"The LS estimator Beta hat is the vector {[round(b,2) for b in list(beta_hat)]}^T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2729e3-d480-48b1-8e8a-833b58f9b1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = Z @ beta_hat \n",
    "sns.scatterplot(y=y_t, x=data['Date'], palette='blue')\n",
    "sns.scatterplot(y=y_hat, x=data['Date'], palette='red')\n",
    "plt.title('preidcted rainfall vs. actual rainfall per data'.title())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba55d74-117d-4dca-a834-ae9c2d8a22b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['trivial'] = np.ones_like(len(data))\n",
    "X_trivial = data['trivial']\n",
    "Z_trivial = np.c_[np.ones_like(y_t)]\n",
    "\n",
    "beta_hat_trivial = np.linalg.inv(Z_trivial.T @ Z_trivial) @ Z_trivial.T @ y_t\n",
    "y_hat_trivial = Z_trivial @ beta_hat_trivial\n",
    "\n",
    "print(f\"The LS estimator Beta hat is the vector {[round(b,2) for b in list(beta_hat_trivial)]}^T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5ab557-b0a6-44fe-8ac1-3376917e90f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_sss(y_hat, y, r_squared_mode=False):\n",
    "    y_bar = np.mean(y)\n",
    "    ss_fit = np.sum((y_hat - y_bar)**2)\n",
    "    ss_tot = np.sum((y - y_bar)**2)\n",
    "    if r_squared_mode:\n",
    "        return ss_fit / ss_tot \n",
    "    return ss_fit, ss_tot, ss_tot - ss_fit\n",
    "\n",
    "print(f\"The R^2 of the more complex model is {calc_sss(y_hat, y_t, r_squared_mode=True):.3f} while the R^2 of the trivial model is {calc_sss(y_hat_trivial, y_t, r_squared_mode=True):.3f}.\\nHence, we have a good reason to believe the more complex model is the better one.\")\n",
    "      \n",
    "      \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c1c718-9aa2-4a07-8bd2-19f3a727196c",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 3\n",
    "q = 2\n",
    "dof = len(y_t) - p\n",
    "\n",
    "ss_fit_full, ss_tot_full, ss_full = calc_sss(y_hat, y_t)\n",
    "ss_fit_trivial, ss_tot_trivial, ss_trivial = calc_sss(y_hat_trivial, y_t)\n",
    "\n",
    "F = ((1/(p-q)) * (ss_trivial - ss_full)) / (ss_full / dof)\n",
    "p_val_f = 2 * (1 - st.f.cdf(F, dfn=p-q, dfd=dof))\n",
    "print(f\"The p value of the F-test is {p_val_f:.4f}. \\n Meaning, we cannot say that the trivial model is significantly better than the larger model. \\n Hence it does not provide a valid representation of the data, compared to the larger model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dcf3fd-bf6a-44bb-ba59-f9aff8c0d8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smf.ols(formula='Rainfall ~ X_1 + X_2', data=data).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57c0c0d-78ef-4e55-adb3-952e44ea0694",
   "metadata": {},
   "source": [
    "We can see that we can reject $H_0: X_1 = 0$ at $\\alpha = 0.01$, while we cannot do the same for $X_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7343c68",
   "metadata": {},
   "source": [
    "2) We would like to test whether future data follows a similar distribution to past data. Consider two datasets modeled by\n",
    "$$\n",
    "\\begin{equation}\n",
    "y = Z \\beta + \\epsilon,\\qquad y_{new} = Z_{new} \\beta + \\epsilon_{new}\n",
    "\\label{eq:model} \\tag{2}\n",
    "\\end{equation}\n",
    "$$\n",
    "where $Z \\in \\reals^{m\\times p}$ and $Z_{new} \\in \\reals^{n\\times p}$ are the given design matrices which both assume to have rank $p$. We also assume that $\\epsilon$ and $\\epsilon_{new}$ are independent. We will think of $(Z, y)$ as the initial data pair and $(Z_{new},y_{new})$ as the new data.\n",
    "\n",
    "Let $\\hat{\\beta} = (Z \\top Z)^{-1}Z^\\top y$ be the usual least-squares (LS) estimate on the initial data. Define the predicted values as\n",
    " $$\n",
    " \\hat{y}_{new} := Z_{new} \\hat{\\beta}\n",
    " $$\n",
    " (note that $\\hat{y}_{new}$ is not the LS estiamte of $y_{new}$ from $Z_{new}$)\n",
    " \n",
    " - Show that $\\mathrm{Cov}(y-\\hat{y},y_{new} - \\hat{y}_{new})=0$\n",
    " \n",
    " - Assume $\\epsilon_{new} \\sim \\Ncal(0,\\sigma^2 I_n)$. Find a (symmetric, positive definite) matrix $M \\in \\reals^{n \\times n}$ so that\n",
    " $$\n",
    " M(y_{new} - \\hat{y}_{new}) \\sim \\Ncal(0, \\sigma^2 I_n).\n",
    " $$\n",
    " \n",
    "- Give the distribution of the ratio\n",
    "$$\n",
    "\\begin{equation}\n",
    "A:= \\frac{\\frac{1}{n}\\left\\| M(Y_{new} - \\hat{Y}_{new})\\right\\|^2}{\\frac{1}{m-d} \\left\\| Y  - \\hat{Y} \\right\\|^2 }\n",
    "\\label{eq:A} \\tag{3}\n",
    "\\end{equation}\n",
    "$$\n",
    "under the null hypothesis:\n",
    "$$\n",
    "H_0\\,:\\,\\begin{cases} Y = Z \\beta + \\epsilon,\\qquad Y_{new} = Z_{new} \\beta + \\epsilon_{new} \\\\\n",
    "\\epsilon \\sim \\Ncal(0, \\sigma^2 I_m),\\qquad \\epsilon_{new} \\sim \\Ncal(0, \\sigma^2 I_n) \\\\\n",
    "\\text{$\\epsilon$ and $\\epsilon_{new}$ are independent}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "We now consider implementing a series of hypothesis tests about whether daily rainfall is remaining consistent over the years or whether it is changing in some meaningful way. \n",
    "\n",
    "- For each of the years 2010, 2011,...,2021, repeat the following. Define a data matrix $Z$ using the features in $\\eqref{eq:model}$ consisting of all dates prior to that year (so that for 2010, $Z$ will be\n",
    "a data matrix for the years 2005‚Äì2009, for 2011, $Z$ will be the data for years 2005-2011, and\n",
    "so on). Define the responses $y$ to consist of rainfall for the given years. Define the new data matrix $Z_{new} \\in \\reals^{n \\times p}$ to consist of the $n$ days of measurements in the given year ($n\\leq 366$) and the responses $y_{new}$ to be the rainfall in those days. For this data, compute the statistic $A$ in $\\eqref{eq:A}$ and its p-value, that is, conditional on\n",
    "$A = a$, report\n",
    "$$\n",
    "p := \\Pr[A \\geq a] \\quad \\text{under $H_0$}\n",
    "$$\n",
    "Plot the P-values for each of the years and also print their values. Discuss briefly. \n",
    "- Suppose that you obtained a very small p-value of some year, say $p \\approx 10^{-5}$. Does rejecting the null hypothesis necessarily mean that the distribution of rainfall is changing over time? explain in 2-3 sentences. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327d74ac-bdfd-4347-9a19-38a1c69ee50f",
   "metadata": {},
   "source": [
    "### Answer\n",
    "#### Part 2.1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da00710b-a57a-4046-970b-6e160c8b109c",
   "metadata": {},
   "source": [
    "We know that $y - \\hat{y} = \\hat{\\epsilon}$, that $y_{new} = Z_{new} \\beta + \\epsilon_{new} $ and that $\\hat{y}_{new} = Z_{new}\\hat{\\beta} $, so we can plug in:\n",
    "$$ Cov(\\hat{\\epsilon}, Z_{new}\\beta + \\epsilon_{new} - Z_{new} \\hat{\\beta})$$\n",
    "$$ = Cov(\\hat{\\epsilon}, \\epsilon_{new}) - Cov(\\hat{\\epsilon}, Z_{new}\\hat{\\beta})$$\n",
    "\n",
    "Since $Z_{new}$ is not a random variable, it goes out of the parentheses:\n",
    "$$ = Cov(\\hat{\\epsilon}, \\epsilon_{new}) - Cov(\\hat{\\epsilon}, \\hat{\\beta})Z_{new}$$\n",
    "\n",
    "We assume that the first expressions is $0$, since we have assumed that $\\epsilon$ and $\\epsilon_{new}$ are independent. In addition, since $\\hat{\\epsilon}$ is set to be independent of $\\hat{\\beta}_{new}$ (was proved in previous assignment), we know that the second expressions is $0$ as well.\n",
    "\n",
    "$$Q.E.D$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4f00ed-5771-4739-8cdd-9cbed98d8716",
   "metadata": {},
   "source": [
    "#### Part 2.1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6160a65-6d72-4112-87af-1a99cf37e7db",
   "metadata": {},
   "source": [
    "We know that:\n",
    "1. $y_{new} - \\hat{y}_new = Z_{new}\\beta + \\epsilon_{new} - Z_{new}\\beta $\n",
    "2. $\\hat{\\beta} \\sim \\Ncal(\\beta, \\sigma^2(Z^TZ)^{-1})$\n",
    "\n",
    "We can multiply $ \\beta $ from the left by $Z_{new} $\n",
    "\n",
    "$$ Z_{new}\\beta \\sim \\Ncal(Z_new \\beta, Z_new \\sigma^2(Z^TZ)^{-1} Z_{new}^T)$$\n",
    "\n",
    "Looking at 1., since $\\epsilon_{new} \\sim \\Ncal (0, \\sigma^2 I)$, and since $\\epsilon_{new}$ and $\\hat{\\beta}$ are independent:\n",
    "$$ y_{new} - \\hat{y}_{new} \\sim \\Ncal(0, \\sigma^2(I-Z_{new}(Z^TZ)^{-1}Z_{new}^T)) $$\n",
    "\n",
    "Now we'll multiple $y_{new} - \\hat{y}_{new}$ by $M$ from the left:\n",
    "\n",
    "$$ M(y_{new} - \\hat{y}_{new}) \\sim \\Ncal(0, M\\sigma^2(I-Z_{new}(Z^TZ)^{-1}Z_{new}^T)M^T) $$\n",
    "\n",
    "We now need to find an $M$ that will statisfy the conditions of the question, namely, that $$ M\\sigma^2(I-Z_{new}(Z^TZ)^{-1}Z_{new}^T)M^T = \\sigma^2I$$\n",
    "\n",
    "Divide by $\\sigma^2 > 0$\n",
    "$$ M(I-Z_{new}(Z^TZ)^{-1}Z_{new}^T)M^T = I$$\n",
    "\n",
    "We'll define the middle expression to be $R$ and hence:\n",
    "$$  M R M^T = I$$\n",
    "where \n",
    "$$ R = (I-Z_{new}(Z^TZ)^{-1}Z_{new}^T) $$\n",
    "\n",
    "$$ Q.E.D$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e573fe-b583-4a12-bb9f-5061cf737142",
   "metadata": {},
   "source": [
    "#### Part 2.1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b656e2f-287a-463d-bb1a-86aa9b6cea78",
   "metadata": {},
   "source": [
    "We now consider implementing a series of hypothesis tests about whether daily rainfall is remaining consistent over the years or whether it is changing in some meaningful way.\n",
    "\n",
    "For each of the years 2010, 2011,...,2021, repeat the following. Define a data matrix  ùëç  using the features in  (2)  consisting of all dates prior to that year (so that for 2010,  ùëç  will be a data matrix for the years 2005‚Äì2009, for 2011,  ùëç  will be the data for years 2005-2011, and so on). Define the responses  ùë¶  to consist of rainfall for the given years. Define the new data matrix  ùëçùëõùëíùë§‚àà‚Ñùùëõ√óùëù  to consist of the  ùëõ  days of measurements in the given year ( ùëõ‚â§366 ) and the responses  ùë¶ùëõùëíùë§  to be the rainfall in those days. For this data, compute the statistic  ùê¥  in  (3)  and its p-value, that is, conditional on  ùê¥=ùëé , report\n",
    "ùëù:=Pr[ùê¥‚â•ùëé]under ùêª0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41e4239-6afa-4fa5-9091-89d866d6e6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, I'll define the helpers that were previosuly calculated - get_m_matrix and get_a, as defined by the question\n",
    "\n",
    "def get_m_matrix(Z_new, Z_past):\n",
    "    \n",
    "    n = Z_new.shape[0]\n",
    "    I_matrix = np.identity(n)\n",
    "        \n",
    "    R_matrix = I_matrix - Z_new @ np.linalg.inv(Z_past.T @ Z_past) @ Z_new.T\n",
    "\n",
    "    U, s, vh = np.linalg.svd(R_matrix)\n",
    "    \n",
    "    M_matrix = (np.diag(s) ** 0.5) @ (U.T)\n",
    "        \n",
    "    return M_matrix    \n",
    "\n",
    "\n",
    "\n",
    "def calc_a(n, m, d, M_matrix, y_new, y_hat_new, y_past, y_hat_past):   \n",
    "    \n",
    "    num = M_matrix @ (y_new - y_hat_new)\n",
    "    numerator = (1/n) * (num @ num)\n",
    "    \n",
    "    den = y_past - y_hat_past\n",
    "    denominator = (1 / (m-d)) * (den @ den)\n",
    "\n",
    "    return numerator / denominator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0194232b-eb7b-4ab8-9570-24ab0b35d955",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_p_vals = {}\n",
    "\n",
    "for year in range(2010, 2022):\n",
    "    past_data = data[data[\"Year\"] < year]\n",
    "\n",
    "    y_past = past_data[\"Rainfall\"].values\n",
    "    X_past = past_data[[\"X_1\", \"X_2\"]]\n",
    "    Z_past = np.c_[np.ones_like(y_past), X_past]\n",
    "\n",
    "    beta_hat = np.linalg.inv(Z_past.T @ Z_past) @ Z_past.T @ y_past\n",
    "    y_hat_past = Z_past @ beta_hat\n",
    "\n",
    "    new = data[data[\"Year\"] == year]\n",
    "\n",
    "    y_new = new[\"Rainfall\"].values\n",
    "    X_new = new[['X_1', 'X_2']]\n",
    "    Z_new = np.c_[np.ones_like(y_new), X_new]\n",
    "\n",
    "    y_hat_new = Z_new @ beta_hat\n",
    "\n",
    "    M_matrix = get_m_matrix(Z_new, Z_past)\n",
    "    \n",
    "    n = y_new.shape[0]\n",
    "    m, d = Z_past.shape    \n",
    "    \n",
    "    A = calc_a(n, m, d, M_matrix, y_new, y_hat_new, y_past, y_hat_past)\n",
    "    p_val = st.f.sf(A, n, m - d)\n",
    "    print(f\"For year {year}, the A statistic is {A:.3f} and the p value is {p_val:.3f}.\")\n",
    "    year_p_vals.update({year: p_val})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254c8cb4-eb0b-47c8-822f-f4a950655575",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x = year_p_vals.keys(), y = year_p_vals.values(), data=year_p_vals)\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"P Value\")\n",
    "plt.title(\"Years And P Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef30c227-d39d-483b-a262-3e135e7481d2",
   "metadata": {},
   "source": [
    "We can see that i most years, the p value does not allow us to reject $H_0$. However, we can see that in recent years the p values tend to get lower. In any case, we cannot reject the hypothesis that the rainfall does not dwindle over the years.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b51082-acd5-4376-9866-c67ddc958fcf",
   "metadata": {},
   "source": [
    "#### Part 2.1.4\n",
    "\n",
    "No, it wouldn't. Considering that we've tested on multiple years, we would have to adjust the p value calculation to the number of tests we are running to see real significance.\n",
    "\n",
    "Also, we should remember that the p value is affected by both $n$, the sample size and $k$, the magnitude of the effect. If we have a very large sample size, the effect itself might be negligible while the p value is extremly small."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70334c9e-7306-4fd9-890e-d64ad34b593d",
   "metadata": {},
   "source": [
    "3) Consider the total amount of rainfall within each month. Suppose that we assume that there is no change in the distribution over time across years, but we suspect that December is usuallly rainier than February. Design a test procedure that checks whether this is true. Use two apporaches:\n",
    " - Two-sample t-test \n",
    " - Paired t-test \n",
    " - Which approach seems more approproate here? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b822de1-3e20-4b1d-93bb-63c2aa933214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'll start with a two-sample t-test, asking whether Dec = Feb\n",
    "# Afterward, I'll run a simple t-test, asking if Dec > Feb\n",
    "\n",
    "alpha = 0.05\n",
    "\n",
    "feb_data = data[data[\"Month\"] == 2]\n",
    "dec_data = data[data[\"Month\"] == 12]\n",
    "\n",
    "feb_gb_year_y = feb_data.groupby(\"Year\").sum()[\"Rainfall\"].values\n",
    "dec_gb_year_y = dec_data.groupby(\"Year\").sum()[\"Rainfall\"].values\n",
    "\n",
    "feb_n = feb_gb_year_y.shape[0]\n",
    "dec_n = dec_gb_year_y.shape[0]\n",
    "\n",
    "feb_year_y_mean = np.mean(feb_gb_year_y)\n",
    "dec_year_y_mean = np.mean(dec_gb_year_y)\n",
    "\n",
    "feb_year_y_std = np.std(feb_gb_year_y)\n",
    "dec_year_y_std = np.std(dec_gb_year_y)\n",
    "\n",
    "dof = dec_n + feb_n -2\n",
    "\n",
    "two_sampled_s_squared = (np.sum((dec_gb_year_y - dec_year_y_mean)** 2)  + np.sum((feb_gb_year_y - feb_year_y_mean) ** 2) ) / dof\n",
    "\n",
    "two_sampled_t_stat = (dec_year_y_mean - feb_year_y_mean) / np.sqrt(two_tailed_s_squared * (1 / feb_n + 1 / dec_n))\n",
    "\n",
    "\n",
    "two_sampled_critical_value = st.t.ppf(1 - alpha/2, df=dof) \n",
    "two_sampled_t_stat_p_value = st.t.sf(two_sampled_t_stat, dof)\n",
    "assert abs(two_sampled_t_stat_p_value) < two_sampled_critical_value\n",
    "\n",
    "t_stat, p_val = st.ttest_ind(dec_gb_year_y, feb_gb_year_y)\n",
    "\n",
    "print(f\"The p value for Dec = Feb is {p_val}, therefore we can reject H_0 for alpha = 0.05\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b8bc04-6d69-49e9-baec-31ae9feb72bb",
   "metadata": {},
   "source": [
    "December and February are indeed different, next we'll see if Dec > Feb in a one sampled t test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2977d2ed-7f1a-460a-b04c-32e9086f51cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_sampled_s_sqaured = (1 / (dec_n - 1)) * np.sum((dec_gb_year_y - dec_year_y_mean) ** 2)\n",
    "\n",
    "one_sampled_t_num = (dec_year_y_mean - feb_year_y_mean)\n",
    "one_sampled_t_den = np.sqrt(one_sampled_s_sqaured) / np.sqrt(dec_n)\n",
    "\n",
    "one_sampled_t = one_sampled_t_num / one_sampled_t_den\n",
    "\n",
    "p_val = st.t.sf(one_sampled_t, dec_n)\n",
    "\n",
    "print(f\"The p value for Dec > Feb is {p_val:.3f}, therefore we can reject H_0 for alpha = 0.05\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0523c157-90b5-4715-ba9d-cda459e11cd5",
   "metadata": {},
   "source": [
    "next, I'll move on to the paired t test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac63b449-2d2c-4a39-892f-57c95719361f",
   "metadata": {},
   "outputs": [],
   "source": [
    "deltas = dec_gb_year_y - feb_gb_year_y\n",
    "\n",
    "deltas_mean = np.mean(deltas)\n",
    "dof = len(dec_gb_year_y) - 1\n",
    "\n",
    "paired_s_squared =  (1 / dof) * np.sum((deltas - deltas_mean) ** 2)\n",
    "\n",
    "paired_t_stat = deltas_mean / (np.sqrt(paired_s_squared / len(deltas)))\n",
    "paired_critical_value = st.t.ppf(1 - alpha, df=dof)\n",
    "paired_t_stat_p_value = st.t.sf(paired_t_stat, df=dof)\n",
    "\n",
    "assert abs(paired_t_stat_p_value) < paired_critical_value\n",
    "\n",
    "print(f\"The p value for the paired test is {paired_t_stat_p_value:.3f}, therefore we can reject H_0 for alpha = 0.05\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990220a3-d1e9-4d45-9a84-aff417a3a543",
   "metadata": {},
   "source": [
    "In this case, it seems that the paired test is the more accurate test. In this case, we do not assume that the data $within$ the year is independent, but assume that the data is independent only $between$ years. In this case, it is better to use the paired test and not the two-sampled test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3605e381-a12a-4660-a6dd-e33197606f8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

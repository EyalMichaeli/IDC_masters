{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Platform\n",
    "## Small files and MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**By:**  \n",
    "\n",
    "Eyal Michaeli, 207380528\n",
    "Tzach Larboni, 302673355\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Abstract: <br>\n",
    "In the following notebook we will implement the Consolidated Batch Files solution to the small files problem of MapReduce. We chose to implement a slight modification of the word-counter implementation of MapReduce, in which we will count the number of appearances of each first name in the origin files. <br>\n",
    "First, we will generate multiple small files and run MapReduce on them while timing the run. Later, we will implement a consolidation function and run the same MapReduce code on the consolidated files while timing it as well. <br>\n",
    "Our implementation shows that the proposed solution indeed makes MapReduce more efficient and thus resolves the small files problem. For further discussion of the solution, please see the relevant section in the attached paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import warnings\n",
    "import concurrent\n",
    "import sqlite3\n",
    "import traceback\n",
    "import shutil\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "# ml\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hide Warnings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disable Autoscrolling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "IPython.OutputArea.prototype._should_scroll = function(lines) {\n    return false;\n}\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set Random Seed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# insert your path here:\n",
    "my_path = \"/Users/eyalmichaeli/Desktop/School/Master's/IDC_masters/BigDataPlatforms/Final Project - MapReduce/output\" \n",
    "# \"/Users/mymac/IDC_masters/big_data_platforms_ex2\"\n",
    "\n",
    "path = Path(my_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstname = ['John', 'Dana', 'Scott', 'Marc', 'Steven', 'Michael', 'Albert', 'Johanna']\n",
    "city = ['NewYork', 'Haifa', 'Munchen', 'London', 'PaloAlto', 'TelAviv', 'Kiel', 'Hamburg']\n",
    "secondname = ['Lennon', 'McCartney', 'Starr', 'Harrison', 'Ono', 'Sutcliffe', 'Epstein', 'Preston']\n",
    "\n",
    "def create_csvs(number_of_csvs: int, rows_per_csv) -> None:\n",
    "    \"\"\"\n",
    "    Creates <number_of_csvs> csv files, with the firstname, city, secondname defined above \n",
    "    \"\"\"\n",
    "    csvs_path = path / \"csvs\"\n",
    "    csvs_path.mkdir(parents=True, exist_ok=True)\n",
    "    csv_paths = list()\n",
    "    for i in range(0, number_of_csvs):\n",
    "        temp_df = pd.DataFrame({\"firstname\": np.random.choice(firstname, rows_per_csv),\n",
    "                                \"secondname\": np.random.choice(secondname, rows_per_csv),\n",
    "                                \"city\": np.random.choice(city, rows_per_csv),\n",
    "                                })\n",
    "\n",
    "        csv_path = str(csvs_path / f\"myCSV{i+1}.csv\")   \n",
    "        csv_paths.append(csv_path)                  \n",
    "        temp_df.to_csv(csv_path, index=False)\n",
    "\n",
    "    print(f\"Created {number_of_csvs} CSV files\")\n",
    "\n",
    "    return csv_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use python to Create `mapreducetemp` and `mapreducefinal` folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created folders\n"
     ]
    }
   ],
   "source": [
    "mapreducetemp_folder = path / \"mapreducetemp\"\n",
    "mapreducetemp_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "mapreducefinal_folder = path / \"mapreducefinal\"\n",
    "mapreducefinal_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Created folders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "## MapReduceEngine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following Python code creates an SQLite database with the following table\n",
    "\n",
    "`TableName: temp_results`   \n",
    "`schema: (key:TEXT, value:TEXT)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a Data Base, in path: /Users/eyalmichaeli/Desktop/School/Master's/IDC_masters/BigDataPlatforms/Final Project - MapReduce/output/temp_results.db\n"
     ]
    }
   ],
   "source": [
    "# Creates the database \"temp_results.db\", then closes it.\n",
    "def create_db(db_path):\n",
    "    conn = None\n",
    "    cursor = None\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"CREATE TABLE IF NOT EXISTS temp_results (key, value);\")\n",
    "\n",
    "    except Exception:\n",
    "        traceback.print_exc()\n",
    "\n",
    "    finally:\n",
    "        print(f\"Created a Database, in the following path: {db_path}\")\n",
    "        cursor.close()\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "\n",
    "create_db(str(path / \"temp_results.db\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Explanation about the following Python Class and its method:** The class is `MapReduceEngine` and the method is `def execute(input_data, map_function, reduce_function)` such that:\n",
    "    - `input_data`: is an array of elements\n",
    "    - `map_function`: is a pointer to the Python function that returns a list where each entry of the form (key,value) \n",
    "    - `reduce_function`: is pointer to the Python function that returns a list where each entry of the form (key,value)\n",
    "\n",
    "In more detail, the execute method implements:\n",
    "<br>\n",
    "1. For each key of `input_data`, start a new Python thread that executes map_function(key) <br>\n",
    "2. Each thread will store results of the map_function into mapreducetemp/part-tmp-X.csv where X is a unique number per each thread. <br>\n",
    "3. Keep the list of all threads and check whether they are completed.<br>\n",
    "4. Once all threads completed, load the content of all CSV files into the temp_results table in SQLite.\n",
    "\n",
    "    Remark: The easiest way is to loop over all CSV files and load them into Pandas DataFrame first and then load them into SQLite\n",
    "    `data = pd.read_csv(path to csv)`\n",
    "    `data.to_sql(‘temp_results’,sql_conn, if_exists=’append’,index=False)`\n",
    "5. Write SQL statement that generates a sorted list by key of the form `(key, value)`, where `value` is a concatenation of ALL values in the value column that match the specific key.<br>\n",
    "6. Start a new thread for each value from the generated list in the previous step and execute `reduce_function(key,value)`. <br>\n",
    "7. Each thread stores the results of reduce_function into `mapreducefinal/part-X-final.csv` file. <br>\n",
    "8. Keep list of all threads and check whether they are completed  <br>\n",
    "9. Once all threads completed, print `MapReduce Completed`. Otherwise, print `MapReduce Failed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class MapReduceEngine:\n",
    "    \"\"\"\n",
    "    a class that implements MapReduce. Gets an Sqlite connection in its __init__.\n",
    "    calls the functions: inverted_map and inverted_reduce in its execute method,\n",
    "    which constitutes the MapReduce engine.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, conn):\n",
    "        self.conn = conn\n",
    "\n",
    "    def execute(self, input_data: List[str], map_function, reduce_function, params: dict, print_count_of_occurances=False):\n",
    "        thread_list_map, csvs_paths_map, thread_list_reduce, csvs_paths_reduce = list(), list(), list(), list()\n",
    "        exec_map = concurrent.futures.ThreadPoolExecutor()\n",
    "        for csv_key in input_data:\n",
    "            t = exec_map.submit(map_function, csv_key, params['column_index'])\n",
    "            threads_returns = t.result()\n",
    "            csv_index = input_data.index(csv_key)  # an index of the relative csv in the input_array\n",
    "            csv_path = f'{mapreducetemp_folder}/part-tmp-{csv_index}.csv'\n",
    "            csvs_paths_map.append(csv_path)\n",
    "            pd.DataFrame(threads_returns).to_csv(csv_path,\n",
    "                                                 header=['key', 'value'],\n",
    "                                                 index=False)\n",
    "            thread_list_map.append(t)\n",
    "\n",
    "        # wait until the threads are completed\n",
    "        exec_map.shutdown(wait=True)\n",
    "\n",
    "        # Once all threads completed, load content of all CSV files into the temp_results table in Sqlite\n",
    "        for path_to_csv in csvs_paths_map:\n",
    "            data = pd.read_csv(path_to_csv)\n",
    "            data.to_sql(name='temp_results', con=self.conn, if_exists='append', index=False)\n",
    "\n",
    "\n",
    "        results_df = pd.read_sql_query(\"SELECT key, GROUP_CONCAT(value) as value \"\n",
    "                                       \"FROM temp_results \"\n",
    "                                       \"GROUP BY key \"\n",
    "                                       \"ORDER BY key\",\n",
    "                                       conn)\n",
    "\n",
    "        exec_reduce = concurrent.futures.ThreadPoolExecutor()\n",
    "        for res_i in range(len(results_df)):\n",
    "            try:\n",
    "                key = results_df[\"key\"].iloc[res_i]\n",
    "                value = results_df[\"value\"].iloc[res_i]\n",
    "                t = exec_reduce.submit(reduce_function, key, value, print_count_of_occurances)\n",
    "                t_results = t.result() # t_results is one list, in which the 1st index is the key and the 2nd is a concat of all of the files it appears in.\n",
    "                csv_path = f'{mapreducefinal_folder}/part-{res_i}-final.csv'\n",
    "                csvs_paths_reduce.append(csv_path)\n",
    "                pd.DataFrame({'key': t_results[0], 'value': t_results[1]}, index=[0]).to_csv(csv_path,\n",
    "                                                     index=False)\n",
    "                thread_list_reduce.append(t)\n",
    "\n",
    "            except Exception:\n",
    "                print(f\"Mapreduce failed for result index: {res_i} with key: {key}, value: {value}\")\n",
    "                traceback.print_exc()\n",
    "                # close connection to db\n",
    "                if conn:\n",
    "                    conn.close()\n",
    "\n",
    "        # wait until the threads are completed\n",
    "        exec_reduce.shutdown(wait=True)\n",
    "\n",
    "        # close connection to db\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "        return 'MapReduce Completed'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code implements the `inverted_map(document_name)` function, which reads the CSV document from the local disc and returns a list containing entries of the form (key_value, document name)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverted_map(document_name: str, column_index: int) -> List[tuple]:\n",
    "    \"\"\"\n",
    "    reads the CSV document from the local disc and returns a list that contains entries of the form (key_value, document name) for the specific column_index provided.\n",
    "    :param document_name: csv file name.\n",
    "    :param column_index: column index in the csv file (Note: starting from 1)\n",
    "    :return: List[tuple] where each tuple contains 2 strings\n",
    "    \"\"\"\n",
    "    csv_path = str(path / 'csvs'/ document_name)\n",
    "    df = pd.read_csv(csv_path)\n",
    "    col_series = df[df.columns[column_index-1]]\n",
    "    csv_path_list = [csv_path] * len(df)\n",
    "    return list(zip(col_series.values, csv_path_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The following code implements the `inverted_reduce(value, documents)` function, where the field `documents` contains a list of all CSV documents per given a value. <br>\n",
    "This list might have duplicates.<br>\n",
    "The reduce function will return a list of [key, count_of_occurrences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_count_dict = {}\n",
    "\n",
    "def inverted_reduce(key: str, documents: str, print_number_of_occurances: bool) -> List[str]:\n",
    "    \"\"\"\n",
    "    reduce function\n",
    "    :param key: key value (for example: if the column is 'first_name' it could be 'Albert'.\n",
    "    :param documents: a string (list) of all CSV documents per given key.\n",
    "    :return: List: [key, count_of_occurances]\n",
    "    \"\"\"\n",
    "    count_of_occurances = len(documents.split(\",\"))\n",
    "    global keys_count_dict\n",
    "    keys_count_dict[key] = count_of_occurances\n",
    "\n",
    "    if print_number_of_occurances:\n",
    "        print(f\"Key: {key}, count of occurances: {count_of_occurances}\\n\")\n",
    "        \n",
    "    return [key, count_of_occurances]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "\n",
    "## Testing Our MapReduce\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compare the default implementation of MapReduce to the consolidated run of MapReduce, we will run two executes:\n",
    "1. Regular MapReduce on all the CSVs, as is.\n",
    "2. Regular MapReduce, after we have consolidated the CSVs into several bigger files (in terms of rows and size)\n",
    "\n",
    "First, we will create 1000 small CSVs (with 5 rows) as our input data for the MapReduce job, (which is to count number of appearances of each value in the files):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 1000 CSV files\n"
     ]
    }
   ],
   "source": [
    "NUMBER_OF_CSVS = 1000\n",
    "ROWS_PER_CSV = 5\n",
    "small_csvs_paths = create_csvs(number_of_csvs=NUMBER_OF_CSVS, rows_per_csv=ROWS_PER_CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will submit our MapReduce job on the data as-is and time the operation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: Albert, count of occurances: 661\n",
      "\n",
      "Key: Dana, count of occurances: 623\n",
      "\n",
      "Key: Johanna, count of occurances: 616\n",
      "\n",
      "Key: John, count of occurances: 612\n",
      "\n",
      "Key: Marc, count of occurances: 582\n",
      "\n",
      "Key: Michael, count of occurances: 654\n",
      "\n",
      "Key: Scott, count of occurances: 615\n",
      "\n",
      "Key: Steven, count of occurances: 637\n",
      "\n",
      "MapReduce Completed\n",
      "\n",
      "it took 19.841 seconds to run on the data as-is\n"
     ]
    }
   ],
   "source": [
    "# create an SQL DB connection\n",
    "conn = sqlite3.connect(str(path / \"temp_results.db\"))\n",
    "\n",
    "# create MapReduceEngine instance\n",
    "mapreduce = MapReduceEngine(conn=conn)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# execute MapReduce on input_data, on first_name (same as HW2)\n",
    "status = mapreduce.execute(small_csvs_paths,\n",
    "                           inverted_map,\n",
    "                           inverted_reduce,\n",
    "                           params={'column_index': 1},\n",
    "                           print_count_of_occurances=True)  # assign true if you want the reduce function to print the number of total occurances for each key (also helps to debug)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "\n",
    "print(status)\n",
    "print(f\"\\nit took {end_time-start_time:.3f} seconds to run on the data as-is\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will submit our MapReduce job on the data after consolidating the small files and time the operation.\n",
    "First, we will consolidate the small files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_small_files(csv_files_paths: List[str], min_file_size: int, merged_csvs_folder: str, print_each_file_size=False):\n",
    "    \"\"\"\n",
    "    Reads csv files, ip a csv file is smaller than <min_file_size> (in bytes) than appends the next csv file, until reaches a size that's bigger than <min_file_size>.\n",
    "    Then, write the new csv files to a new_folder: <merged_csvs_folder>.\n",
    "    \"\"\"\n",
    "    merged_csv_folder = Path(merged_csvs_folder)\n",
    "    list_of_merged_dfs = list()\n",
    "    append = False\n",
    "\n",
    "    # go over the csvs\n",
    "    for index, csv_file in enumerate(csv_files_paths):\n",
    "        file_size = Path(csv_file).stat().st_size\n",
    "            \n",
    "        temp_df = pd.read_csv(csv_file)\n",
    "\n",
    "        if append:\n",
    "            temp_df = last_temp_df.append(temp_df)  # take the temp_df from the last operation, and then append the current temp_df\n",
    "            file_size += last_file_size\n",
    "            if print_each_file_size:\n",
    "                print(f\"{file_size:.3f}\")\n",
    "\n",
    "        if file_size < min_file_size:\n",
    "            append = True\n",
    "            if index == len(csv_files_paths) - 1:  # if we are in the last file, add it to the CSVs, even if it might not be in size of 2000 or bigger\n",
    "                list_of_merged_dfs.append(temp_df)\n",
    "                break # we are in the end, and don't want to append the current temp_df again\n",
    "\n",
    "        else:\n",
    "            list_of_merged_dfs.append(temp_df)\n",
    "            append = False\n",
    "\n",
    "        # for the append of the dataframes, keep track of this iteration' temp_df & file_size for the next iteration\n",
    "        last_file_size = file_size\n",
    "        last_temp_df = temp_df\n",
    "    \n",
    "    merged_csvs_paths = list()\n",
    "    # write the new merged csv files\n",
    "    for i, merged_df in enumerate(start=1, iterable=list_of_merged_dfs):\n",
    "        merged_csv_path = merged_csv_folder / f\"merged_csv_{i}.csv\"\n",
    "        merged_csvs_paths.append(merged_csv_path)\n",
    "        merged_df.to_csv(merged_csv_path, index=False)\n",
    "\n",
    "    return merged_csvs_paths\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it took 5.451 seconds to do the merging\n"
     ]
    }
   ],
   "source": [
    "merged_csvs_folder = path / \"merged_csvs\"\n",
    "Path(merged_csvs_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "start_time = time.time()\n",
    "merged_csvs_paths = merge_small_files(\n",
    "                    csv_files_paths=small_csvs_paths, \n",
    "                    min_file_size=2000, \n",
    "                    merged_csvs_folder=merged_csvs_folder)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"it took {end_time-start_time:.3f} seconds to do the merging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will run MapReduce on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We can see that from 1000 CSV files, after merging some of them, we have: 65 CSV files (which are bigger in size)\n"
     ]
    }
   ],
   "source": [
    "print(f\"We can see that from {NUMBER_OF_CSVS} CSV files, after merging some of them, we have: {len(merged_csvs_paths)} CSV files (which are bigger in size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleted the folders\n",
      "deleted the db\n"
     ]
    }
   ],
   "source": [
    "# delete the mapreduce folders created by the last operation\n",
    "try:\n",
    "    shutil.rmtree(str(mapreducetemp_folder))\n",
    "    shutil.rmtree(str(mapreducefinal_folder))\n",
    "    print(\"deleted the folders\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f'Error: {e.strerror}')\n",
    "\n",
    "# delete the SQLite database\n",
    "try:\n",
    "    os.remove(str(path / 'temp_results.db'))\n",
    "    print(\"deleted the db\")\n",
    "except Exception as e:\n",
    "    print(f'Error: {str(path / \"temp_results.db\")}, {e.strerror}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created empty folders\n",
      "Created a Data Base, in path: /Users/eyalmichaeli/Desktop/School/Master's/IDC_masters/BigDataPlatforms/Final Project - MapReduce/output/temp_results.db\n"
     ]
    }
   ],
   "source": [
    "# create new empty folders\n",
    "try: \n",
    "    mapreducetemp_folder.mkdir(parents=True, exist_ok=True)\n",
    "    mapreducefinal_folder.mkdir(parents=True, exist_ok=True)\n",
    "    print(\"Created empty folders\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f'Error: {e.strerror}')\n",
    "    \n",
    "former_keys_count_dict = keys_count_dict.copy()  # copy the last dict, to compare later to the new dict\n",
    "keys_count_dict = {}  # set it back to empty\n",
    "\n",
    "# create the db again\n",
    "create_db(str(path / \"temp_results.db\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dicts(keys_count_dict, new_keys_count_dict):\n",
    "    \"\"\"\n",
    "    A function to make sure the 2 dicts created from the output of the reducers, are the same, between the 2 MapReduce jobs.\n",
    "    \"\"\"\n",
    "    passed = True\n",
    "    try:          \n",
    "        assert len(new_keys_count_dict) == len(keys_count_dict)\n",
    "\n",
    "    except Exception as e:\n",
    "        passed = False\n",
    "        print(len(new_keys_count_dict), len(keys_count_dict))\n",
    "        print(\"the 2 dicts length are not equal (not the same number of keys)\")\n",
    "\n",
    "    for key in keys_count_dict:\n",
    "        try:\n",
    "            assert new_keys_count_dict[key] == keys_count_dict[key]\n",
    "\n",
    "        except Exception as e:\n",
    "            passed = False\n",
    "            print(new_keys_count_dict[key], keys_count_dict[key])\n",
    "            print(f\"For key: {key}, the values are not equal!\")\n",
    "    \n",
    "    if passed:\n",
    "        print(\"All Tests Passed Successfully!\")\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: Albert, count of occurances: 661\n",
      "\n",
      "Key: Dana, count of occurances: 623\n",
      "\n",
      "Key: Johanna, count of occurances: 616\n",
      "\n",
      "Key: John, count of occurances: 612\n",
      "\n",
      "Key: Marc, count of occurances: 582\n",
      "\n",
      "Key: Michael, count of occurances: 654\n",
      "\n",
      "Key: Scott, count of occurances: 615\n",
      "\n",
      "Key: Steven, count of occurances: 637\n",
      "\n",
      "All Tests Passed Successfully!\n",
      "MapReduce Completed\n",
      "\n",
      "It took 1.839 seconds to run on the merged data\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# create an SQL DB connection\n",
    "conn = sqlite3.connect(str(path / \"temp_results.db\"))\n",
    "\n",
    "# create MapReduceEngine instance\n",
    "mapreduce = MapReduceEngine(conn=conn)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# execute MapReduce on input_data, on first_name (same as HW2)\n",
    "status = mapreduce.execute(merged_csvs_paths,\n",
    "                           inverted_map,\n",
    "                           inverted_reduce,\n",
    "                           params={'column_index': 1},\n",
    "                           print_count_of_occurances=True)  # assign true if you want the reduce function to print the number of total occurances for each key (also helps to debug)\n",
    "                           \n",
    "end_time = time.time()\n",
    "\n",
    "# unit test: make sure that the dicts are the same\n",
    "test_dicts(former_keys_count_dict, keys_count_dict)\n",
    "\n",
    "print(status)\n",
    "\n",
    "print(f\"\\nIt took {end_time-start_time:.3f} seconds to run on the merged data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As we can see, on the data as-is, it takes 15-20 seconds as opposed to on the consolidated data, which on it takes 1-2 seconds.** This difference is meaningful and it showcases that the solution's impact. <br>\n",
    "We will also note that we had to preprocess the data before, which took about 6-10 seconds. However, it is still quicker than the 15-20 seconds of the origin case - consolidating all of the small files AND running MapReduce still took less than 12 seconds in total.<br>\n",
    "We would like to emphasize that even if the entire process was not faster than running MapReduce on the origin files, we can still schedule the consolidation to run during off-hours. Hence, in our opinion, it would still a better solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we delete all temporary data from mapreducetemp folder and delete SQLite database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# delete all temp data from mapreducetemp\n",
    "try:\n",
    "    shutil.rmtree(str(mapreducetemp_folder))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f'Error: {str(mapreducetemp_folder)}, {e.strerror}')\n",
    "\n",
    "\n",
    "# delete the SQLite database\n",
    "try:\n",
    "    os.remove(str(path / 'temp_results.db'))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f'Error: {str(path / \"temp_results.db\")}, {e.strerror}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}